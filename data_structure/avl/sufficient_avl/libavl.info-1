This is libavl.info, produced by makeinfo version 4.8 from libavl.texi.


File: libavl.info,  Node: Top,  Next: Preface,  Prev: (dir),  Up: (dir)

GNU libavl 2.0.3
****************

* Menu:

* Preface::
* Introduction::
* The Table ADT::
* Search Algorithms::
* Binary Search Trees::
* AVL Trees::
* Red-Black Trees::
* Threaded Binary Search Trees::
* Threaded AVL Trees::
* Threaded Red-Black Trees::
* Right-Threaded Binary Search Trees::
* Right-Threaded AVL Trees::
* Right-Threaded Red-Black Trees::
* BSTs with Parent Pointers::
* AVL Trees with Parent Pointers::
* Red-Black Trees with Parent Pointers::
* References::
* Supplementary Code::
* GNU Free Documentation License::
* Glossary::
* Answers to All the Exercises::
* Catalogue of Algorithms::
* Index::


File: libavl.info,  Node: Preface,  Next: Introduction,  Prev: Top,  Up: Top

Preface
*******

Early in 1998, I wanted an AVL tree library for use in writing GNU
PSPP.  At the time, few of these were available on the Internet.  Those
that were had licenses that were not entirely satisfactory for
inclusion in GNU software.  I resolved to write my own.  I sat down
with Knuth's `The Art of Computer Programming' and did so.  The result
was the earliest version of libavl.  As I wrote it, I learned valuable
lessons about implementing algorithms for binary search trees, and
covered many notebook pages with scribbled diagrams.

   Later, I decided that what I really wanted was a similar library for
threaded AVL trees, so I added an implementation to libavl.  Along the
way, I ended up having to relearn many of the lessons I'd already
painstakingly uncovered in my earlier work.  Even later, I had much the
same experience in writing code for right-threaded AVL trees and
red-black trees, which was done as much for my own education as any
intention of using the code in real software.

   In late 1999, I contributed a chapter on binary search trees and
balanced trees to a book on programming in C.  This again required a
good deal of duplication of effort as I rediscovered old techniques.
By now I was beginning to see the pattern, so I decided to document
once and for all the algorithms I had chosen and the tradeoffs I had
made.  Along the way, the project expanded in scope several times.

   You are looking at the results.  I hope you find that it is as useful
for reading and reference as I found that writing it was enjoyable for
me.  As I wrote later chapters, I referred less and less to my other
reference books and more and more to my own earlier chapters, so I
already know that it can come in handy for me.

   Please feel free to copy and distribute this book, in accordance with
the license agreement.  If you make multiple printed copies, consider
contacting me by email first to check whether there are any
late-breaking corrections or new editions in the pipeline.

* Menu:

* Acknowledgements::
* Contacting the Author::


File: libavl.info,  Node: Acknowledgements,  Next: Contacting the Author,  Prev: Preface,  Up: Preface

Acknowledgements
================

libavl has grown into its current state over a period of years.  During
that time, many people have contributed advice, bug reports, and
occasional code fragments.  I have attempted to individually
acknowledge all of these people, along with their contributions, in the
`NEWS' and `ChangeLog' files included with the libavl source
distribution.  Without their help, libavl would not be what it is
today.  If you believe that you should be listed in one of these files,
but are not, please contact me.

   Many people have indirectly contributed by providing computer science
background and software infrastructure, without which libavl would not
have been possible at all.  For a partial list, please see `THANKS' in
the libavl source distribution.

   Special thanks are due to Erik Goodman of the A. H. Case Center for
Computer-Aided Engineering and Manufacturing at Michigan State
University for making it possible for me to receive MSU honors credit
for rewriting libavl as a literate program, and to Dann Corbit for his
invaluable suggestions during development.


File: libavl.info,  Node: Contacting the Author,  Prev: Acknowledgements,  Up: Preface

Contacting the Author
=====================

libavl, including this book, the source code, the TexiWEB software, and
related programs, was written by Ben Pfaff, who welcomes your feedback.
Please send libavl-related correspondence, including bug reports and
suggestions for improvement, to him at <blp@gnu.org>.

   Ben received his B.S. in electrical engineering from Michigan State
University in May 2001.  He is now studying for a Ph.D. in computer
science at Stanford University as a Stanford Graduate Fellow.

   Ben's personal webpage is at `http://benpfaff.org/', where you can
find a list of his current projects, including the status of libavl
test releases.  You can also find him hanging out in the Internet
newsgroup comp.lang.c.


File: libavl.info,  Node: Introduction,  Next: The Table ADT,  Prev: Preface,  Up: Top

1 Introduction
**************

libavl is a library in ANSI C for manipulation of various types of
binary trees.  This book provides an introduction to binary tree
techniques and presents all of libavl's source code, along with
annotations and exercises for the reader.  It also includes practical
information on how to use libavl in your programs and discussion of the
larger issues of how to choose efficient data structures and libraries.
The book concludes with suggestions for further reading, answers to
all the exercises, glossary, and index.

* Menu:

* Audience::
* Reading the Code::
* Code Conventions::
* Licenses::


File: libavl.info,  Node: Audience,  Next: Reading the Code,  Prev: Introduction,  Up: Introduction

1.1 Audience
============

This book is intended both for novices interested in finding out about
binary search trees and practicing programmers looking for a cookbook of
algorithms.  It has several features that will be appreciated by both
groups:

   * Tested code: With the exception of code presented as
     counterexamples, which are clearly marked, all code presented has
     been tested.  Most code comes with a working program for testing or
     demonstrating it.

   * No pseudo-code: Pseudo-code can be confusing, so it is not used.

   * Motivation: An important goal is to demonstrate general methods for
     programming, not just the particular algorithms being examined.
     As a result, the rationale for design choices is explained
     carefully.

   * Exercises and answers: To clarify issues raised within the text,
     many sections conclude with exercises.  All exercises come with
     complete answers in an appendix at the back of the book.

     Some exercises are marked with one or more stars (*).  Exercises
     without stars are recommended for all readers, but starred
     exercises deal with particularly obscure topics or make reference
     to topics covered later.

     Experienced programmers should find the exercises particularly
     interesting, because many of them present alternatives to choices
     made in the main text.

   * Asides: Occasionally a section is marked as an "aside".  Like
     exercises, asides often highlight alternatives to techniques in the
     main text, but asides are more extensive than most exercises.
     Asides are not essential to comprehension of the main text, so
     readers not interested may safely skip over them to the following
     section.

   * Minimal C knowledge assumed: Basic familiarity with the C language
     is assumed, but obscure constructions are briefly explained the
     first time they occur.

     Those who wish for a review of C language features before beginning
     should consult *Note Summit 1999::.  This is especially recommended
     for novices who feel uncomfortable with pointer and array concepts.

   * References: When appropriate, other texts that cover the same or
     related material are referenced at the end of sections.

   * Glossary: Terms are "emphasized" and defined the first time they
     are used.  Definitions for these terms and more are collected into
     a glossary at the back of the book.

   * Catalogue of algorithms: *Note Catalogue of Algorithms::, for a
     handy list of all the algorithms implemented in this book.


File: libavl.info,  Node: Reading the Code,  Next: Code Conventions,  Prev: Audience,  Up: Introduction

1.2 Reading the Code
====================

This book contains all the source code to libavl.  Conversely, much of
the source code presented in this book is part of libavl.

   libavl is written in ANSI/ISO C89 using TexiWEB, a "literate
programming" (*note literate programming::) system.  Literate
programming is a philosophy that regards software as a kind of
literature.  The ideas behind literate programming have been around for
a long time, but the term itself was invented by computer scientist
Donald Knuth in 1984, who wrote two of his most famous programs (TeX
and METAFONT) with a literate programming system of his own design.
That system, called WEB, inspired the form and much of the syntax of
TexiWEB.

   A TexiWEB document is a C program that has been cut into sections,
rearranged, and annotated, with the goal to make the program as a whole
as comprehensible as possible to a reader who starts at the beginning
and reads the entire program in order.  Of course, understanding large,
complex programs cannot be trivial, but TexiWEB tries to make it as
easy as possible.

   Each section of a TexiWEB program is assigned both a number and a
name.  Section numbers are assigned sequentially, starting from 1 with
the first section, and they are used for cross-references between
sections.  Section names are words or phrases assigned by the TexiWEB
program's author to describe the role of the section's code.

   Here's a sample TexiWEB section:

19. <Clear hash table entries 19> =
for (i = 0; i < hash->m; i++)
  hash->entry[i] = NULL;
   This code is included in 15.

   The first line of a section, as shown here, gives the section's name
and its number within angle brackets.  The section number is also given
at the left margin to make individual sections easy to find.

   Code segments often contain references to other code segments, shown
as a section name and number within angle brackets.  These act something
like macros, in that they stand for the corresponding replacement text.
For instance, consider the following segment:

15. <Initialize hash table 15> =
hash->m = 13;
<Clear hash table entries 19>
   See also 16.

   This means that the code for `Clear hash table entries' should be
inserted as part of `Initialize hash table'.  Because the name of a
section explains what it does, it's often unnecessary to know anything
more.  If you do want more detail, the section number 19 in <*Note
Clear hash table entries: 19.> can easily be used to find the full text
and annotations for `Clear hash table entries'.  At the bottom of
section 19 you will find a note reading `This code is included in 15.',
making it easy to move back to section 15 that includes it.

   There's also a note following the code in the section above: `See
also 16.'.  This demonstrates how TexiWEB handles multiple sections
that have the same name.  When a name that corresponds to multiple
sections is referenced, code from all the sections with that name is
substituted, in order of appearance.  The first section with the name
ends with a note listing the numbers of all other same-named sections.
Later sections show their own numbers in the left margin, but the
number of the first section within angle brackets, to make the first
section easy to find.  For example, here's another line of code for
<*Note Clear hash table entries: 15.>:

16. <Initialize hash table 15> +=
hash->n = 0;

   Code segment references have one more feature: the ability to do
special macro replacements within the referenced code.  These
replacements are made on all words within the code segment referenced
and recursively within code segments that the segment references, and
so on.  Word prefixes as well as full words are replaced, as are even
occurrences within comments in the referenced code.  Replacements take
place regardless of case, and the case of the replacement mirrors the
case of the replaced text. This odd feature is useful for adapting a
section of code written for one library having a particular identifier
prefix for use in a different library with another identifier prefix.
For instance, the reference `<BST types; bst => avl>' inserts the
contents of the segment named `BST types', replacing `bst' by `avl'
wherever the former appears at the beginning of a word.

   When a TexiWEB program is converted to C, conversion conceptually
begins from sections named for files; e.g., <`foo.c' 37>.  Within these
sections, all section references are expanded, then references within
those sections are expanded, and so on.  When expansion is complete,
the specified files are written out.

   A final resource in reading a TexiWEB is the index, which contains an
entry for the points of declaration of every section name, function,
type, structure, union, global variable, and macro.  Declarations within
functions are not indexed.

See also:  *Note Knuth 1992::, "How to read a WEB".


File: libavl.info,  Node: Code Conventions,  Next: Licenses,  Prev: Reading the Code,  Up: Introduction

1.3 Code Conventions
====================

Where possible, the libavl source code complies to the requirements
imposed by ANSI/ISO C89 and C99.  Features present only in C99 are not
used. In addition, most of the GNU Coding Standards are followed.
Indentation style is an exception to the latter: in print, to conserve
vertical space, K&R indentation style is used instead of GNU style.

See also:  *Note ISO 1990::; *Note ISO 1999::; *Note FSF 2001::,
"Writing C".


File: libavl.info,  Node: Licenses,  Prev: Code Conventions,  Up: Introduction

1.4 Licenses
============

This book is licensed under the GNU Free Documentation License, version
1.2 or later.  The book includes complete source code for the libavl
libraries and related programs, so these are also released under the
GNU Free Documentation License.

   The libraries in this book are also released under the following
license:

1. <Library License 1> =
/* GNU libavl - library for manipulation of binary trees.
   Copyright (C) 1998, 1999, 2000, 2001, 2002, 2004 Free
   Software Foundation, Inc.

   This library is free software; you can redistribute it and/or
   modify it under the terms of the GNU Lesser General Public
   License as published by the Free Software Foundation; either
   version 3 of the License, or (at your option) any later version.

   This library is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   Lesser General Public License for more details.

   You should have received a copy of the GNU Lesser General Public
   License along with this library; if not, write to the Free Software
   Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
   02110-1301 USA.
*/
   This code is included in *Note 25::, *Note 26::, *Note 143::, *Note
144::, *Note 194::, *Note 195::, *Note 249::, *Note 250::, *Note 299::,
*Note 300::, *Note 335::, *Note 336::, *Note 374::, *Note 375::, *Note
417::, *Note 418::, *Note 454::, *Note 455::, *Note 488::, *Note 489::,
*Note 521::, *Note 522::, *Note 553::, *Note 554::, and *Note 651::.

   The programs in this book are also released under the following
license:

2. <Program License 2> =
/* GNU libavl - library for manipulation of binary trees.
   Copyright (C) 1998, 1999, 2000, 2001, 2002, 2004 Free
   Software Foundation, Inc.

   This program is free software; you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation; either version 3 of the License, or
   (at your option) any later version.

   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License along
   with this program; if not, write to the Free Software Foundation, Inc.,
   51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
*/
   This code is included in *Note 98::, *Note 99::, *Note 100::, *Note
188::, *Note 240::, *Note 292::, *Note 332::, *Note 370::, *Note 413::,
*Note 451::, *Note 484::, *Note 517::, *Note 550::, *Note 585::, *Note
597::, *Note 601::, and *Note 619::.


File: libavl.info,  Node: The Table ADT,  Next: Search Algorithms,  Prev: Introduction,  Up: Top

2 The Table ADT
***************

Most of the chapters in this book implement a table structure as some
kind of binary tree, so it is important to understand what a table is
before we begin.  That is this chapter's purpose.

   This chapter begins with a brief definition of the meaning of "table"
for the purposes of this book, then moves on to describe in a more
formal way the interface of a table used by all of the tables in this
book.  The next chapter motivates the basic idea of a binary tree
starting from simple, everyday concepts.  Experienced programmers may
skip these chapters after skimming through the definitions below.

* Menu:

* Informal Definition::
* Identifiers::
* Comparison Function::
* Item and Copy Functions::
* Memory Allocation::
* Creation and Destruction::
* Count::
* Insertion and Deletion::
* Assertions::
* Traversers::
* Table Headers::
* Additional Exercises for Tables::


File: libavl.info,  Node: Informal Definition,  Next: Identifiers,  Prev: The Table ADT,  Up: The Table ADT

2.1 Informal Definition
=======================

If you've written even a few programs, you've probably noticed the
necessity for searchable collections of data.  Compilers search their
symbol tables for identifiers and network servers often search tables to
match up data with users.  Many applications with graphical user
interfaces deal with mouse and keyboard activity by searching a table of
possible actions.  In fact, just about every nontrivial program,
regardless of application domain, needs to maintain and search tables of
some kind.

   In this book, the term "table" does not refer to any particular data
structure.  Rather, it is the name for a abstract data structure or ADT,
defined in terms of the operations that can be performed on it.  A table
ADT can be implemented in any number of ways.  Later chapters will show
how to implement tables in terms of various binary tree data structures.

   The purpose of a table is to keep track of a collection of items,
all of the same type.  Items can be inserted into and deleted from a
table, with no arbitrary limit on the number of items in the table.  We
can also search a table for items that match a given item.

   Other operations are supported, too.  Traversal is the most
important of these: all of the items in a table can be visited, in
sorted order from smallest to largest, or from largest to smallest.
Traversals can also start from an item in the middle, or a newly
inserted item, and move in either direction.

   The data in a table may be of any C type, but all the items in a
table must be of the same type.  Structure types are common.  Often,
only part of each data item is used in item lookup, with the rest for
storage of auxiliary information.  A table that contains two-part data
items like this is called a "dictionary" or an "associative array".
The part of table data used for lookup, whether the table is a
dictionary or not, is the "key".  In a dictionary, the remainder is the
"value".

   Our tables cannot contain duplicates.  An attempt to insert an item
into a table that already contains a matching item will fail.

Exercises:

1. Suggest a way to simulate the ability to insert duplicate items in a
table.  [*Note answer: 2-1#1..]


File: libavl.info,  Node: Identifiers,  Next: Comparison Function,  Prev: Informal Definition,  Up: The Table ADT

2.2 Identifiers
===============

In C programming it is necessary to be careful if we expect to avoid
clashes between our own names and those used by others.  Any identifiers
that we pick might also be used by others.  The usual solution is to
adopt a prefix that is applied to the beginning of every identifier that
can be visible in code outside a single source file.  In particular,
most identifiers in a library's public header files must be prefixed.

   libavl is a collection of mostly independent modules, each of which
implements the table ADT.  Each module has its own, different identifier
prefix.  Identifiers that begin with this prefix are reserved for any
use in source files that #include the module header file.  Also
reserved (for use as macro names) are identifiers that begin with the
all-uppercase version of the prefix.  Both sets of identifiers are also
reserved as external names(1) throughout any program that uses the
module.

   In addition, all identifiers that begin with libavl_ or LIBAVL_ are
reserved for any use in source files that #include any libavl module.
Likewise, these identifiers are reserved as external names in any
program that uses any libavl module.  This is primarily to allow for
future expansion, but see *Note Memory Allocation:: and Exercise 2.5-1
for a sample use.

   The prefix used in code samples in this chapter is tbl_, short for
"table".  This can be considered a generic substitute for the prefix
used by any of the table implementation.  All of the statements about
these functions here apply equally to all of the table implementation in
later chapters, except that the tbl_ prefix must be replaced by the
prefix used by the chapter's table implementation.

Exercises:

1. The following kinds of identifiers are among those that might appear
in a header file.  Which of them can be safely appear unprefixed?  Why?

  a. Parameter names within function prototypes.

  b. Macro parameter names.

  c. Structure and union tags.

  d. Structure and union member names.
        [*Note answer: 2-2#1..]

2. Suppose that we create a module for reporting errors.  Why is err_ a
poorly chosen prefix for the module's identifiers?  [*Note answer:
2-2#2..]

   ---------- Footnotes ----------

   (1) External names are identifiers visible outside a single source
file.  These are, mainly, non-static functions and variables declared
outside a function.


File: libavl.info,  Node: Comparison Function,  Next: Item and Copy Functions,  Prev: Identifiers,  Up: The Table ADT

2.3 Comparison Function
=======================

The C language provides the void * generic pointer for dealing with
data of unknown type.  We will use this type to allow our tables to
contain a wide range of data types.  This flexibility does keep the
table from working directly with its data.  Instead, the table's user
must provide means to operate on data items.  This section describes
the user-provided functions for comparing items, and the next section
describes two other kinds of user-provided functions.

   There is more than one kind of generic algorithm for searching.  We
can search by comparison of keys, by digital properties of the keys, or
by computing a function of the keys.  In this book, we are only
interested in the first possibility, so we need a way to compare data
items.  This is done with a user-provided function compatible with
tbl_comparison_func, declared as follows:

3. <Table function types 3> =
/* Function types. */
typedef int tbl_comparison_func (const void *tbl_a, const void *tbl_b, 
                                 void *tbl_param);
   See also *Note 5::.
This code is included in *Note 15::.

   A comparison function takes two pointers to data items, here called a
and b, and compares their keys.  It returns a negative value if a < b,
zero if a == b, or a positive value if a > b.  It takes a third
parameter, here called param, which is user-provided.

   A comparison function must work more or less like an arithmetic
comparison within the domain of the data.  This could be alphabetical
ordering for strings, a set of nested sort orders (e.g., sort first by
last name, with duplicates by first name), or any other comparison
function that behaves in a "natural" way.  A comparison function in the
exact class of those acceptable is called a "strict weak ordering", for
which the exact rules are explained in Exercise 5.

   Here's a function that can be used as a comparison function for the
case that the void * pointers point to single ints:

4. <Comparison function for ints 4> =
/* Comparison function for pointers to ints.
   param is not used. */
int
compare_ints (const void *pa, const void *pb, void *param)
{
  const int *a = pa;
  const int *b = pb;

  if (*a < *b)
    return -1;
  else if (*a > *b)
    return +1;
  else
    return 0;
}
   This code is included in *Note 135::.

   Here's another comparison function for data items that point to
ordinary C strings:

/* Comparison function for strings.
   param is not used. */
int
compare_strings (const void *pa, const void *pb, void *param)
{
  return strcmp (pa, pb);
}

See also:  *Note FSF 1999::, node "Defining the Comparison Function";
*Note ISO 1998::, section 25.3, "Sorting and related operations"; *Note
SGI 1993::, section "Strict Weak Ordering".

Exercises:

1. In C, integers may be cast to pointers, including void *, and vice
versa.  Explain why it is not a good idea to use an integer cast to
void * as a data item.  When would such a technique would be acceptable?
[*Note answer: 2-3#1..]

2. When would the following be an acceptable alternate definition for
compare_ints()?

int
compare_ints (const void *pa, const void *pb, void *param)
{
  return *((int *) pa) - *((int *) pb);
}

   [*Note answer: 2-3#2..]

3. Could strcmp(), suitably cast, be used in place of compare_strings()?
[*Note answer: 2-3#3..]

4. Write a comparison function for data items that, in any particular
table, are character arrays of fixed length.  Among different tables,
the length may differ, so the third parameter to the function points to
a size_t specifying the length for a given table.  [*Note answer:
2-3#4..]

*5. For a comparison function f() to be a strict weak ordering, the
following must hold for all possible data items a, b, and c:

   * _Irreflexivity:_ For every a, f(a, a) == 0.

   * _Antisymmetry_: If f(a, b) > 0, then f(b, a) < 0.

   * _Transitivity_: If f(a, b) > 0 and f(b, c) > 0, then f(a, c) > 0.

   * _Transitivity of equivalence_: If f(a, b) == 0 and f(b, c) == 0,
     then f(a, c) == 0.

Consider the following questions that explore the definition of a strict
weak ordering.

  a. Explain how compare_ints() above satisfies each point of the
     definition.

  b. Can the standard C library function strcmp() be used for a strict
     weak ordering?

  c. Propose an irreflexive, antisymmetric, transitive function that
     lacks transitivity of equivalence.
        [*Note answer: 2-3#5..]

*6. libavl uses a ternary comparison function that returns a negative
value for <, zero for ==, positive for >.  Other libraries use binary
comparison functions that return nonzero for < or zero for >=.
Consider these questions about the differences:

  a. Write a C expression, in terms of a binary comparison function f()
     and two items a and b, that is nonzero if and only if a == b as
     defined by f().  Write a similar expression for a > b.

  b. Write a binary comparison function "wrapper" for a libavl
     comparison function.

  c. Rewrite bst_find() based on a binary comparison function.  (You can
     use the wrapper from above to simulate a binary comparison
     function.)
        [*Note answer: 2-3#6..]


File: libavl.info,  Node: Item and Copy Functions,  Next: Memory Allocation,  Prev: Comparison Function,  Up: The Table ADT

2.4 Item and Copy Functions
===========================

Besides tbl_comparison_func, there are two kinds of functions used in
libavl to manipulate item data:

5. <Table function types 3> +=
typedef void tbl_item_func (void *tbl_item, void *tbl_param);
typedef void *tbl_copy_func (void *tbl_item, void *tbl_param);

Both of these function types receive a table item as their first
argument tbl_item and the tbl_param associated with the table as their
second argument.  This tbl_param is the same one passed as the third
argument to tbl_comparison_func.  libavl will never pass a null pointer
as tbl_item to either kind of function.

   A tbl_item_func performs some kind of action on tbl_item.  The
particular action that it should perform depends on the context in which
it is used and the needs of the calling program.

   A tbl_copy_func creates and returns a new copy of tbl_item.  If
copying fails, then it returns a null pointer.


File: libavl.info,  Node: Memory Allocation,  Next: Creation and Destruction,  Prev: Item and Copy Functions,  Up: The Table ADT

2.5 Memory Allocation
=====================

The standard C library functions malloc() and free() are the usual way
to obtain and release memory for dynamic data structures like tables.
Most users will be satisfied if libavl uses these routines for memory
management.  On the other hand, some users will want to supply their
own methods for allocating and freeing memory, perhaps even different
methods from table to table.  For these users' benefit, each table is
associated with a memory allocator, which provides functions for memory
allocation and deallocation.  This allocator has the same form in each
table implementation.  It looks like this:

6. <Memory allocator 6> =
#ifndef LIBAVL_ALLOCATOR
#define LIBAVL_ALLOCATOR
/* Memory allocator. */
struct libavl_allocator
  {
    void *(*libavl_malloc) (struct libavl_allocator *, size_t libavl_size);
    void (*libavl_free) (struct libavl_allocator *, void *libavl_block);
  };
#endif
   This code is included in *Note 15::, *Note 100::, and *Note 651::.

   Members of struct libavl_allocator have the same interfaces as the
like-named standard C library functions, except that they are each
additionally passed a pointer to the struct libavl_allocator * itself
as their first argument.  The table implementations never call
tbl_malloc() with a zero size or tbl_free() with a null pointer block.

   The struct libavl_allocator type is shared between all of libavl's
modules, so its name begins with libavl_, not with the specific module
prefix that we've been representing generically here as tbl_.  This
makes it possible for a program to use a single allocator with multiple
libavl table modules, without the need to declare instances of
different structures.

   The default allocator is just a wrapper around malloc() and free().
Here it is:

7. <Default memory allocation functions 7> =
/* Allocates size bytes of space using malloc().
   Returns a null pointer if allocation fails. */
void *
tbl_malloc (struct libavl_allocator *allocator, size_t size)
{
  assert (allocator != NULL && size > 0);
  return malloc (size);
}

/* Frees block. */
void
tbl_free (struct libavl_allocator *allocator, void *block)
{
  assert (allocator != NULL && block != NULL);
  free (block);
}

/* Default memory allocator that uses malloc() and free(). */
struct libavl_allocator tbl_allocator_default =
  {
    tbl_malloc,
    tbl_free
  };
   This code is included in *Note 30::, *Note 147::, *Note 198::, *Note
253::, *Note 302::, *Note 338::, *Note 377::, *Note 420::, *Note 457::,
*Note 491::, *Note 524::, *Note 556::, and *Note 651::.

   The default allocator comes along with header file declarations:

8. <Default memory allocator header 8> =
/* Default memory allocator. */
extern struct libavl_allocator tbl_allocator_default;
void *tbl_malloc (struct libavl_allocator *, size_t);
void tbl_free (struct libavl_allocator *, void *);
   This code is included in *Note 15:: and *Note 651::.

See also:  *Note FSF 1999::, nodes "Malloc Examples" and "Changing Block
Size".

Exercises:

1. This structure is named with a libavl_ prefix because it is shared
among all of libavl's module.  Other types are shared among libavl
modules, too, such as tbl_item_func.  Why don't the names of these
other types also begin with libavl_?  [*Note answer: 2-5#1..]

2. Supply an alternate allocator, still using malloc() and free(), that
prints an error message to stderr and aborts program execution when
memory allocation fails.  [*Note answer: 2-5#2..]

*3. Some kinds of allocators may need additional arguments.  For
instance, if memory for each table is taken from a separate
Apache-style "memory pool", then a pointer to the pool structure is
needed.  Show how this can be done without modifying existing types.
[*Note answer: 2-5#3..]


File: libavl.info,  Node: Creation and Destruction,  Next: Count,  Prev: Memory Allocation,  Up: The Table ADT

2.6 Creation and Destruction
============================

This section describes the functions that create and destroy tables.

9. <Table creation function prototypes 9> =
/* Table functions. */
struct tbl_table *tbl_create (tbl_comparison_func *, void *,
                              struct libavl_allocator *);
struct tbl_table *tbl_copy (const struct tbl_table *, tbl_copy_func *,
                            tbl_item_func *, struct libavl_allocator *);
void tbl_destroy (struct tbl_table *, tbl_item_func *);
   This code is included in *Note 16::.

   * tbl_create(): Creates and returns a new, empty table as a
     struct tbl_table *.  The table is associated with the given
     arguments.  The void * argument is passed as the third argument to
     the comparison function when it is called.  If the allocator is a
     null pointer, then tbl_allocator_default is used.

   * tbl_destroy(): Destroys a table.  During destruction, the
     tbl_item_func provided, if non-null, is called once for every item
     in the table, in no particular order.  The function, if provided,
     must not invoke any table function or macro on the table being
     destroyed.

   * tbl_copy(): Creates and returns a new table with the same contents
     as the existing table passed as its first argument. Its other three
     arguments may all be null pointers.

     If a tbl_copy_func is provided, then it is used to make a copy of
     each table item as it is inserted into the new table, in no
     particular order (a "deep copy" (*note deep copy::)).  Otherwise,
     the void * table items are copied verbatim (a "shallow copy"
     (*note shallow copy::)).

     If the table copy fails, either due to memory allocation failure
     or a null pointer returned by the tbl_copy_func, tbl_copy()
     returns a null pointer.  In this case, any provided tbl_item_func
     is called once for each new item already copied, in no particular
     order.

     By default, the new table uses the same memory allocator as the
     existing one.  If non-null, the struct libavl_allocator * given is
     used instead as the new memory allocator.  To use the
     tbl_allocator_default allocator, specify &tbl_allocator_default
     explicitly.


File: libavl.info,  Node: Count,  Next: Insertion and Deletion,  Prev: Creation and Destruction,  Up: The Table ADT

2.7 Count
=========

This function returns the number of items currently in a table.

10. <Table count function prototype 10> =
size_t tbl_count (const struct tbl_table *);

The actual tables instead use a macro for implementation.

Exercises:

1. Implement tbl_count() as a macro, on the assumption that struct
tbl_table keeps the number of items in the table in a size_t member
named tbl_count.  [*Note answer: 2-7#1..]


File: libavl.info,  Node: Insertion and Deletion,  Next: Assertions,  Prev: Count,  Up: The Table ADT

2.8 Insertion and Deletion
==========================

These functions insert and delete items in tables.  There is also a
function for searching a table without modifying it.

   The design behind the insertion functions takes into account a
couple of important issues:

   * What should happen if there is a matching item already in the
     tree?  If the items contain only keys and no values, then there's
     no point in doing anything.  If the items do contain values, then
     we might want to leave the existing item or replace it, depending
     on the particular circumstances.  The tbl_insert() and
     tbl_replace() functions are handy in simple cases like these.

   * Occasionally it is convenient to insert one item into a table, then
     immediately replace it by a different item that has identical key
     data.  For instance, if there is a good chance that a data item
     already exists within a table, then it might make sense to insert
     data allocated as a local variable into a table, then replace it
     by a dynamically allocated copy if it turned out that the item
     wasn't already in the table.  That way, we save the time required
     to make an additional copy of the item to insert.  The tbl_probe()
     function allows for this kind of flexibility.

11. <Table insertion and deletion function prototypes 11> =
void **tbl_probe (struct tbl_table *, void *);
void *tbl_insert (struct tbl_table *, void *);
void *tbl_replace (struct tbl_table *, void *);
void *tbl_delete (struct tbl_table *, const void *);
void *tbl_find (const struct tbl_table *, const void *);
   This code is included in *Note 16::.

Each of these functions takes a table to manipulate as its first
argument and a table item as its second argument, here called table and
item, respectively.  Both arguments must be non-null in all cases.  All
but tbl_probe() return a table item or a null pointer.

   * tbl_probe(): Searches in table for an item matching item.  If
     found, a pointer to the void * data item is returned.  Otherwise,
     item is inserted into the table and a pointer to the copy within
     the table is returned.  Memory allocation failure causes a null
     pointer to be returned.

     The pointer returned can be used to replace the item found or
     inserted by a different item.  This must only be done if the
     replacement item has the same position relative to the other items
     in the table as did the original item.  That is, for existing item
     e, replacement item r, and the table's comparison function f(),
     the return values of f(e, x) and f(r, x) must have the same sign
     for every other item x currently in the table.  Calling any other
     table function invalidates the pointer returned and it must not be
     referenced subsequently.

   * tbl_insert(): Inserts item into table, but not if a matching item
     exists.  Returns a null pointer if successful or if a memory
     allocation error occurs.  If a matching item already exists in the
     table, returns that item.

   * tbl_replace(): Inserts item into table, replacing and returning
     any matching item.  Returns a null pointer if the item was
     inserted but there was no matching item to replace, or if a memory
     allocation error occurs.

   * tbl_delete(): Removes from table and returns an item matching
     item.  Returns a null pointer if no matching item exists in the
     table.

   * tbl_find(): Searches table for an item matching item and returns
     any item found.  Returns a null pointer if no matching item exists
     in the table.

Exercises:

1. Functions tbl_insert() and tbl_replace() return NULL in two very
different situations: an error or successful insertion.  Why is this not
necessarily a design mistake?  [*Note answer: 2-8#1..]

2. Suggest a reason for disallowing insertion of a null item.  [*Note
answer: 2-8#2..]

3. Write generic implementations of tbl_insert() and tbl_replace() in
terms of tbl_probe().  [*Note answer: 2-8#3..]


File: libavl.info,  Node: Assertions,  Next: Traversers,  Prev: Insertion and Deletion,  Up: The Table ADT

2.9 Assertions
==============

Sometimes an insertion or deletion must succeed because it is known in
advance that there is no way that it can fail.  For instance, we might
be inserting into a table from a list of items known to be unique, using
a memory allocator that cannot return a null pointer.  In this case, we
want to make sure that the operation succeeded, and abort if not,
because that indicates a program bug.  We also would like to be able to
turn off these tests for success in our production versions, because we
don't want them slowing down the code.

12. <Table assertion function prototypes 12> =
void tbl_assert_insert (struct tbl_table *, void *);
void *tbl_assert_delete (struct tbl_table *, void *);
   This code is included in *Note 16::.

   These functions provide assertions for tbl_insert() and
tbl_delete().  They expand, via macros, directly into calls to those
functions when NDEBUG, the same symbol used to turn off assert()
checks, is declared.  As for the standard C header <assert.h>, header
files for tables may be included multiple times in order to turn these
assertions on or off.

Exercises:

1. Write a set of preprocessor directives for a table header file that
implement the behavior described in the final paragraph above.  [*Note
answer: 2-9#1..]

2. Write a generic implementation of tbl_assert_insert() and
tbl_assert_delete() in terms of existing table functions.  Consider the
base functions carefully.  Why must we make sure that assertions are
always enabled for these functions?  [*Note answer: 2-9#2..]

3. Why must tbl_assert_insert() not be used if the table's memory
allocator can fail?  (See also Exercise 2.8-1.)  [*Note answer:
2-9#3..]


File: libavl.info,  Node: Traversers,  Next: Table Headers,  Prev: Assertions,  Up: The Table ADT

2.10 Traversers
===============

A struct tbl_traverser is a table "traverser" that allows the items in
a table to be examined.  With a traverser, the items within a table can
be enumerated in sorted ascending or descending order, starting from
either end or from somewhere in the middle.

   The user of the traverser declares its own instance of struct
tbl_traverser, typically as a local variable.  One of the traverser
constructor functions described below can be used to initialize it.
Until then, the traverser is invalid.  An invalid traverser must not be
passed to any traverser function other than a constructor.

   Seen from the viewpoint of a table user, a traverser has only one
attribute: the current item.  The current item is either an item in the
table or the "null item", represented by a null pointer and not
associated with any item.

   Traversers continue to work when their tables are modified.  Any
number of insertions and deletions may occur in the table without
affecting the current item selected by a traverser, with only a few
exceptions:

   * Deleting a traverser's current item from its table invalidates the
     traverser (even if the item is later re-inserted).

   * Using the return value of tbl_probe() to replace an item in the
     table invalidates all traversers with that item current, unless the
     replacement item has the same key data as the original item (that
     is, the table's comparison function returns 0 when the two items
     are compared).

   * Similarly, tbl_t_replace() invalidates all _other_ traversers with
     the same item selected, unless the replacement item has the same
     key data.

   * Destroying a table with tbl_destroy() invalidates all of that
     table's traversers.

   There is no need to destroy a traverser that is no longer needed.  An
unneeded traverser can simply be abandoned.

* Menu:

* Constructors::
* Manipulators::


File: libavl.info,  Node: Constructors,  Next: Manipulators,  Prev: Traversers,  Up: Traversers

2.10.1 Constructors
-------------------

These functions initialize traversers.  A traverser must be initialized
with one of these functions before it is passed to any other traverser
function.

13. <Traverser constructor function prototypes 13> =
/* Table traverser functions. */
void tbl_t_init (struct tbl_traverser *, struct tbl_table *);
void *tbl_t_first (struct tbl_traverser *, struct tbl_table *);
void *tbl_t_last (struct tbl_traverser *, struct tbl_table *);
void *tbl_t_find (struct tbl_traverser *, struct tbl_table *, void *);
void *tbl_t_insert (struct tbl_traverser *, struct tbl_table *, void *);
void *tbl_t_copy (struct tbl_traverser *, const struct tbl_traverser *);
   This code is included in *Note 16::.

All of these functions take a traverser to initialize as their first
argument, and most take a table to associate the traverser with as their
second argument.  These arguments are here called trav and table.  All,
except tbl_t_init(), return the item to which trav is initialized,
using a null pointer to represent the null item.  None of the arguments
to these functions may ever be a null pointer.

   * tbl_t_init(): Initializes trav to the null item in table.

   * tbl_t_first(): Initializes trav to the least-valued item in table.
     If the table is empty, then trav is initialized to the null item.

   * tbl_t_last(): Same as tbl_t_first(), for the greatest-valued item
     in table.

   * tbl_t_find(): Searches table for an item matching the one given.
     If one is found, initializes trav with it.  If none is found,
     initializes trav to the null item.

   * tbl_t_insert(): Attempts to insert the given item into table.  If
     it is inserted succesfully, trav is initialized to its location.
     If it cannot be inserted because of a duplicate, the duplicate
     item is set as trav's current item.  If there is a memory
     allocation error, trav is initialized to the null item.

   * tbl_t_copy(): Initializes trav to the same table and item as a
     second valid traverser.  Both arguments pointing to the same valid
     traverser is valid and causes no change in either.


File: libavl.info,  Node: Manipulators,  Prev: Constructors,  Up: Traversers

2.10.2 Manipulators
-------------------

These functions manipulate valid traversers.

14. <Traverser manipulator function prototypes 14> =
void *tbl_t_next (struct tbl_traverser *);
void *tbl_t_prev (struct tbl_traverser *);
void *tbl_t_cur (struct tbl_traverser *);
void *tbl_t_replace (struct tbl_traverser *, void *);
   This code is included in *Note 16::.

   Each of these functions takes a valid traverser, here called trav, as
its first argument, and returns a data item.  All but tbl_t_replace()
can also return a null pointer that represents the null item.  All
arguments to these functions must be non-null pointers.

   * tbl_t_next(): Advances trav to the next larger item in its table.
     If trav was at the null item in a nonempty table, then the smallest
     item in the table becomes current. If trav was already at the
     greatest item in its table or the table is empty, the null item
     becomes current.  Returns the new current item.

   * tbl_t_prev(): Advances trav to the next smaller item in its table.
     If trav was at the null item in a nonempty table, then the greatest
     item in the table becomes current. If trav was already at the
     lowest item in the table or the table is empty, the null item
     becomes current.  Returns the new current item.

   * tbl_t_cur(): Returns trav's current item.

   * tbl_t_replace(): Replaces the data item currently selected in trav
     by the one provided.  The replacement item is subject to the same
     restrictions as for the same replacement using tbl_probe().  The
     item replaced is returned.  If the null item is current, the
     behavior is undefined.

   Seen from the outside, the traverser treats the table as a circular
arrangement of items, with the null item at the top of the circle and
the least-valued item just clockwise of it, then the next-lowest-valued
item, and so on until the greatest-valued item is just counterclockwise
of the null item.  Moving clockwise in the circle is equivalent, under
our traverser, to moving to the next item with tbl_t_next().  Moving
counterclockwise is equivalent to moving to the previous item with
tbl_t_prev().

   An equivalent view is that the traverser treats the table as a linear
arrangement of nodes:

 [image src="trav-line.png" text="	 .-> 1 <-> 2 <-> 3 <-> 4 <-> 5 <-> 6 <-> 7 <-> 8 <-.
         |                                                |
         `---------------------> NULL <--------------------'
" ]

From this perspective, nodes are arranged from least to greatest in
left to right order, and the null node lies in the middle as a
connection between the least and greatest nodes.  Moving to the next
node is the same as moving to the right and moving to the previous node
is motion to the left, except where the null node is concerned.


File: libavl.info,  Node: Table Headers,  Next: Additional Exercises for Tables,  Prev: Traversers,  Up: The Table ADT

2.11 Table Headers
==================

Here we gather together in one place all of the types and prototypes for
a generic table.

15. <Table types 15> =
<*Note Table function types: 3.>
<*Note Memory allocator: 6.>
<*Note Default memory allocator header: 8.>
   This code is included in *Note 25::, *Note 143::, *Note 194::, *Note
249::, *Note 299::, *Note 335::, *Note 374::, *Note 417::, *Note 454::,
*Note 488::, *Note 521::, and *Note 553::.

16. <Table function prototypes 16> =
<*Note Table creation function prototypes: 9.>
<*Note Table insertion and deletion function prototypes: 11.>
<*Note Table assertion function prototypes: 12.>
<*Note Table count macro: 593.>
<*Note Traverser constructor function prototypes: 13.>
<*Note Traverser manipulator function prototypes: 14.>
   This code is included in *Note 25::, *Note 143::, *Note 194::, *Note
249::, *Note 299::, *Note 335::, *Note 374::, *Note 417::, *Note 454::,
*Note 488::, *Note 521::, and *Note 553::.

All of our tables fit the specification given in Exercise 2.7-1, so
<*Note Table count macro: 593.> is directly included above.


File: libavl.info,  Node: Additional Exercises for Tables,  Prev: Table Headers,  Up: The Table ADT

2.12 Additional Exercises
=========================

Exercises:

*1. Compare and contrast the design of libavl's tables with that of the
set container in the C++ Standard Template Library.  [*Note answer:
2-12#1..]

2. What is the smallest set of table routines such that all of the other
routines can be implemented in terms of the interfaces of that set as
defined above?  [*Note answer: 2-12#2..]


File: libavl.info,  Node: Search Algorithms,  Next: Binary Search Trees,  Prev: The Table ADT,  Up: Top

3 Search Algorithms
*******************

In libavl, we are primarily concerned with binary search trees and
balanced binary trees.  If you're already familiar with these concepts,
then you can move right into the code, starting from the next chapter.
But if you're not, then a little motivation and an explanation of
exactly what a binary search tree is can't hurt.  That's the goal of
this chapter.

   More particularly, this chapter concerns itself with algorithms for
searching.  Searching is one of the core problems in organizing a table.
As it will turn out, arranging a table for fast searching also
facilitates some other table features.

* Menu:

* Sequential Search::
* Sequential Search with Sentinel::
* Sequential Search of Ordered Array::
* Sequential Search of Ordered Array with Sentinel::
* Binary Search of Ordered Array::
* Binary Search Tree in Array::
* Dynamic Lists::


File: libavl.info,  Node: Sequential Search,  Next: Sequential Search with Sentinel,  Prev: Search Algorithms,  Up: Search Algorithms

3.1 Sequential Search
=====================

Suppose that you have a bunch of things (books, magazines, CDs, ...)
in a pile, and you're looking for one of them.  You'd probably start by
looking at the item at the top of the pile to check whether it was the
one you were looking for.  If it wasn't, you'd check the next item down
the pile, and so on, until you either found the one you wanted or ran
out of items.

   In computer science terminology, this is a "sequential search"
(*note sequential search::).  It is easy to implement sequential search
for an array or a linked list.  If, for the moment, we limit ourselves
to items of type int, we can write a function to sequentially search an
array like this:

17. <Sequentially search an array of ints 17> =
/* Returns the smallest i such that array[i] == key,
   or -1 if key is not in array[].
   array[] must be an array of n ints. */
int
seq_search (int array[], int n, int key)
{
  int i;

  for (i = 0; i < n; i++)
    if (array[i] == key)
      return i;
  return -1;
}
   This code is included in *Note 597:: and *Note 602::.

   We can hardly hope to improve on the data requirements, space, or
complexity of simple sequential search, as they're about as good as we
can want.  But the speed of sequential search leaves something to be
desired.  The next section describes a simple modification of the
sequential search algorithm that can sometimes lead to big improvements
in performance.

See also:  *Note Knuth 1998b::, algorithm 6.1S; *Note Kernighan 1976::,
section 8.2; *Note Cormen 1990::, section 11.2; *Note Bentley 2000::,
sections 9.2 and 13.2, appendix 1.

Exercises:

1. Write a simple test framework for seq_search().  It should read
sample data from stdin and collect them into an array, then search for
each item in the array in turn and compare the results to those
expected, reporting any discrepancies on stdout and exiting with an
appropriate return value.  You need not allow for the possibility of
duplicate input values and may limit the maximum number of input values.
[*Note answer: 3-1#1..]


File: libavl.info,  Node: Sequential Search with Sentinel,  Next: Sequential Search of Ordered Array,  Prev: Sequential Search,  Up: Search Algorithms

3.2 Sequential Search with Sentinel
===================================

Try to think of some ways to improve the speed of sequential search.  It
should be clear that, to speed up a program, it pays to concentrate on
the parts that use the most time to begin with.  In this case, it's the
loop.

   Consider what happens each time through the loop:

  1. The loop counter i is incremented and compared against n.

  2. array[i] is compared against key.

   If we could somehow eliminate one of these comparisons, the loop
might be a lot faster.  So, let's try... why do we need step 1?  It's
because, otherwise, we might run off the end of array[], causing
undefined behavior, which is in turn because we aren't sure that key is
in array[].  If we knew that key was in array[], then we could skip
step 1.

   But, hey! we _can_ ensure that the item we're looking for is in the
array.  How?  By putting a copy of it at the end of the array.  This
copy is called a "sentinel" (*note sentinel::), and the search
technique as a whole is called "sequential search with sentinel" (*note
sequential search with sentinel::).  Here's the code:

18. <Sequentially search an array of ints using a sentinel 18> =
/* Returns the smallest i such that array[i] == key,
   or -1 if key is not in array[].
   array[] must be an modifiable array of n ints
   with room for a (n + 1)th element. */
int
seq_sentinel_search (int array[], int n, int key)
{
  int *p;

  array[n] = key;
  for (p = array; *p != key; p++)
    /* Nothing to do. */;
  return p - array < n ? p - array : -1;
}
   This code is included in *Note 602::.

   Notice how the code above uses a pointer, int *p, rather than a
counter i as in <*Note Sequentially search an array of ints: 17.>
earlier.  For the most part, this is simply a style preference: for
iterating through an array, C programmers usually prefer pointers to
array indexes.  Under older compilers, code using pointers often
compiled into faster code as well, but modern C compilers usually
produce the same code whether pointers or indexes are used.

   The return statement in this function uses two somewhat advanced
features of C: the conditional or "ternary" operator ?: and pointer
arithmetic.  The former is a bit like an expression form of an if
statement.  The expression a ? b : c first evaluates a.  Then, if
a != 0, b is evaluated and the expression takes that value.  Otherwise,
a == 0, c is evaluated, and the result is the expression's value.

   Pointer arithmetic is used in two ways here.  First, the expression
p++ acts to advance p to point to the next int in array.  This is
analogous to the way that i++ would increase the value of an integer or
floating point variable i by one.  Second, the expression p - array
results in the "difference" between p and array, i.e., the number of
int elements between the locations to which they point.  For more
information on these topics, please consult a good C reference, such as
*Note Kernighan 1988::.

   Searching with a sentinel requires that the array be modifiable and
large enough to hold an extra element.  Sometimes these are inherently
problematic--the array may not be modifiable or it might be too
small--and sometimes they are problems because of external
circumstances.  For instance, a program with more than one concurrent
"thread" (*note thread::) cannot modify a shared array for sentinel
search without expensive locking.

   Sequential sentinel search is an improvement on ordinary sequential
search, but as it turns out there's still room for
improvement--especially in the runtime for unsuccessful searches, which
still always take n comparisons.  In the next section, we'll see one
technique that can reduce the time required for unsuccessful searches,
at the cost of longer runtime for successful searches.

See also:  *Note Knuth 1998b::, algorithm 6.1Q; *Note Cormen 1990::,
section 11.2; *Note Bentley 2000::, section 9.2.


File: libavl.info,  Node: Sequential Search of Ordered Array,  Next: Sequential Search of Ordered Array with Sentinel,  Prev: Sequential Search with Sentinel,  Up: Search Algorithms

3.3 Sequential Search of Ordered Array
======================================

Let's jump back to the pile-of-things analogy from the beginning of this
chapter (*note Sequential Search::).  This time, suppose that instead of
being in random order, the pile you're searching through is ordered on
the property that you're examining; e.g., magazines sorted by
publication date, if you're looking for, say, the July 1988 issue.

   Think about how this would simplify searching through the pile.  Now
you can sometimes tell that the magazine you're looking for isn't in the
pile before you get to the bottom, because it's not between the
magazines that it otherwise would be.  On the other hand, you still
might have to go through the entire pile if the magazine you're looking
for is newer than the newest magazine in the pile (or older than the
oldest, depending on the ordering that you chose).

   Back in the world of computers, we can apply the same idea to
searching a sorted array:

19. <Sequentially search a sorted array of ints 19> =
/* Returns the smallest i such that array[i] == key,
   or -1 if key is not in array[].
   array[] must be an array of n ints sorted in ascending order. */
int
seq_sorted_search (int array[], int n, int key)
{
  int i;

  for (i = 0; i < n; i++)
    if (key <= array[i])
      return key == array[i] ? i : -1;

  return -1;
}
   This code is included in *Note 602::.

   At first it might be a little tricky to see exactly how
seq_sorted_search() works, so we'll work through a few examples.
Suppose that array[] has the four elements {3, 5, 6, 8}, so that n is
4.  If key is 6, then the first time through the loop the if condition
is 6 <= 3, or false, so the loop repeats with i == 1.  The second time
through the loop we again have a false condition, 6 <= 5, and the loop
repeats again.  The third time the if condition, 6 <= 6, is true, so
control passes to the if statement's dependent return.  This return
verifies that 6 == 6 and returns i, or 2, as the function's value.

   On the other hand, suppose key is 4, a value not in array[].  For
the first iteration, when i is 0, the if condition, 4 <= 3, is false,
but in the second iteration we have 4 <= 5, which is true.  However,
this time key == array[i] is 4 == 5, or false, so -1 is returned.

See also:  *Note Sedgewick 1998::, program 12.4.


File: libavl.info,  Node: Sequential Search of Ordered Array with Sentinel,  Next: Binary Search of Ordered Array,  Prev: Sequential Search of Ordered Array,  Up: Search Algorithms

3.4 Sequential Search of Ordered Array with Sentinel
====================================================

When we implemented sequential search in a sorted array, we lost the
benefits of having a sentinel.  But we can reintroduce a sentinel in the
same way we did before, and obtain some of the same benefits.  It's
pretty clear how to proceed:

20. <Sequentially search a sorted array of ints using a sentinel 20> =
/* Returns the smallest i such that array[i] == key,
   or -1 if key is not in array[].
   array[] must be an modifiable array of n ints,
   sorted in ascending order,
   with room for a (n + 1)th element at the end. */
int
seq_sorted_sentinel_search (int array[], int n, int key)
{
  int *p;

  array[n] = key;
  for (p = array; *p < key; p++)
    /* Nothing to do. */;
  return p - array < n && *p == key ? p - array : -1;
}
   This code is included in *Note 602::.

   With a bit of additional cleverness we can eliminate one objection to
this sentinel approach.  Suppose that instead of using the value being
searched for as the sentinel value, we used the maximum possible value
for the type in question.  If we did this, then we could use almost the
same code for searching the array.

   The advantage of this approach is that there would be no need to
modify the array in order to search for different values, because the
sentinel is the same value for all searches.  This eliminates the
potential problem of searching an array in multiple contexts, due to
nested searches, threads, or signals, for instance.  (In the code
below, we will still put the sentinel into the array, because our
generic test program won't know to put it in for us in advance, but in
real-world code we could avoid the assignment.)

   We can easily write code for implementation of this technique:

21. <Sequentially search a sorted array of ints using a sentinel (2) 21> =
/* Returns the smallest i such that array[i] == key,
   or -1 if key is not in array[].
   array[] must be an array of n ints,
   sorted in ascending order,
   with room for an (n + 1)th element to set to INT_MAX. */
int
seq_sorted_sentinel_search_2 (int array[], int n, int key)
{
  int *p;

  array[n] = INT_MAX;
  for (p = array; *p < key; p++)
    /* Nothing to do. */;
  return p - array < n && *p == key ? p - array : -1;
}
   This code is included in *Note 602::.

Exercises:

1. When can't the largest possible value for the type be used as a
sentinel?  [*Note answer: 3-4#1..]


File: libavl.info,  Node: Binary Search of Ordered Array,  Next: Binary Search Tree in Array,  Prev: Sequential Search of Ordered Array with Sentinel,  Up: Search Algorithms

3.5 Binary Search of Ordered Array
==================================

At this point we've squeezed just about all the performance we can out
of sequential search in portable C.  For an algorithm that searches
faster than our final refinement of sequential search, we'll have to
reconsider our entire approach.

   What's the fundamental idea behind sequential search?  It's that we
examine array elements in order.  That's a fundamental limitation: if
we're looking for an element in the middle of the array, we have to
examine every element that comes before it.  If a search algorithm is
going to be faster than sequential search, it will have to look at fewer
elements.

   One way to look at search algorithms based on repeated comparisons
is to consider what we learn about the array's content at each step.
Suppose that array[] has n elements in sorted order, without duplicates,
that array[j] contains key, and that we are trying to learn the value
j.  In sequential search, we learn only a little about the data set
from each comparison with array[i]: either key == array[i] so that i ==
j, or key != array[i] so that i != j and therefore j > i.  As a result,
we eliminate only one possibility at each step.

   Suppose that we haven't made any comparisons yet, so that we know
nothing about the contents of array[].  If we compare key to array[i]
for arbitrary i such that 0 <= i < n, what do we learn?  There are
three possibilities:

   * key < array[i]: Now we know that key < array[i] < array[i + 1] <
     ...  < array[n - 1].(1) Therefore, 0 <= j < i.

   * key == array[i]: We're done: j == i.

   * key > array[i]: Now we know that key > array[i] > array[i - 1] >
     ...  > array[0].  Therefore, i < j < n.

   So, after one step, if we're not done, we know that j > i or that j
< i.  If we're equally likely to be looking for each element in
array[], then the best choice of i is n / 2: for that value, we
eliminate about half of the possibilities either way.  (If n is odd,
we'll round down.)

   After the first step, we're back to essentially the same situation:
we know that key is in array[j] for some j in a range of about n / 2.
So we can repeat the same process.  Eventually, we will either find key
and thus j, or we will eliminate all the possibilities.

   Let's try an example.  For simplicity, let array[] contain the values
100 through 114 in numerical order, so that array[i] is 100 + i and n
is 15.  Suppose further that key is 110.  The steps that we'd go
through to find j are described below.  At each step, the facts are
listed: the known range that j can take, the selected value of i, the
results of comparing key to array[i], and what was learned from the
comparison.

  1. 0 <= j <= 14: i becomes (0 + 14) / 2 == 7. 110 > array[i] == 107,
     so now we know that j > 7.

  2. 8 <= j <= 14: i becomes (8 + 14) / 2 == 11. 110 < array[i] == 111,
     so now we know that j < 11.

  3. 8 <= j <= 10: i becomes (8 + 10) / 2 == 9. 110 > array[i] == 109,
     so now we know that j > 9.

  4. 10 <= j <= 10: i becomes (10 + 10) / 2 == 10.  110 == array[i] ==
     110, so we're done and i == j == 10.

   In case you hadn't yet figured it out, this technique is called
"binary search" (*note binary search::).  We can make an initial C
implementation pretty easily:

22. <Binary search of ordered array 22> =
/* Returns the offset within array[] of an element equal to key,
   or -1 if key is not in array[].
   array[] must be an array of n ints sorted in ascending order. */
int
binary_search (int array[], int n, int key)
{
  int min = 0;
  int max = n - 1;

  while (max >= min)
    {
      int i = (min + max) / 2;
      if (key < array[i])
        max = i - 1;
      else if (key > array[i])
        min = i + 1;
      else
        return i;
    }

  return -1;
}
   This code is included in *Note 602::.

   The maximum number of comparisons for a binary search in an array of
n elements is about log2(n), as opposed to a maximum of n comparisons
for sequential search.  For moderate to large values of n, this is a
lot better.

   On the other hand, for small values of n, binary search may actually
be slower because it is more complicated than sequential search.  We
also have to put our array in sorted order before we can use binary
search.  Efficiently sorting an n-element array takes time proportional
to n * log2(n) for large n.  So binary search is preferred if n is
large enough (see the answer to Exercise 4 for one typical value) and if
we are going to do enough searches to justify the cost of the initial
sort.

   Further small refinements are possible on binary search of an ordered
array.  Try some of the exercises below for more information.

See also:  *Note Knuth 1998b::, algorithm 6.2.1B; *Note Kernighan
1988::, section 3.3; *Note Bentley 2000::, chapters 4 and 5, section
9.3, appendix 1; *Note Sedgewick 1998::, program 12.6.

Exercises:

1. Function binary_search() above uses three local variables: min and
max for the ends of the remaining search range and i for its midpoint.
Write and test a binary search function that uses only two variables: i
for the midpoint as before and m representing the width of the range on
either side of i.  You may require the existence of a dummy element
just before the beginning of the array.  Be sure, if so, to specify
what its value should be.  [*Note answer: 3-5#1..]

2. The standard C library provides a function, bsearch(), for searching
ordered arrays.  Commonly, bsearch() is implemented as a binary search,
though ANSI C does not require it.  Do the following:

  a. Write a function compatible with the interface for binary_search()
     that uses bsearch() "under the hood."  You'll also have to write an
     additional callback function for use by bsearch().

  b. Write and test your own version of bsearch(), implementing it
     using a binary search.  (Use a different name to avoid conflicts
     with the C library.)
        [*Note answer: 3-5#2..]

3. An earlier exercise presented a simple test framework for
seq_search(), but now we have more search functions.  Write a test
framework that will handle all of them presented so far.  Add code for
timing successful and unsuccessful searches.  Let the user specify, on
the command line, the algorithm to use, the size of the array to search,
and the number of search iterations to run.  [*Note answer:
3-5#3..]

4. Run the test framework from the previous exercise on your own system
for each algorithm.  Try different array sizes and compiler optimization
levels.  Be sure to use enough iterations to make the searches take at
least a few seconds each.  Analyze the results: do they make sense?
Try to explain any apparent discrepancies.  [*Note answer:
3-5#4..]

   ---------- Footnotes ----------

   (1) This sort of notation means very different things in C and
mathematics.  In mathematics, writing a < b < c asserts both of the
relations a < b and b < c, whereas in C, it expresses the evaluation of
a < b, then the comparison of the 0 or 1 result to the value of c.  In
mathematics this notation is invaluable, but in C it is rarely
meaningful.  As a result, this book uses this notation only in the
mathematical sense.


File: libavl.info,  Node: Binary Search Tree in Array,  Next: Dynamic Lists,  Prev: Binary Search of Ordered Array,  Up: Search Algorithms

3.6 Binary Search Tree in Array
===============================

Binary search is pretty fast.  Suppose that we wish to speed it up
anyhow.  Then, the obvious speed-up targets in <*Note Binary search of
ordered array: 22.> above are the while condition and the calculations
determining values of i, min, and max.  If we could eliminate these,
we'd have an incrementally faster technique, all else being equal.  And,
as it turns out, we _can_ eliminate both of them, the former by use of
a sentinel and the latter by precalculation.

   Let's consider precalculating i, min, and max first.  Think about
the nature of the choices that binary search makes at each step.
Specifically, in <*Note Binary search of ordered array: 22.> above,
consider the dependence of min and max upon i.  Is it ever possible for
min and max to have different values for the same i and n?

   The answer is no.  For any given i and n, min and max are fixed.
This is important because it means that we can represent the entire
"state" of a binary search of an n-element array by the single variable
i.  In other words, if we know i and n, we know all the choices that
have been made to this point and we know the two possible choices of i
for the next step.

   This is the key insight in eliminating calculations.  We can use an
array in which the items are labeled with the next two possible choices.

   An example is indicated.  Let's continue with our example of an array
containing the 16 integers 100 to 115.  We define an entry in the array
to contain the item value and the array index of the item to examine
next for search values smaller and larger than the item:

23. <Binary search tree entry 23> =
/* One entry in a binary search tree stored in an array. */
struct binary_tree_entry
  {
    int value;          /* This item in the binary search tree. */
    int smaller;        /* Array index of next item for smaller targets. */
    int larger;         /* Array index of next item for larger targets. */
  };
   This code is included in *Note 619::.

   Of course, it's necessary to fill in the values for smaller and
larger.  A few moments' reflection should allow you to figure out one
method for doing so.  Here's the full array, for reference:

const struct binary_tree_entry bins[16] =
  {
    {100, 15, 15},
    {101, 0, 2},
    {102, 15, 15},
    {103, 1, 5},
    {104, 15, 15},
    {105, 4, 6},
    {106, 15, 15},
    {107, 3, 11},
    {108, 15, 15},
    {109, 8, 10},
    {110, 15, 15},
    {111, 9, 13},
    {112, 15, 15},
    {113, 12, 14},
    {114, 15, 15},
    {0, 0, 0},
  };

   For now, consider only bins[]'s first 15 rows.  Within these rows,
the first column is value, the item value, and the second and third
columns are smaller and larger, respectively.  Values 0 through 14 for
smaller and larger indicate the index of the next element of bins[] to
examine.  Value 15 indicates "element not found".  Element array[15] is
not used for storing data.

   Try searching for key == 110 in bins[], starting from element 7, the
midpoint:

  1. i == 7: 110 > bins[i].value == 107, so let i = bins[i].larger, or
     11.

  2. i == 11: 110 < bins[i].value == 111, so let i = bins[i].smaller,
     or 10.

  3. i == 10: 110 == bins[i].value == 110, so we're done.

   We can implement this search in C code.  The function uses the
common C idiom of writing for (;;) for an "infinite" loop:

24. <Search of binary search tree stored as array 24> =
/* Returns i such that array[i].value == key,
   or -1 if key is not in array[].
   array[] is an array of n elements forming a binary search tree,
   with its root at array[n / 2],
   and space for an (n + 1)th value at the end. */
int
binary_search_tree_array (struct binary_tree_entry array[], int n,
                          int key)
{
  int i = n / 2;

  array[n].value = key;
  for (;;)
    if (key > array[i].value)
      i = array[i].larger;
    else if (key < array[i].value)
      i = array[i].smaller;
    else
      return i != n ? i : -1;
}
   This code is included in *Note 619::.

   Examination of the code above should reveal the purpose of bins[15].
It is used as a sentinel value, allowing the search to always terminate
without the use of an extra test on each loop iteration.

   The result of augmenting binary search with "pointer" values like
smaller and larger is called a "binary search tree" (*note binary
search tree::).

Exercises:

1. Write a function to automatically initialize smaller and larger
within bins[].  [*Note answer: 3-6#1..]

2. Write a simple automatic test program for binary_search_tree_array().
Let the user specify the size of the array to test on the command line.
You may want to use your results from the previous exercise.  [*Note
answer: 3-6#2..]


File: libavl.info,  Node: Dynamic Lists,  Prev: Binary Search Tree in Array,  Up: Search Algorithms

3.7 Dynamic Lists
=================

Up until now, we've considered only lists whose contents are fixed and
unchanging, that is, "static" (*note static::) lists.  But in real
programs, many lists are "dynamic" (*note dynamic::), with their
contents changing rapidly and unpredictably.  For the case of dynamic
lists, we need to reconsider some of the attributes of the types of
lists that we've examined.(1)

   Specifically, we want to know how long it takes to insert a new
element into a list and to remove an existing element from a list.
Think about it for each type of list examined so far:

Unordered array
     Adding items to the list is easy and fast, unless the array grows
     too large for the block and has to be copied into a new area of
     memory. Just copy the new item to the end of the list and increase
     the size by one.

     Removing an item from the list is almost as simple. If the item to
     delete happens to be located at the very end of the array, just
     reduce the size of the list by one. If it's located at any other
     spot, you must also copy the element that is located at the very
     end onto the location that the deleted element used to occupy.

Ordered array
     In terms of inserting and removing elements, ordered arrays are
     mechanically the same as unordered arrays.  The difference is that
     insertions and deletions can only be at one end of the array if
     the item in question is the largest or smallest in the list.  The
     practical upshot is that dynamic ordered arrays are only efficient
     if items are added and removed in sorted order.

Binary search tree
     Insertions and deletions are where binary search trees have their
     chance to shine.  Insertions and deletions are efficient in binary
     search trees whether they're made at the beginning, middle, or end
     of the lists.

   Clearly, binary search trees are superior to ordered or unordered
arrays in situations that require insertion and deletion in random
positions.  But insertion and deletion operations in binary search
trees require a bit of explanation if you've never seen them before.
This is what the next chapter is for, so read on.

   ---------- Footnotes ----------

   (1) These uses of the words "static" and "dynamic" are different
from their meanings in the phrases "static allocation" and "dynamic
allocation."  *Note Glossary::, for more details.


File: libavl.info,  Node: Binary Search Trees,  Next: AVL Trees,  Prev: Search Algorithms,  Up: Top

4 Binary Search Trees
*********************

The previous chapter motivated the need for binary search trees.  This
chapter implements a table ADT backed by a binary search tree.  Along
the way, we'll see how binary search trees are constructed and
manipulated in abstract terms as well as in concrete C code.

   The library includes a header file <*Note bst.h: 25.> and an
implementation file <*Note bst.c: 26.>, outlined below.  We borrow most
of the header file from the generic table headers designed a couple of
chapters back, simply replacing tbl by bst, the prefix used in this
table module.

25. <bst.h 25> =
<*Note Library License: 1.>
#ifndef BST_H
#define BST_H 1

#include <stddef.h>

<*Note Table types; tbl => bst: 15.>
<*Note BST maximum height: 29.>
<*Note BST table structure: 28.>
<*Note BST node structure: 27.>
<*Note BST traverser structure: 62.>
<*Note Table function prototypes; tbl => bst: 16.>
<*Note BST extra function prototypes: 89.>

#endif /* bst.h */

<*Note Table assertion function control directives; tbl => bst: 595.>

26. <bst.c 26> =
<*Note Library License: 1.>
#include <assert.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include "bst.h"

<*Note BST operations: 30.>

Exercises:

1. What is the purpose of #ifndef BST_H ... #endif in <*Note bst.h:
25.> above?  [*Note answer: 4#1..]

* Menu:

* BST Vocabulary::
* BST Data Types::
* BST Rotations::
* BST Operations::
* Creating a BST::
* Searching a BST::
* Inserting into a BST::
* Deleting from a BST::
* Traversing a BST::
* Copying a BST::
* Destroying a BST::
* Balancing a BST::
* Joining BSTs::
* Testing BST Functions::
* Additional Exercises for BSTs::


File: libavl.info,  Node: BST Vocabulary,  Next: BST Data Types,  Prev: Binary Search Trees,  Up: Binary Search Trees

4.1 Vocabulary
==============

When binary search trees, or BSTs, were introduced in the previous
chapter, the reason that they were called binary search trees wasn't
explained.  The diagram below should help to clear up matters, and
incidentally let us define some BST-related vocabulary:

 [image src="100-114.png" text="                                   107
                       ____....---'   `---...___
                      103                       111
                __..-'   `._              __..-'   `._
               101          105          109          113
             _'   \\       _'   \\       _'   \\       _'   \\
            100    102   104    106   108    110   112    114

" ]

This diagram illustrates the binary search tree example from the
previous chapter.  The circle or "node" (*note node::) at the top,
labeled 107, is the starting point of any search.  As such, it is
called the "root" (*note root::) of the tree.  The node connected to it
below to the left, labeled 103, is the root's "left child" (*note left
child::), and node 111 to its lower right is its "right child" (*note
right child::).  A node's left child corresponds to smaller from the
array-based BST of the previous chapter, and a right child corresponds
to larger.

   Some nodes, such as 106 here, don't have any children.  Such a node
is called a "leaf" (*note leaf::) or "terminal node" (*note terminal
node::).  Although not shown here, it's also possible for a node to
have only one child, either on the left or the right side.  A node with
at least one child is called a "nonterminal node" (*note nonterminal
node::).

   Each node in a binary search tree is, conceptually, the root of its
own tree.  Such a tree is called a "subtree" (*note subtree::) of the
tree that contains it.  The left child of a node and recursively all of
that child's children is a subtree of the node, called the "left
subtree" (*note left subtree::) of the node.  The term "right subtree"
(*note right subtree::) is defined similarly for the right side of the
node.  For instance, above, nodes 104, 105, and 106 are the right
subtree of node 103, with 105 as the subtree's root.

   A BST without any nodes is called an "empty tree" (*note empty
tree::).  Both subtrees of all even-numbered nodes in the BST above are
empty trees.

   In a binary search tree, the left child of a node, if it exists, has
a smaller value than the node, and the right child of a node has a
larger value.  The more general term "binary tree" (*note binary
tree::), on the other hand, refers to a data structure with the same
form as a binary search tree, but which does not necessarily share this
property.  There are also related, but different, structures simply
called "trees".

   In this book, all our binary trees are binary search trees, and this
book will not discuss plain trees at all.  As a result, we will often
be a bit loose in terminology and use the term "binary tree" or "tree"
when "binary search tree" is the proper term.

   Although this book discusses binary search trees exclusively, it is
instructive to occasionally display, as a counterexample, a diagram of
a binary tree whose nodes are out of order and therefore not a BST.
Such diagrams are marked ** to reinforce their non-BST nature to the
casual browser.

See also:  *Note Knuth 1997::, section 2.3; *Note Knuth 1998b::,
section 6.2.2; *Note Cormen 1990::, section 13.1; *Note Sedgewick
1998::, section 5.4.

* Menu:

* Differing Definitions::


File: libavl.info,  Node: Differing Definitions,  Prev: BST Vocabulary,  Up: BST Vocabulary

4.1.1 Aside: Differing Definitions
----------------------------------

The definitions in the previous section are the ones used in this book.
They are the definitions that programmers often use in designing and
implementing real programs.  However, they are slightly different from
the definitions used in formal computer science textbooks.  This
section gives these formal definitions and contrasts them against our
own.

   The most important difference is in the definition of a binary tree
itself.  Formally, a binary tree is either an "external node" or an
"internal node" connected to a pair of binary trees called the internal
node's left subtree and right subtree.  Internal nodes correspond to
our notion of nodes, and external nodes correspond roughly to nodes'
empty left or right subtrees.  The generic term "node" includes both
internal and external nodes.

   Every internal node always has exactly two children, although those
children may be external nodes, so we must also revise definitions that
depend on a node's number of children.  Then, a "leaf" is an internal
node with two external node children and a "nonterminal node" is an
internal node at least one of whose children is an internal node.
Finally, an "empty tree" is a binary tree that contains of only an
external node.

   Tree diagrams in books that use these formal definitions show both
internal and external nodes.  Typically, internal nodes are shown as
circles, external nodes as square boxes.  Here's an example BST in the
format used in this book, shown alongside an identical BST in the
format used in formal computer science books:

 [image src="altdef.png" text="                                            4
                        4             __..-' `_
                       / \\           2         5
                      2   5       _.' `_      / \\
                      ^          1      3    []  []
                     1 3        / \\    / \\
                               []  [] []  []

" ]

See also:  *Note Sedgewick 1998::, section 5.4.


File: libavl.info,  Node: BST Data Types,  Next: BST Rotations,  Prev: BST Vocabulary,  Up: Binary Search Trees

4.2 Data Types
==============

The types for memory allocation and managing data as void * pointers
were discussed previously (*note The Table ADT::), but to build a table
implementation using BSTs we must define some additional types.  In
particular, we need struct bst_node to represent an individual node and
struct bst_table to represent an entire table.  The following sections
take care of this.

* Menu:

* BST Node Structure::
* BST Structure::
* BST Maximum Height::


File: libavl.info,  Node: BST Node Structure,  Next: BST Structure,  Prev: BST Data Types,  Up: BST Data Types

4.2.1 Node Structure
--------------------

When binary search trees were introduced in the last chapter, we used
indexes into an array to reference items' smaller and larger values.
But in C, BSTs are usually constructed using pointers.  This is a more
general technique, because pointers aren't restricted to references
within a single array.

27. <BST node structure 27> =
/* A binary search tree node. */
struct bst_node
  {
    struct bst_node *bst_link[2];   /* Subtrees. */
    void *bst_data;                 /* Pointer to data. */
  };
   This code is included in *Note 25::.

   In struct bst_node, bst_link[0] takes the place of smaller, and
bst_link[1] takes the place of larger.  If, in our array implementation
of binary search trees, either of these would have pointed to the
sentinel, it instead is assigned NULL, the null pointer constant.

   In addition, bst_data replaces value.  We use a void * generic
pointer here, instead of int as used in the last chapter, to let any
kind of data be stored in the BST.  *Note Comparison Function::, for
more information on void * pointers.


File: libavl.info,  Node: BST Structure,  Next: BST Maximum Height,  Prev: BST Node Structure,  Up: BST Data Types

4.2.2 Tree Structure
--------------------

The struct bst_table structure ties together all of the data needed to
keep track of a table implemented as a binary search tree:

28. <BST table structure 28> =
/* Tree data structure. */
struct bst_table
  {
    struct bst_node *bst_root;          /* Tree's root. */
    bst_comparison_func *bst_compare;   /* Comparison function. */
    void *bst_param;                    /* Extra argument to bst_compare. */
    struct libavl_allocator *bst_alloc; /* Memory allocator. */
    size_t bst_count;                   /* Number of items in tree. */
    unsigned long bst_generation;       /* Generation number. */
  };
   This code is included in *Note 25::, *Note 143::, and *Note 194::.

   Most of struct bst_table's members should be familiar.  Member
bst_root points to the root node of the BST.  Together, bst_compare and
bst_param specify how items are compared (*note Item and Copy
Functions::).  The members of bst_alloc specify how to allocate memory
for the BST (*note Memory Allocation::).  The number of items in the BST
is stored in bst_count (*note Count::).

   The final member, bst_generation, is a "generation number".  When a
tree is created, it starts out at zero.  After that, it is incremented
every time the tree is modified in a way that might disturb a traverser.
We'll talk more about the generation number later (*note Better
Iterative Traversal::).

Exercises:

*1. Why is it a good idea to include bst_count in struct bst_table?
Under what circumstances would it be better to omit it?  [*Note answer:
4-2-2#1..]


File: libavl.info,  Node: BST Maximum Height,  Prev: BST Structure,  Up: BST Data Types

4.2.3 Maximum Height
--------------------

For efficiency, some of the BST routines use a stack of a fixed maximum
height.  This maximum height affects the maximum number of nodes that
can be fully supported by libavl in any given tree, because a binary
tree of height n contains at most 2**n - 1 nodes.

   The BST_MAX_HEIGHT macro sets the maximum height of a BST.  The
default value of 64 allows for trees with up to 2**64 - 1.  On today's
common 32- and 64-bit computers, this is hardly a limit, because memory
would be exhausted long before the tree became too big.

   The BST routines that use fixed stacks also detect stack overflow and
call a routine to "balance" or restructure the tree in order to reduce
its height to the permissible range.  The limit on the BST height is
therefore not a severe restriction.

29. <BST maximum height 29> =
/* Maximum BST height. */
#ifndef BST_MAX_HEIGHT
#define BST_MAX_HEIGHT 32
#endif
   This code is included in *Note 25::, *Note 299::, *Note 417::, and
*Note 521::.

Exercises:

1. Suggest a reason why the BST_MAX_HEIGHT macro is defined
conditionally.  Are there any potential pitfalls?  [*Note answer:
4-2-3#1..]


File: libavl.info,  Node: BST Rotations,  Next: BST Operations,  Prev: BST Data Types,  Up: Binary Search Trees

4.3 Rotations
=============

Soon we'll jump right in and start implementing the table functions for
BSTs.  But before that, there's one more topic to discuss, because
they'll keep coming up from time to time throughout the rest of the
book.  This topic is the concept of a "rotation" (*note rotation::).  A
rotation is a simple transformation of a binary tree that looks like
this:

 [image src="rotation.png" text="                               |        |
                               Y        X
                              / \\      / \\
                             X   c    a   Y
                             ^            ^
                            a b          b c

" ]

In this diagram, X and Y represent nodes and a, b, and c are arbitrary
binary trees that may be empty.  A rotation that changes a binary tree
of the form shown on the left to the form shown on the right is called
a "right rotation" (*note right rotation::) on Y.  Going the other way,
it is a "left rotation" (*note left rotation::) on X.

   This figure also introduces new graphical conventions.  First, the
line leading vertically down to the root explicitly shows that the BST
may be a subtree of a larger tree.  Also, the use of both uppercase and
lowercase letters emphasizes the distinction between individual nodes
and subtrees: uppercase letters are nodes, lowercase letters represent
(possibly empty) subtrees.

   A rotation changes the local structure of a binary tree without
changing its ordering as seen through inorder traversal.  That's a
subtle statement, so let's dissect it bit by bit.  Rotations have the
following properties:

Rotations change the structure of a binary tree.
     In particular, rotations can often, depending on the tree's shape,
     be used to change the height of a part of a binary tree.

Rotations change the local structure of a binary tree.
     Any given rotation only affects the node rotated and its immediate
     children.  The node's ancestors and its children's children are
     unchanged.

Rotations do not change the ordering of a binary tree.
     If a binary tree is a binary search tree before a rotation, it is a
     binary search tree after a rotation.  So, we can safely use
     rotations to rearrange a BST-based structure, without concerns
     about upsetting its ordering.

See also:  *Note Cormen 1990::, section 14.2; *Note Sedgewick 1998::,
section 12.8.

Exercises:

1. For each of the binary search trees below, perform a right rotation
at node 4.

 [image src="bstrot1.png" text="                          4          4       4
                         / \\        /       / \\
                        2   5      2       2   6
                        ^         /        ^   ^
                       1 3       1        1 3 5 7

" ]
[*Note answer: 4-3#1..]

2. Write a pair of functions, one to perform a right rotation at a given
BST node, one to perform a left rotation.  What should be the type of
the functions' parameter?  [*Note answer: 4-3#2..]


File: libavl.info,  Node: BST Operations,  Next: Creating a BST,  Prev: BST Rotations,  Up: Binary Search Trees

4.4 Operations
==============

Now can start to implement the operations that we'll want to perform on
BSTs.  Here's the outline of the functions we'll implement.  We use the
generic table insertion convenience functions from Exercise 2.8-3 to
implement bst_insert() and bst_replace(), as well the generic assertion
function implementations from Exercise 2.9-2 to implement
tbl_assert_insert() and tbl_assert_delete().  We also include a copy of
the default memory allocation functions for use with BSTs:

30. <BST operations 30> =
<*Note BST creation function: 31.>
<*Note BST search function: 32.>
<*Note BST item insertion function: 33.>
<*Note Table insertion convenience functions; tbl => bst: 594.>
<*Note BST item deletion function: 38.>
<*Note BST traversal functions: 64.>
<*Note BST copy function: 84.>
<*Note BST destruction function: 85.>
<*Note BST balance function: 88.>
<*Note Default memory allocation functions; tbl => bst: 7.>
<*Note Table assertion functions; tbl => bst: 596.>
   This code is included in *Note 26::.


File: libavl.info,  Node: Creating a BST,  Next: Searching a BST,  Prev: BST Operations,  Up: Binary Search Trees

4.5 Creation
============

We need to write bst_create() to create an empty BST.  All it takes is
a little bit of memory allocation and initialization:

31. <BST creation function 31> =
struct bst_table *
bst_create (bst_comparison_func *compare, void *param,
            struct libavl_allocator *allocator)
{
  struct bst_table *tree;

  assert (compare != NULL);

  if (allocator == NULL)
    allocator = &bst_allocator_default;

  tree = allocator->libavl_malloc (allocator, sizeof *tree);
  if (tree == NULL)
    return NULL;

  tree->bst_root = NULL;
  tree->bst_compare = compare;
  tree->bst_param = param;
  tree->bst_alloc = allocator;
  tree->bst_count = 0;
  tree->bst_generation = 0;

  return tree;
}
   This code is included in *Note 30::, *Note 147::, and *Note 198::.


File: libavl.info,  Node: Searching a BST,  Next: Inserting into a BST,  Prev: Creating a BST,  Up: Binary Search Trees

4.6 Search
==========

Searching a binary search tree works just the same way as it did before
when we were doing it inside an array.  We can implement bst_find()
immediately:

32. <BST search function 32> =
void *
bst_find (const struct bst_table *tree, const void *item)
{
  const struct bst_node *p;

  assert (tree != NULL && item != NULL);
  for (p = tree->bst_root; p != NULL; )
    {
      int cmp = tree->bst_compare (item, p->bst_data, tree->bst_param);

      if (cmp < 0)
        p = p->bst_link[0];
      else if (cmp > 0)
        p = p->bst_link[1];
      else /* cmp == 0 */
        return p->bst_data;
    }

  return NULL;
}
   This code is included in *Note 30::, *Note 147::, *Note 198::, *Note
491::, *Note 524::, and *Note 556::.

See also:  *Note Knuth 1998b::, section 6.2.2; *Note Cormen 1990::,
section 13.2; *Note Kernighan 1988::, section 3.3; *Note Bentley
2000::, chapters 4 and 5, section 9.3, appendix 1; *Note Sedgewick
1998::, program 12.7.


File: libavl.info,  Node: Inserting into a BST,  Next: Deleting from a BST,  Prev: Searching a BST,  Up: Binary Search Trees

4.7 Insertion
=============

Inserting new nodes into a binary search tree is easy.  To start out,
we work the same way as in a search, traversing the tree from the top
down, as if we were searching for the item that we're inserting.  If we
find one, the item is already in the tree, and we need not insert it
again.  But if the new item is not in the tree, eventually we "fall
off" the bottom of the tree.  At this point we graft the new item as a
child of the node that we last examined.

   An example is in order.  Consider this binary search tree:

 [image src="preins.png" text="                                    5
                                   / `_
                                  3    8
                                  ^   /
                                 2 4 6

" ]

Suppose that we wish to insert a new item, 7, into the tree.  7 is
greater than 5, so examine 5's right child, 8.  7 is less than 8, so
examine 8's left child, 6.  7 is greater than 6, but 6 has no right
child.  So, make 7 the right child of 6:

 [image src="postins.png" text="                                   5
                                  / `._
                                 3     8
                                 ^   _'
                                2 4 6
                                     \\
                                      7

" ]

We cast this in a form compatible with the abstract description as
follows:

33. <BST item insertion function 33> =
void **
bst_probe (struct bst_table *tree, void *item)
{
  struct bst_node *p, *q; /* Current node in search and its parent. */
  int dir;                /* Side of q on which p is located. */
  struct bst_node *n;     /* Newly inserted node. */

  assert (tree != NULL && item != NULL);

  for (q = NULL, p = tree->bst_root; p != NULL; q = p, p = p->bst_link[dir])
    {
      int cmp = tree->bst_compare (item, p->bst_data, tree->bst_param);
      if (cmp == 0)
        return &p->bst_data;
      dir = cmp > 0;
    }

  n = tree->bst_alloc->libavl_malloc (tree->bst_alloc, sizeof *p);
  if (n == NULL)
    return NULL;

  tree->bst_count++;
  n->bst_link[0] = n->bst_link[1] = NULL;
  n->bst_data = item;
  if (q != NULL)
    q->bst_link[dir] = n;
  else
    tree->bst_root = n;

  return &n->bst_data;
}
   This code is included in *Note 30::.

See also:  *Note Knuth 1998b::, algorithm 6.2.2T; *Note Cormen 1990::,
section 13.3; *Note Bentley 2000::, section 13.3; *Note Sedgewick
1998::, program 12.7.

Exercises:

1. Explain the expression p = (struct bst_node *) &tree->bst_root.
Suggest an alternative.  [*Note answer: 4-7#1..]

2. Rewrite bst_probe() to use only a single local variable of type
struct bst_node **.  [*Note answer: 4-7#2..]

3. Suppose we want to make a new copy of an existing binary search tree,
preserving the original tree's shape, by inserting items into a new,
currently empty tree.  What constraints are there on the order of item
insertion?  [*Note answer: 4-7#3..]

4. Write a function that calls a provided bst_item_func for each node
in a provided BST in an order suitable for reproducing the original
BST, as discussed in Exercise 3.  [*Note answer: 4-7#4..]

* Menu:

* Root Insertion in a BST::


File: libavl.info,  Node: Root Insertion in a BST,  Prev: Inserting into a BST,  Up: Inserting into a BST

4.7.1 Aside: Root Insertion
---------------------------

One side effect of the usual method for BST insertion, implemented in
the previous section, is that items inserted more recently tend to be
farther from the root, and therefore it takes longer to find them than
items inserted longer ago.  If all items are equally likely to be
requested in a search, this is unimportant, but this is regrettable for
some common usage patterns, where recently inserted items tend to be
searched for more often than older items.

   In this section, we examine an alternative scheme for insertion that
addresses this problem, called "insertion at the root" or "root
insertion".  An insertion with this algorithm always places the new
node at the root of the tree.  Following a series of such insertions,
nodes inserted more recently tend to be nearer the root than other
nodes.

   As a first attempt at implementing this idea, we might try simply
making the new node the root and assigning the old root as one of its
children.  Unfortunately, this and similar approaches will not work
because there is no guarantee that nodes in the existing tree have
values all less than or all greater than the new node.

   An approach that will work is to perform a conventional insertion as
a leaf node, then use a series of rotations to move the new node to the
root.  For example, the diagram below illustrates rotations to move
node 4 to the root.  A left rotation on 3 changes the first tree into
the second, a right rotation on 5 changes the second into the third,
and finally a left rotation on 1 moves 4 into the root position:

 [image src="rootins.png" text="            1          1             1                 4
             `._        `-..__        `-._          _.' \\
                5             5           4        1     5
               / \\           / \\         / \\    =>  `_    \\
              3   6 =>      4   6 =>    3   5         3    6
              ^            /           /     \\       /
             2 4          3           2       6     2
                         /
                        2

" ]

The general rule follows the pattern above.  If we moved down to the
left from a node x during the insertion search, we rotate right at x.
If we moved down to the right, we rotate left.

   The implementation is straightforward.  As we search for the
insertion point we keep track of the nodes we've passed through, then
after the insertion we return to each of them in reverse order and
perform a rotation:

34. <BST item insertion function, root insertion version 34> =
void **
bst_probe (struct bst_table *tree, void *item)
{
  <*Note rb_probe() local variables; rb => bst: 200.>

  <*Note Step 1: Search BST for insertion point, root insertion version: 35.>
  <*Note Step 2: Insert new BST node, root insertion version: 36.>
  <*Note Step 3: Move BST node to root: 37.>

  return &n->bst_data;
}

35. <Step 1: Search BST for insertion point, root insertion version 35> =
pa[0] = (struct bst_node *) &tree->bst_root;
da[0] = 0;
k = 1;
for (p = tree->bst_root; p != NULL; p = p->bst_link[da[k - 1]])
  {
    int cmp = tree->bst_compare (item, p->bst_data, tree->bst_param);
    if (cmp == 0)
      return &p->bst_data;

    if (k >= BST_MAX_HEIGHT)
      {
        bst_balance (tree);
        return bst_probe (tree, item);
      }

    pa[k] = p;
    da[k++] = cmp > 0;
  }
   This code is included in *Note 34::.

36. <Step 2: Insert new BST node, root insertion version 36> =
n = pa[k - 1]->bst_link[da[k - 1]] =
  tree->bst_alloc->libavl_malloc (tree->bst_alloc, sizeof *n);
if (n == NULL)
  return NULL;

n->bst_link[0] = n->bst_link[1] = NULL;
n->bst_data = item;
tree->bst_count++;
tree->bst_generation++;
   This code is included in *Note 34::.

37. <Step 3: Move BST node to root 37> =
for (; k > 1; k--)
  {
    struct bst_node *q = pa[k - 1];

    if (da[k - 1] == 0)
      {
        q->bst_link[0] = n->bst_link[1];
        n->bst_link[1] = q;
      }
    else /* da[k - 1] == 1 */
      {
        q->bst_link[1] = n->bst_link[0];
        n->bst_link[0] = q;
      }
    pa[k - 2]->bst_link[da[k - 2]] = n;
  }
   This code is included in *Note 34::, *Note 624::, and *Note 629::.

See also:  *Note Sedgewick 1998::, section 12.8.

Exercises:

1. Root insertion will prove useful later when we write a function to
join a pair of disjoint BSTs (*note Joining BSTs::).  For that purpose,
we need to be able to insert a preallocated node as the root of an
arbitrary tree that may be a subtree of some other tree.  Write a
function to do this matching the following prototype:

static int root_insert (struct bst_table *tree, struct bst_node **root,
                        struct bst_node *new_node);

Your function should insert new_node at *root using root insertion,
storing new_node into *root, and return nonzero only if successful.
The subtree at *root is in tree.  You may assume that no node matching
new_node exists within subtree root.  [*Note answer: 4-7-1#1..]

2. Now implement a root insertion as in Exercise 1, except that the
function is not allowed to fail, and rebalancing the tree is not
acceptable either.  Use the same prototype with the return type changed
to void.  [*Note answer: 4-7-1#2..]

*3. Suppose that we perform a series of root insertions in an initially
empty BST.  What kinds of insertion orders require a large amount of
stack?  [*Note answer: 4-7-1#3..]


File: libavl.info,  Node: Deleting from a BST,  Next: Traversing a BST,  Prev: Inserting into a BST,  Up: Binary Search Trees

4.8 Deletion
============

Deleting an item from a binary search tree is a little harder than
inserting one.  Before we write any code, let's consider how to delete
nodes from a binary search tree in an abstract fashion.  Here's a BST
from which we can draw examples during the discussion:

 [image src="bstdel.png" text="                                    5
                                _.-' `-.__
                               2          9
                              / \\        /
                             1   3      8
                                  \\   _'
                                   4 6
                                      \\
                                       7

" ]

It is more difficult to remove some nodes from this tree than to remove
others.  Here, we recognize three distinct cases (Exercise 4.8-1 offers
a fourth), described in detail below in terms of the deletion of a node
designated p.

Case 1: p has no right child
............................

It is trivial to delete a node with no right child, such as node 1, 4,
7, or 8 above.  We replace the pointer leading to p by p's left child,
if it has one, or by a null pointer, if not.  In other words, we
replace the deleted node by its left child.  For example, the process
of deleting node 8 looks like this:

 [image src="bstdel2.png" text="                            5
                        _.-' `-..__            5
                       2           9       _.-' `._
                      / \\        _'       2        9
                     1   3      8,p  =>  / \\     _'
                          \\   _'        1   3   6
                           4 6               \\   \\
                              \\               4   7
                               7

" ]

This diagram shows the convention of separating multiple labels on a
single node by a comma: node 8 is also node p.

Case 2: p's right child has no left child
.........................................

This case deletes any node p with a right child r that itself has no
left child.  Nodes 2, 3, and 6 in the tree above are examples.  In this
case, we move r into p's place, attaching p's former left subtree, if
any, as the new left subtree of r.  For instance, to delete node 2 in
the tree above, we can replace it by its right child 3, giving node 2's
left child 1 to node 3 as its new left child.  The process looks like
this:

 [image src="bstdel3.png" text="                             5                 5
                     ___..--' `-.__        _.-' `-.__
                    2,p            9      3,r        9
                   /   \\          /      /   \\      /
                  1     3,r      8   => 1     4    8
                           \\   _'                _'
                            4 6                 6
                               \\                 \\
                                7                 7

" ]

Case 3: p's right child has a left child
........................................

This is the "hard" case, where p's right child r has a left child.  but
if we approach it properly we can make it make sense.  Let p's "inorder
successor" (*note inorder successor::), that is, the node with the
smallest value greater than p, be s.  Then, our strategy is to detach s
from its position in the tree, which is always an easy thing to do, and
put it into the spot formerly occupied by p, which disappears from the
tree.  In our example, to delete node 5, we move inorder successor node
6 into its place, like this:

 [image src="bstdel4.png" text="                         5,p
                     _.-'   `--..__            6,s
                    2              9       _.-'   `-._
                   / \\            /       2           9
                  1   3          8   =>  / \\         /
                       \\     _.-'       1   3       8
                        4   6,s              \\     /
                               \\              4   7
                                7

" ]

But how do we know that node s exists and that we can delete it easily?
We know that it exists because otherwise this would be case 1 or case
2 (consider their conditions).  We can easily detach from its position
for a more subtle reason: s is the inorder successor of p and is
therefore has the smallest value in p's right subtree, so s cannot have
a left child.  (If it did, then this left child would have a smaller
value than s, so it, rather than s, would be p's inorder successor.)
Because s doesn't have a left child, we can simply replace it by its
right child, if any.  This is the mirror image of case 1.

Implementation
..............

The code for BST deletion closely follows the above discussion.  Let's
start with an outline of the function:

38. <BST item deletion function 38> =
void *
bst_delete (struct bst_table *tree, const void *item)
{
  struct bst_node *p, *q; /* Node to delete and its parent. */
  int cmp;                /* Comparison between p->bst_data and item. */
  int dir;                /* Side of q on which p is located. */

  assert (tree != NULL && item != NULL);

  <*Note Step 1: Find BST node to delete: 39.>
  <*Note Step 2: Delete BST node: 40.>
  <*Note Step 3: Finish up after deleting BST node: 45.>
}
   This code is included in *Note 30::.

   We begin by finding the node to delete, in much the same way that
bst_find() did.  But, in every case above, we replace the link leading
to the node being deleted by another node or a null pointer.  To do so,
we have to keep track of the pointer that led to the node to be deleted.
This is the purpose of q and dir in the code below.

39. <Step 1: Find BST node to delete 39> =
p = (struct bst_node *) &tree->bst_root;
for (cmp = -1; cmp != 0;
     cmp = tree->bst_compare (item, p->bst_data, tree->bst_param))
  {
    dir = cmp > 0;
    q = p;
    p = p->bst_link[dir];
    if (p == NULL)
      return NULL;
  }
item = p->bst_data;
   This code is included in *Note 38::.

   Now we can actually delete the node.  Here is the code to distinguish
between the three cases:

40. <Step 2: Delete BST node 40> =
if (p->bst_link[1] == NULL) { <*Note Case 1 in BST deletion: 41.> }
else
  {
    struct bst_node *r = p->bst_link[1];
    if (r->bst_link[0] == NULL)
      {
        <*Note Case 2 in BST deletion: 42.>
      }
    else
      {
        <*Note Case 3 in BST deletion: 43.>
      }
  }
   This code is included in *Note 38::.

   In case 1, we simply replace the node by its left subtree:

41. <Case 1 in BST deletion 41> =
q->bst_link[dir] = p->bst_link[0];
   This code is included in *Note 40::.

   In case 2, we attach the node's left subtree as its right child r's
left subtree, then replace the node by r:

42. <Case 2 in BST deletion 42> =
r->bst_link[0] = p->bst_link[0];
q->bst_link[dir] = r;
   This code is included in *Note 40::.

   We begin case 3 by finding p's inorder successor as s, and the
parent of s as r.  Node p's inorder successor is the smallest value in
p's right subtree and that the smallest value in a tree can be found by
descending to the left until a node with no left child is found:

43. <Case 3 in BST deletion 43> =
struct bst_node *s;
for (;;)
  {
    s = r->bst_link[0];
    if (s->bst_link[0] == NULL)
      break;

    r = s;
  }
   See also *Note 44::.
This code is included in *Note 40::.

   Case 3 wraps up by adjusting pointers so that s moves into p's place:

44. <Case 3 in BST deletion 43> +=
r->bst_link[0] = s->bst_link[1];
s->bst_link[0] = p->bst_link[0];
s->bst_link[1] = p->bst_link[1];
q->bst_link[dir] = s;

   As the final step, we decrement the number of nodes in the tree, free
the node, and return its data:

45. <Step 3: Finish up after deleting BST node 45> =
tree->bst_alloc->libavl_free (tree->bst_alloc, p);
tree->bst_count--;
tree->bst_generation++;
return (void *) item;
   This code is included in *Note 38::.

See also:  *Note Knuth 1998b::, algorithm 6.2.2D; *Note Cormen 1990::,
section 13.3.

Exercises:

1. Write code for a case 1.5 which handles deletion of nodes with no
left child.  [*Note answer: 4-8#1..]

2. In the code presented above for case 3, we update pointers to move s
into p's position, then free p.  An alternate approach is to replace
p's data by s's and delete s.  Write code to use this approach.  Can a
similar modification be made to either of the other cases?  [*Note
answer: 4-8#2..]

*3. The code in the previous exercise is a few lines shorter than that
in the main text, so it would seem to be preferable.  Explain why the
revised code, and other code based on the same idea, cannot be used in
libavl.  (Hint: consider the semantics of libavl traversers.)  [*Note
answer: 4-8#3..]

* Menu:

* Deletion by Merging::


File: libavl.info,  Node: Deletion by Merging,  Prev: Deleting from a BST,  Up: Deleting from a BST

4.8.1 Aside: Deletion by Merging
--------------------------------

The libavl algorithm for deletion is commonly used, but it is also
seemingly ad-hoc and arbitrary in its approach.  In this section we'll
take a look at another algorithm that may seem a little more uniform.
Unfortunately, though it is conceptually simpler in some ways, in
practice this algorithm is both slower and more difficult to properly
implement.

   The idea behind this algorithm is to consider deletion as breaking
the links between the deleted node and its parent and children.  In the
most general case, we end up with three disconnected BSTs, one that
contains the deleted node's parent and two corresponding to the deleted
node's former subtrees.  The diagram below shows how this idea works
out for the deletion of node 5 from the tree on the left:

 [image src="rotdel.png" text="                          2              2
                         / `._          /
                        1     5        1
                            _' `._
                           3      9 =>    3     9
                            \\    /         \\   /
                             4  7           4 7
                                ^             ^
                               6 8           6 8

" ]

Of course, the problem then becomes to reassemble the pieces into a
single binary search tree.  We can do this by merging the two former
subtrees of the deleted node and attaching them as the right child of
the parent subtree.  As the first step in merging the subtrees, we take
the minimum node r in the former right subtree and repeatedly perform a
right rotation on its parent, until it is the root of its subtree.  The
process up to this point looks like this for our example, showing only
the subtree containing r:

 [image src="rotdel2.png" text="                                       9    6,r
                           9     __..-'        `._
                         _'     6,r               9
                        7    =>    \\     =>     _'
                      _' \\          7          7
                     6,r  8          \\          \\
                                      8          8

" ]

Now, because r is the root and the minimum node in its subtree, r has
no left child.  Also, all the nodes in the opposite subtree are smaller
than r.  So to merge these subtrees, we simply link the opposite
subtree as r's left child and connect r in place of the deleted node:

 [image src="rotdel3.png" text="                       2
                      /                  2
                     1                  / `._
                                       1     6,r
                        3   6,r            _'   `._
                         \\     `._  =>    3        9
                          4       9        \\     _'
                                _'          4   7
                               7                 \\
                                \\                 8
                                 8

" ]

The function outline is straightforward:

46. <BST item deletion function, by merging 46> =
void *
bst_delete (struct bst_table *tree, const void *item)
{
  struct bst_node *p;   /* The node to delete, or a node part way to it. */
  struct bst_node *q;	/* Parent of p. */
  int cmp, dir;         /* Result of comparison between item and p. */

  assert (tree != NULL && item != NULL);

  <*Note Step 1: Find BST node to delete by merging: 47.>
  <*Note Step 2: Delete BST node by merging: 48.>
  <*Note Step 3: Finish up after BST deletion by merging: 49.>

  return (void *) item;
}

   First we search for the node to delete, storing it as p and its
parent as q:

47. <Step 1: Find BST node to delete by merging 47> =
p = (struct bst_node *) &tree->bst_root;
for (cmp = -1; cmp != 0;
     cmp = tree->bst_compare (item, p->bst_data, tree->bst_param))
  {
    dir = cmp > 0;
    q = p;
    p = p->bst_link[dir];
    if (p == NULL)
      return NULL;
  }
   This code is included in *Note 46::.

   The actual deletion process is not as simple.  We handle specially
the case where p has no right child.  This is unfortunate for
uniformity, but simplifies the rest of the code considerably.  The main
case starts off with a loop on variable r to build a stack of the nodes
in the right subtree of p that will need to be rotated.  After the
loop, r is the minimum value in p's right subtree.  This will be the
new root of the merged subtrees after the rotations, so we set r as q's
child on the appropriate side and r's left child as p's former left
child.  After that the only remaining task is the rotations themselves,
so we perform them and we're done:

48. <Step 2: Delete BST node by merging 48> =
if (p->bst_link[1] != NULL)
  {
    struct bst_node *pa[BST_MAX_HEIGHT]; /* Nodes on stack. */
    unsigned char da[BST_MAX_HEIGHT];    /* Directions moved from stack nodes. */
    int k = 0;                           /* Stack height. */

    struct bst_node *r; /* Iterator; final value is minimum node in subtree. */

    for (r = p->bst_link[1]; r->bst_link[0] != NULL; r = r->bst_link[0])
      {
        if (k >= BST_MAX_HEIGHT)
          {
            bst_balance (tree);
            return bst_delete (tree, item);
          }

        pa[k] = r;
        da[k++] = 0;
      }
    q->bst_link[dir] = r;
    r->bst_link[0] = p->bst_link[0];

    for (; k > 0; k--)
      {
        struct bst_node *y = pa[k - 1];
        struct bst_node *x = y->bst_link[0];
        y->bst_link[0] = x->bst_link[1];
        x->bst_link[1] = y;
        if (k > 1)
          pa[k - 2]->bst_link[da[k - 2]] = x;
      }
  }
else
  q->bst_link[dir] = p->bst_link[0];
   This code is included in *Note 46::.

   Finally, there's a bit of obligatory bookkeeping:

49. <Step 3: Finish up after BST deletion by merging 49> =
item = p->bst_data;
tree->bst_alloc->libavl_free (tree->bst_alloc, p);
tree->bst_count--;
tree->bst_generation++;
   This code is included in *Note 46::.

See also:  *Note Sedgewick 1998::, section 12.9.


File: libavl.info,  Node: Traversing a BST,  Next: Copying a BST,  Prev: Deleting from a BST,  Up: Binary Search Trees

4.9 Traversal
=============

After we've been manipulating a binary search tree for a while, we will
want to know what items are in it.  The process of enumerating the items
in a binary search tree is called "traversal" (*note traversal::).
libavl provides the bst_t_* functions for a particular kind of
traversal called "inorder traversal" (*note inorder traversal::),
so-called because items are enumerated in sorted order.

   In this section we'll implement three algorithms for traversal.
Each of these algorithms is based on and in some way improves upon the
previous algorithm.  The final implementation is the one used in
libavl, so we will implement all of the bst_t_* functions for it.

   Before we start looking at particular algorithms, let's consider some
criteria for evaluating traversal algorithms.  The following are not the
only criteria that could be used, but they are indeed important:(1)

complexity
     Is it difficult to describe or to correctly implement the
     algorithm?  Complex algorithms also tend to take more code than
     simple ones.

efficiency
     Does the algorithm make good use of time and memory?  The ideal
     traversal algorithm would require time proportional to the number
     of nodes traversed and a constant amount of space.  In this
     chapter we will meet this ideal time criterion and come close on
     the space criterion for the average case.  In future chapters we
     will be able to do better even in the worst case.

convenience
     Is it easy to integrate the traversal functions into other code?
     Callback functions are not as easy to use as other methods that
     can be used from for loops (*note Improving Convenience::).

reliability
     Are there pathological cases where the algorithm breaks down?  If
     so, is it possible to fix these problems using additional time or
     space?

generality
     Does the algorithm only allow iteration in a single direction?
     Can we begin traversal at an arbitrary node, or just at the least
     or greatest node?

resilience
     If the tree is modified during a traversal, is it possible to
     continue traversal, or does the modification invalidate the
     traverser?

   The first algorithm we will consider uses recursion.  This algorithm
is worthwhile primarily for its simplicity.  In C, such an algorithm
cannot be made as efficient, convenient, or general as other algorithms
without unacceptable compromises.  It is possible to make it both
reliable and resilient, but we won't bother because of its other
drawbacks.

   We arrive at our second algorithm through a literal transformation of
the recursion in the first algorithm into iteration.  The use of
iteration lets us improve the algorithm's memory efficiency, and, on
many machines, its time efficiency as well.  The iterative algorithm
also lets us improve the convenience of using the traverser.  We could
also add reliability and resilience to an implementation of this
algorithm, but we'll save that for later.  The only problem with this
algorithm, in fact, lies in its generality: it works best for moving
only in one direction and starting from the least or greatest node.

   The importance of generality is what draws us to the third algorithm.
This algorithm is based on ideas from the previous iterative algorithm
along with some simple observations.  This algorithm is no more complex
than the previous one, but it is more general, allowing easily for
iteration in either direction starting anywhere in the tree.  This is
the algorithm used in libavl, so we build an efficient, convenient,
reliable, general, resilient implementation.

* Menu:

* Recursive Traversal of a BST::
* Iterative Traversal of a BST::
* Better Iterative Traversal::

   ---------- Footnotes ----------

   (1) Some of these terms are not generic BST vocabulary.  Rather,
they have been adopted for these particular uses in this text.  You can
consider the above to be our working definitions of these terms.


File: libavl.info,  Node: Recursive Traversal of a BST,  Next: Iterative Traversal of a BST,  Prev: Traversing a BST,  Up: Traversing a BST

4.9.1 Traversal by Recursion
----------------------------

To figure out how to traverse a binary search tree in inorder, think
about a BST's structure.  A BST consists of a root, a left subtree, and
right subtree.  All the items in the left subtree have smaller values
than the root and all the items in the right subtree have larger values
than the root.

   That's good enough right there: we can traverse a BST in inorder by
dealing with its left subtree, then doing with the root whatever it is
we want to do with each node in the tree (generically, "visit" (*note
visit::) the root node), then dealing with its right subtree.  But how
do we deal with the subtrees?  Well, they're BSTs too, so we can do the
same thing: traverse its left subtree, then visit its root, then
traverse its right subtree, and so on.  Eventually the process
terminates because at some point the subtrees are null pointers, and
nothing needs to be done to traverse an empty tree.

   Writing the traversal function is almost trivial.  We use
bst_item_func to visit a node (*note Item and Copy Functions::):

50. <Recursive traversal of BST 50> =
static void
traverse_recursive (struct bst_node *node, bst_item_func *action, void *param)
{
  if (node != NULL)
    {
      traverse_recursive (node->bst_link[0], action, param);
      action (node->bst_data, param);
      traverse_recursive (node->bst_link[1], action, param);
    }
}
   See also *Note 51::.

   We also want a wrapper function to insulate callers from the
existence of individual tree nodes:

51. <Recursive traversal of BST 50> +=
void
walk (struct bst_table *tree, bst_item_func *action, void *param)
{
  assert (tree != NULL && action != NULL);
  traverse_recursive (tree->bst_root, action, param);
}

See also:  *Note Knuth 1997::, section 2.3.1; *Note Cormen 1990::,
section 13.1; *Note Sedgewick 1998::, program 12.8.

Exercises:

1. Instead of checking for a null node at the top of
traverse_recursive(), would it be better to check before calling in
each place that the function is called?  Why or why not?  [*Note
answer: 4-9-1#1..]

2. Some languages, such as Pascal, support the concept of "nested
functions", that is, functions within functions, but C does not.  Some
algorithms, including recursive tree traversal, can be expressed much
more naturally with this feature.  Rewrite walk(), in a hypothetical
C-like language that supports nested functions, as a function that calls
an inner, recursively defined function.  The nested function should only
take a single parameter.  (The GNU C compiler supports nested functions
as a language extension, so you may want to use it to check your code.)
[*Note answer: 4-9-1#2..]


File: libavl.info,  Node: Iterative Traversal of a BST,  Next: Better Iterative Traversal,  Prev: Recursive Traversal of a BST,  Up: Traversing a BST

4.9.2 Traversal by Iteration
----------------------------

The recursive approach of the previous section is one valid way to
traverse a binary search tree in sorted order.  This method has the
advantages of being simple and "obviously correct".  But it does have
problems with efficiency, because each call to traverse_recursive()
receives its own duplicate copies of arguments action and param, and
with convenience, because writing a new callback function for each
traversal is unpleasant.  It has other problems, too, as already
discussed, but these are the ones to be addressed immediately.

   Unfortunately, neither problem can be solved acceptably in C using a
recursive method, the first because the traversal function has to
somehow know the action function and the parameter to pass to it, and
the second because there is simply no way to jump out of and then back
into recursive calls in C.(1)  Our only option is to use an algorithm
that does not involve recursion.

   The simplest way to eliminate recursion is by a literal conversion of
the recursion to iteration.  This is the topic of this section.  Later,
we will consider a slightly different, and in some ways superior,
iterative solution.

   Converting recursion into iteration is an interesting problem.
There are two main ways to do it:

tail recursion elimination
     If a recursive call is the last action taken in a function, then
     it is equivalent to a goto back to the beginning of the function,
     possibly after modifying argument values.  (If the function has a
     return value then the recursive call must be a return statement
     returning the value received from the nested call.)  This form of
     recursion is called "tail recursion" (*note tail recursion::).

save-and-restore recursion elimination
     In effect, a recursive function call saves a copy of argument
     values and local variables, modifies the arguments, then executes
     a goto to the beginning of the function.  Accordingly, the return
     from the nested call is equivalent to restoring the saved
     arguments and local variables, then executing a goto back to the
     point where the call was made.

   We can make use of both of these rules in converting
traverse_recursive() to iterative form.  First, does
traverse_recursive() ever call itself as its last action?  The answer
is yes, so we can convert that to an assignment plus a goto statement:

52. <Iterative traversal of BST, take 1 52> =
static void
traverse_iterative (struct bst_node *node, bst_item_func *action, void *param)
{
start:
  if (node != NULL)
    {
      traverse_iterative (node->bst_link[0], action, param);
      action (node->bst_data, param);
      node = node->bst_link[1];
      goto start;
    }
}

   Sensible programmers are not fond of goto.  Fortunately, it is easy
to eliminate by rephrasing in terms of a while loop:

53. <Iterative traversal of BST, take 2 53> =
static void
traverse_iterative (struct bst_node *node, bst_item_func *action, void *param)
{
  while (node != NULL)
    {
      traverse_iterative (node->bst_link[0], action, param);
      action (node->bst_data, param);
      node = node->bst_link[1];
    }
}

   This still leaves another recursive call, one that is not tail
recursive.  This one must be eliminated by saving and restoring values.
A stack is ideal for this purpose.  For now, we use a stack of fixed
size BST_MAX_HEIGHT and deal with stack overflow by aborting.  Later,
we'll handle overflow more gracefully.  Here's the code:

54. <Iterative traversal of BST, take 3 54> =
static void
traverse_iterative (struct bst_node *node, bst_item_func *action, void *param)
{
  struct bst_node *stack[BST_MAX_HEIGHT];
  size_t height = 0;

start:
  while (node != NULL)
    {
      if (height >= BST_MAX_HEIGHT)
        {
          fprintf (stderr, "tree too deep\n");
          exit (EXIT_FAILURE);
        }
      stack[height++] = node;
      node = node->bst_link[0];
      goto start;

    resume:
      action (node->bst_data, param);
      node = node->bst_link[1];
    }

  if (height > 0)
    {
      node = stack[--height];
      goto resume;
    }
}

   This code, an ugly mash of statements, is a prime example of why goto
statements are discouraged, but its relationship with the earlier code
is clear.  To make it acceptable for real use, we must rephrase it.
First, we can eliminate label resume by recognizing that it can only be
reached from the corresponding goto statement, then moving its code
appropriately:

55. <Iterative traversal of BST, take 4 55> =
static void
traverse_iterative (struct bst_node *node, bst_item_func *action, void *param)
{
  struct bst_node *stack[BST_MAX_HEIGHT];
  size_t height = 0;

start:
  while (node != NULL)
    {
      if (height >= BST_MAX_HEIGHT)
        {
          fprintf (stderr, "tree too deep\n");
          exit (EXIT_FAILURE);
        }
      stack[height++] = node;
      node = node->bst_link[0];
      goto start;
    }

  if (height > 0)
    {
      node = stack[--height];
      action (node->bst_data, param);
      node = node->bst_link[1];
      goto start;
    }
}

   The first remaining goto statement can be eliminated without any
other change, because it is redundant; the second, by enclosing the
whole function body in an "infinite loop":

56. <Iterative traversal of BST, take 5 56> =
static void
traverse_iterative (struct bst_node *node, bst_item_func *action, void *param)
{
  struct bst_node *stack[BST_MAX_HEIGHT];
  size_t height = 0;

  for (;;)
    {
      while (node != NULL)
        {
          if (height >= BST_MAX_HEIGHT)
            {
              fprintf (stderr, "tree too deep\n");
              exit (EXIT_FAILURE);
            }
          stack[height++] = node;
          node = node->bst_link[0];
        }

      if (height == 0)
        break;

      node = stack[--height];
      action (node->bst_data, param);
      node = node->bst_link[1];
    }
}

   This initial iterative version takes care of the efficiency problem.

Exercises:

1. Function traverse_iterative() relies on stack[], a stack of nodes
yet to be visited, which as allocated can hold up to BST_MAX_HEIGHT
nodes.  Consider the following questions concerning stack[]:

  a. What is the maximum height this stack will attain in traversing a
     binary search tree containing n nodes, if the binary tree has
     minimum possible height?

  b. What is the maximum height this stack can attain in traversing any
     binary tree of n nodes?  The minimum height?

  c. Under what circumstances is it acceptable to use a fixed-size
     stack as in the example code?

  d. Rewrite traverse_iterative() to dynamically expand stack[] in case
     of overflow.

  e. Does traverse_recursive() also have potential for running out of
     "stack" or "memory"?  If so, more or less than
     traverse_iterative() as modified by the previous part?
        [*Note answer: 4-9-2#1..]

* Menu:

* Improving Convenience::

   ---------- Footnotes ----------

   (1) This is possible in some other languages, such as Scheme, that
support "coroutines" as well as subroutines.


File: libavl.info,  Node: Improving Convenience,  Prev: Iterative Traversal of a BST,  Up: Iterative Traversal of a BST

4.9.2.1 Improving Convenience
.............................

Now we can work on improving the convenience of our traversal function.
But, first, perhaps it's worthwhile to demonstrate how inconvenient it
really can be to use walk(), regardless of how it's implemented
internally.

   Suppose that we have a BST of character strings and, for whatever
reason, want to know the total length of all the strings in it.  We
could do it like this using walk():

57. <Summing string lengths with walk() 57> =
static void
process_node (void *data, void *param)
{
  const char *string = data;
  size_t *total = param;

  *total += strlen (string);
}

size_t
total_length (struct bst_table *tree)
{
  size_t total = 0;
  walk (tree, process_node, &total);
  return total;
}

With the functions first_item() and next_item() that we'll write in
this section, we can rewrite these functions as the single function
below:

58. <Summing string lengths with next_item() 58> =
size_t
total_length (struct bst_table *tree)
{
  struct traverser t;
  const char *string;
  size_t total = 0;

  for (string = first_item (tree, &t); string != NULL; string = next_item (&t))
    total += strlen (string);
  return total;
}

   You're free to make your own assessment, of course, but many
programmers prefer the latter because of its greater brevity and fewer
"unsafe" conversions to and from void pointers.

   Now to actually write the code.  Our task is to modify
traverse_iterative() so that, instead of calling action, it returns
node->bst_data.  But first, some infrastructure.  We define a structure
to contain the state of the traversal, equivalent to the relevant
argument and local variables in traverse_iterative().  To emphasize
that this is not our final version of this structure or the related
code, we will call it struct traverser, without any name prefix:

59. <Iterative traversal of BST, take 6 59> =
struct traverser
  {
    struct bst_table *table;                  /* Tree being traversed. */
    struct bst_node *node;                    /* Current node in tree. */
    struct bst_node *stack[BST_MAX_HEIGHT];   /* Parent nodes to revisit. */
    size_t height;                            /* Number of nodes in stack. */
  };
   See also *Note 60:: and *Note 61::.

   Function first_item() just initializes a struct traverser and
returns the first item in the tree, deferring most of its work to
next_item():

60. <Iterative traversal of BST, take 6 59> +=
/* Initializes trav for tree.
   Returns data item in tree with the smallest value,
   or NULL if tree is empty.
   In the former case, next_item() may be called with trav
   to retrieve additional data items. */
void *
first_item (struct bst_table *tree, struct traverser *trav)
{
  assert (tree != NULL && trav != NULL);
  trav->table = tree;
  trav->node = tree->bst_root;
  trav->height = 0;
  return next_item (trav);
}

   Function next_item() is, for the most part, a simple modification of
traverse_iterative():

61. <Iterative traversal of BST, take 6 59> +=
/* Returns the next data item in inorder
   within the tree being traversed with trav,
   or if there are no more data items returns NULL.
   In the former case next_item() may be called again
   to retrieve the next item. */
void *
next_item (struct traverser *trav)
{
  struct bst_node *node;

  assert (trav != NULL);
  node = trav->node;
  while (node != NULL)
    {
      if (trav->height >= BST_MAX_HEIGHT)
        {
          fprintf (stderr, "tree too deep\n");
          exit (EXIT_FAILURE);
        }

      trav->stack[trav->height++] = node;
      node = node->bst_link[0];
    }

  if (trav->height == 0)
    return NULL;

  node = trav->stack[--trav->height];
  trav->node = node->bst_link[1];
  return node->bst_data;
}

See also:  *Note Knuth 1997::, algorithm 2.3.1T; *Note Knuth 1992::, p.
50-54, section "Recursion Elimination" within article "Structured
Programming with go to statements".

Exercises:

1. Make next_item() reliable by providing alternate code to execute on
stack overflow.  This code will work by calling bst_balance() to
"balance" the tree, reducing its height such that it can be traversed
with the small stack that we use.  We will develop bst_balance() later.
For now, consider it a "black box" that simply needs to be invoked
with the tree to balance as an argument.  Don't forget to adjust the
traverser structure so that later calls will work properly, too.
[*Note answer: 4-9-2-1#1..]

2. Without modifying next_item() or first_item(), can a function
prev_item() be written that will move to and return the previous item
in the tree in inorder?  [*Note answer: 4-9-2-1#2..]


File: libavl.info,  Node: Better Iterative Traversal,  Prev: Iterative Traversal of a BST,  Up: Traversing a BST

4.9.3 Better Iterative Traversal
--------------------------------

We have developed an efficient, convenient function for traversing a
binary tree.  In the exercises, we made it reliable, and it is possible
to make it resilient as well.  But its algorithm makes it difficult to
add generality.  In order to do that in a practical way, we will have to
use a new algorithm.

   Let us start by considering how to understand how to find the
successor or predecessor of any node in general, as opposed to just
blindly transforming code as we did in the previous section.  Back when
we wrote bst_delete(), we already solved half of the problem, by
figuring out how to find the successor of a node that has a right
child: take the least-valued node in the right subtree of the node
(*note Deletion Case 3: successor.).

   The other half is the successor of a node that doesn't have a right
child.  Take a look at the code for one of the previous traversal
functions--recursive or iterative, whichever you better understand--and
mentally work out the relationship between the current node and its
successor for a node without a right child.  What happens is that we
move up the tree, from a node to its parent, one node at a time, until
it turns out that we moved up to the right (as opposed to up to the
left) and that is the successor node.  Think of it this way: if we move
up to the left, then the node we started at has a lesser value than
where we ended up, so we've already visited it, but if we move up to
the right, then we're moving to a node with a greater value, so we've
found the successor.

   Using these instructions, we can find the predecessor of a node, too,
just by exchanging "left" and "right".  This suggests that all we have
to do in order to generalize our traversal function is to keep track of
all the nodes above the current node, not just the ones that are up and
to the left.  This in turn suggests our final implementation of struct
bst_traverser, with appropriate comments:

62. <BST traverser structure 62> =
/* BST traverser structure. */
struct bst_traverser
  {
    struct bst_table *bst_table;        /* Tree being traversed. */
    struct bst_node *bst_node;          /* Current node in tree. */
    struct bst_node *bst_stack[BST_MAX_HEIGHT];
                                        /* All the nodes above bst_node. */
    size_t bst_height;                  /* Number of nodes in bst_parent. */
    unsigned long bst_generation;       /* Generation number. */
  };
   This code is included in *Note 25::, *Note 143::, and *Note 194::.

   Because user code is expected to declare actual instances of struct
bst_traverser, struct bst_traverser must be defined in <*Note bst.h:
25.> and therefore all of its member names are prefixed by bst_ for
safety.

   The only surprise in struct bst_traverser is member bst_generation,
the traverser's generation number.  This member is set equal to its
namesake in struct bst_table when a traverser is initialized.  After
that, the two values are compared whenever the stack of parent pointers
must be accessed.  Any change in the tree that could disturb the action
of a traverser will cause their generation numbers to differ, which in
turn triggers an update to the stack.  This is what allows this final
implementation to be resilient.

   We need a utility function to actually update the stack of parent
pointers when differing generation numbers are detected.  This is easy
to write:

63. <BST traverser refresher 63> =
/* Refreshes the stack of parent pointers in trav
   and updates its generation number. */
static void
trav_refresh (struct bst_traverser *trav)
{
  assert (trav != NULL);

  trav->bst_generation = trav->bst_table->bst_generation;

  if (trav->bst_node != NULL)
    {
      bst_comparison_func *cmp = trav->bst_table->bst_compare;
      void *param = trav->bst_table->bst_param;
      struct bst_node *node = trav->bst_node;
      struct bst_node *i;

      trav->bst_height = 0;
      for (i = trav->bst_table->bst_root; i != node; )
        {
          assert (trav->bst_height < BST_MAX_HEIGHT);
          assert (i != NULL);

          trav->bst_stack[trav->bst_height++] = i;
          i = i->bst_link[cmp (node->bst_data, i->bst_data, param) > 0];
        }
    }
}
   This code is included in *Note 64:: and *Note 180::.

   The following sections will implement all of the traverser functions
bst_t_*().  *Note Traversers::, for descriptions of the purpose of each
of these functions.

   The traversal functions are collected together into <*Note BST
traversal functions: 64.>:

64. <BST traversal functions 64> =
<*Note BST traverser refresher: 63.>
<*Note BST traverser null initializer: 65.>
<*Note BST traverser least-item initializer: 66.>
<*Note BST traverser greatest-item initializer: 67.>
<*Note BST traverser search initializer: 68.>
<*Note BST traverser insertion initializer: 69.>
<*Note BST traverser copy initializer: 70.>
<*Note BST traverser advance function: 71.>
<*Note BST traverser back up function: 74.>
<*Note BST traverser current item function: 75.>
<*Note BST traverser replacement function: 76.>
   This code is included in *Note 30::.

Exercises:

1. The bst_probe() function doesn't change the tree's generation number.
Why not?  [*Note answer: 4-9-3#1..]

*2. The main loop in trav_refresh() contains the assertion

      assert (trav->bst_height < BST_MAX_HEIGHT);

Prove that this assertion is always true.  [*Note answer:
4-9-3#2..]

3. In trav_refresh(), it is tempting to avoid calls to the user-supplied
comparison function by comparing the nodes on the stack to the current
state of the tree; e.g., move up the stack, starting from the bottom,
and for each node verify that it is a child of the previous one on the
stack, falling back to the general algorithm at the first mismatch.  Why
won't this work?  [*Note answer: 4-9-3#3..]

* Menu:

* BST Traverser Null Initialization::
* BST Traverser First Initialization::
* BST Traverser Last Initialization::
* BST Traverser Find Initialization::
* BST Traverser Insert Initialization::
* BST Traverser Copying::
* BST Traverser Advancing::
* BST Traverser Retreating::
* BST Traversal Current Item::
* BST Traversal Replacing the Current Item::


File: libavl.info,  Node: BST Traverser Null Initialization,  Next: BST Traverser First Initialization,  Prev: Better Iterative Traversal,  Up: Better Iterative Traversal

4.9.3.1 Starting at the Null Node
.................................

The trav_t_init() function just initializes a traverser to the null
item, indicated by a null pointer for bst_node.

65. <BST traverser null initializer 65> =
void
bst_t_init (struct bst_traverser *trav, struct bst_table *tree)
{
  trav->bst_table = tree;
  trav->bst_node = NULL;
  trav->bst_height = 0;
  trav->bst_generation = tree->bst_generation;
}
   This code is included in *Note 64:: and *Note 180::.


File: libavl.info,  Node: BST Traverser First Initialization,  Next: BST Traverser Last Initialization,  Prev: BST Traverser Null Initialization,  Up: Better Iterative Traversal

4.9.3.2 Starting at the First Node
..................................

To initialize a traverser to start at the least valued node, we simply
descend from the root as far down and left as possible, recording the
parent pointers on the stack as we go.  If the stack overflows, then we
balance the tree and start over.

66. <BST traverser least-item initializer 66> =
void *
bst_t_first (struct bst_traverser *trav, struct bst_table *tree)
{
  struct bst_node *x;

  assert (tree != NULL && trav != NULL);

  trav->bst_table = tree;
  trav->bst_height = 0;
  trav->bst_generation = tree->bst_generation;

  x = tree->bst_root;
  if (x != NULL)
    while (x->bst_link[0] != NULL)
      {
        if (trav->bst_height >= BST_MAX_HEIGHT)
          {
            bst_balance (tree);
            return bst_t_first (trav, tree);
          }

        trav->bst_stack[trav->bst_height++] = x;
        x = x->bst_link[0];
      }
  trav->bst_node = x;

  return x != NULL ? x->bst_data : NULL;
}
   This code is included in *Note 64::.

Exercises:

*1. Show that bst_t_first() will never make more than one recursive call
to itself at a time.  [*Note answer: 4-9-3-2#1..]


File: libavl.info,  Node: BST Traverser Last Initialization,  Next: BST Traverser Find Initialization,  Prev: BST Traverser First Initialization,  Up: Better Iterative Traversal

4.9.3.3 Starting at the Last Node
.................................

The code to start from the greatest node in the tree is analogous to
that for starting from the least node.  The only difference is that we
descend to the right instead:

67. <BST traverser greatest-item initializer 67> =
void *
bst_t_last (struct bst_traverser *trav, struct bst_table *tree)
{
  struct bst_node *x;

  assert (tree != NULL && trav != NULL);

  trav->bst_table = tree;
  trav->bst_height = 0;
  trav->bst_generation = tree->bst_generation;

  x = tree->bst_root;
  if (x != NULL)
    while (x->bst_link[1] != NULL)
      {
        if (trav->bst_height >= BST_MAX_HEIGHT)
          {
            bst_balance (tree);
            return bst_t_last (trav, tree);
          }

        trav->bst_stack[trav->bst_height++] = x;
        x = x->bst_link[1];
      }
  trav->bst_node = x;

  return x != NULL ? x->bst_data : NULL;
}
   This code is included in *Note 64::.


File: libavl.info,  Node: BST Traverser Find Initialization,  Next: BST Traverser Insert Initialization,  Prev: BST Traverser Last Initialization,  Up: Better Iterative Traversal

4.9.3.4 Starting at a Found Node
................................

Sometimes it is convenient to begin a traversal at a particular item in
a tree.  This function works in the same was as bst_find(), but records
parent pointers in the traverser structure as it descends the tree.

68. <BST traverser search initializer 68> =
void *
bst_t_find (struct bst_traverser *trav, struct bst_table *tree, void *item)
{
  struct bst_node *p, *q;

  assert (trav != NULL && tree != NULL && item != NULL);
  trav->bst_table = tree;
  trav->bst_height = 0;
  trav->bst_generation = tree->bst_generation;
  for (p = tree->bst_root; p != NULL; p = q)
    {
      int cmp = tree->bst_compare (item, p->bst_data, tree->bst_param);

      if (cmp < 0)
        q = p->bst_link[0];
      else if (cmp > 0)
        q = p->bst_link[1];
      else /* cmp == 0 */
        {
          trav->bst_node = p;
          return p->bst_data;
        }

      if (trav->bst_height >= BST_MAX_HEIGHT)
        {
          bst_balance (trav->bst_table);
          return bst_t_find (trav, tree, item);
        }
      trav->bst_stack[trav->bst_height++] = p;
    }

  trav->bst_height = 0;
  trav->bst_node = NULL;
  return NULL;
}
   This code is included in *Note 64::.


File: libavl.info,  Node: BST Traverser Insert Initialization,  Next: BST Traverser Copying,  Prev: BST Traverser Find Initialization,  Up: Better Iterative Traversal

4.9.3.5 Starting at an Inserted Node
....................................

Another operation that can be useful is to insert a new node and
construct a traverser to the inserted node in a single operation.  The
following code does this:

69. <BST traverser insertion initializer 69> =
void *
bst_t_insert (struct bst_traverser *trav, struct bst_table *tree, void *item)
{
  struct bst_node **q;

  assert (tree != NULL && item != NULL);

  trav->bst_table = tree;
  trav->bst_height = 0;

  q = &tree->bst_root;
  while (*q != NULL)
    {
      int cmp = tree->bst_compare (item, (*q)->bst_data, tree->bst_param);
      if (cmp == 0)
        {
          trav->bst_node = *q;
          trav->bst_generation = tree->bst_generation;
          return (*q)->bst_data;
        }

      if (trav->bst_height >= BST_MAX_HEIGHT)
        {
          bst_balance (tree);
          return bst_t_insert (trav, tree, item);
        }
      trav->bst_stack[trav->bst_height++] = *q;

      q = &(*q)->bst_link[cmp > 0];
    }

  trav->bst_node = *q = tree->bst_alloc->libavl_malloc (tree->bst_alloc,
                                                        sizeof **q);
  if (*q == NULL)
    {
      trav->bst_node = NULL;
      trav->bst_generation = tree->bst_generation;
      return NULL;
    }

  (*q)->bst_link[0] = (*q)->bst_link[1] = NULL;
  (*q)->bst_data = item;
  tree->bst_count++;
  trav->bst_generation = tree->bst_generation;
  return (*q)->bst_data;
}
   This code is included in *Note 64::.


File: libavl.info,  Node: BST Traverser Copying,  Next: BST Traverser Advancing,  Prev: BST Traverser Insert Initialization,  Up: Better Iterative Traversal

4.9.3.6 Initialization by Copying
.................................

This function copies one traverser to another.  It only copies the stack
of parent pointers if they are up-to-date:

70. <BST traverser copy initializer 70> =
void *
bst_t_copy (struct bst_traverser *trav, const struct bst_traverser *src)
{
  assert (trav != NULL && src != NULL);

  if (trav != src)
    {
      trav->bst_table = src->bst_table;
      trav->bst_node = src->bst_node;
      trav->bst_generation = src->bst_generation;
      if (trav->bst_generation == trav->bst_table->bst_generation)
        {
          trav->bst_height = src->bst_height;
          memcpy (trav->bst_stack, (const void *) src->bst_stack,
                  sizeof *trav->bst_stack * trav->bst_height);
        }
    }

  return trav->bst_node != NULL ? trav->bst_node->bst_data : NULL;
}
   This code is included in *Note 64:: and *Note 180::.

Exercises:

1. Without the check that trav != src before copying src into trav,
what might happen?  [*Note answer: 4-9-3-6#1..]


File: libavl.info,  Node: BST Traverser Advancing,  Next: BST Traverser Retreating,  Prev: BST Traverser Copying,  Up: Better Iterative Traversal

4.9.3.7 Advancing to the Next Node
..................................

The algorithm of bst_t_next(), the function for finding a successor,
divides neatly into three cases.  Two of these are the ones that we
discussed earlier in the introduction to this kind of traverser (*note
Better Iterative Traversal::).  The third case occurs when the last
node returned was NULL, in which case we return the least node in the
table, in accordance with the semantics for libavl tables.  The
function outline is this:

71. <BST traverser advance function 71> =
void *
bst_t_next (struct bst_traverser *trav)
{
  struct bst_node *x;

  assert (trav != NULL);

  if (trav->bst_generation != trav->bst_table->bst_generation)
    trav_refresh (trav);

  x = trav->bst_node;
  if (x == NULL)
    {
      return bst_t_first (trav, trav->bst_table);
    }
  else if (x->bst_link[1] != NULL)
    {
      <*Note Handle case where x has a right child: 72.>
    }
  else
    {
      <*Note Handle case where x has no right child: 73.>
    }
  trav->bst_node = x;

  return x->bst_data;
}
   This code is included in *Note 64::.

   The case where the current node has a right child is accomplished by
stepping to the right, then to the left until we can't go any farther,
as discussed in detail earlier.  The only difference is that we must
check for stack overflow.  When stack overflow does occur, we recover by
calling trav_balance(), then restarting bst_t_next() using a
tail-recursive call.  The tail recursion will never happen more than
once, because trav_balance() ensures that the tree's height is small
enough that the stack cannot overflow again:

72. <Handle case where x has a right child 72> =
if (trav->bst_height >= BST_MAX_HEIGHT)
  {
    bst_balance (trav->bst_table);
    return bst_t_next (trav);
  }

trav->bst_stack[trav->bst_height++] = x;
x = x->bst_link[1];

while (x->bst_link[0] != NULL)
  {
    if (trav->bst_height >= BST_MAX_HEIGHT)
      {
        bst_balance (trav->bst_table);
        return bst_t_next (trav);
      }

    trav->bst_stack[trav->bst_height++] = x;
    x = x->bst_link[0];
  }
   This code is included in *Note 71::.

   In the case where the current node has no right child, we move
upward in the tree based on the stack of parent pointers that we saved,
as described before.  When the stack underflows, we know that we've run
out of nodes in the tree:

73. <Handle case where x has no right child 73> =
struct bst_node *y;

do
  {
    if (trav->bst_height == 0)
      {
        trav->bst_node = NULL;
        return NULL;
      }

    y = x;
    x = trav->bst_stack[--trav->bst_height];
  }
while (y == x->bst_link[1]);
   This code is included in *Note 71::.


File: libavl.info,  Node: BST Traverser Retreating,  Next: BST Traversal Current Item,  Prev: BST Traverser Advancing,  Up: Better Iterative Traversal

4.9.3.8 Backing Up to the Previous Node
.......................................

Moving to the previous node is analogous to moving to the next node.
The only difference, in fact, is that directions are reversed from left
to right.

74. <BST traverser back up function 74> =
void *
bst_t_prev (struct bst_traverser *trav)
{
  struct bst_node *x;

  assert (trav != NULL);

  if (trav->bst_generation != trav->bst_table->bst_generation)
    trav_refresh (trav);

  x = trav->bst_node;
  if (x == NULL)
    {
      return bst_t_last (trav, trav->bst_table);
    }
  else if (x->bst_link[0] != NULL)
    {
      if (trav->bst_height >= BST_MAX_HEIGHT)
        {
          bst_balance (trav->bst_table);
          return bst_t_prev (trav);
        }

      trav->bst_stack[trav->bst_height++] = x;
      x = x->bst_link[0];

      while (x->bst_link[1] != NULL)
        {
          if (trav->bst_height >= BST_MAX_HEIGHT)
            {
              bst_balance (trav->bst_table);
              return bst_t_prev (trav);
            }

          trav->bst_stack[trav->bst_height++] = x;
          x = x->bst_link[1];
        }
    }
  else
    {
      struct bst_node *y;

      do
        {
          if (trav->bst_height == 0)
            {
              trav->bst_node = NULL;
              return NULL;
            }

          y = x;
          x = trav->bst_stack[--trav->bst_height];
        }
      while (y == x->bst_link[0]);
    }
  trav->bst_node = x;

  return x->bst_data;
}
   This code is included in *Note 64::.


File: libavl.info,  Node: BST Traversal Current Item,  Next: BST Traversal Replacing the Current Item,  Prev: BST Traverser Retreating,  Up: Better Iterative Traversal

4.9.3.9 Getting the Current Item
................................

75. <BST traverser current item function 75> =
void *
bst_t_cur (struct bst_traverser *trav)
{
  assert (trav != NULL);

  return trav->bst_node != NULL ? trav->bst_node->bst_data : NULL;
}
This code is included in *Note 64::, *Note 180::, *Note 270::, *Note
397::, *Note 504::, and *Note 548::.


File: libavl.info,  Node: BST Traversal Replacing the Current Item,  Prev: BST Traversal Current Item,  Up: Better Iterative Traversal

4.9.3.10 Replacing the Current Item
...................................

76. <BST traverser replacement function 76> =
void *
bst_t_replace (struct bst_traverser *trav, void *new)
{
  void *old;

  assert (trav != NULL && trav->bst_node != NULL && new != NULL);
  old = trav->bst_node->bst_data;
  trav->bst_node->bst_data = new;
  return old;
}
This code is included in *Note 64::, *Note 180::, *Note 270::, *Note
397::, *Note 504::, and *Note 548::.


File: libavl.info,  Node: Copying a BST,  Next: Destroying a BST,  Prev: Traversing a BST,  Up: Binary Search Trees

4.10 Copying
============

In this section, we're going to write function bst_copy() to make a
copy of a binary tree.  This is the most complicated function of all
those needed for BST functionality, so pay careful attention as we
proceed.

* Menu:

* Copying a BST Recursively::
* Copying a BST Iteratively::
* Handling Errors in Iterative BST Copying::


File: libavl.info,  Node: Copying a BST Recursively,  Next: Copying a BST Iteratively,  Prev: Copying a BST,  Up: Copying a BST

4.10.1 Recursive Copying
------------------------

The "obvious" way to copy a binary tree is recursive.  Here's a basic
recursive copy, hard-wired to allocate memory with malloc() for
simplicity:

77. <Recursive copy of BST, take 1 77> =
/* Makes and returns a new copy of tree rooted at x. */
static struct bst_node *
bst_copy_recursive_1 (struct bst_node *x)
{
  struct bst_node *y;

  if (x == NULL)
    return NULL;

  y = malloc (sizeof *y);
  if (y == NULL)
    return NULL;

  y->bst_data = x->bst_data;
  y->bst_link[0] = bst_copy_recursive_1 (x->bst_link[0]);
  y->bst_link[1] = bst_copy_recursive_1 (x->bst_link[1]);
  return y;
}

   But, again, it would be nice to rewrite this iteratively, both
because the iterative version is likely to be faster and for the sheer
mental exercise of it.  Recall, from our earlier discussion of inorder
traversal, that tail recursion (recursion where a function calls itself
as its last action) is easier to convert to iteration than other types.
Unfortunately, neither of the recursive calls above are tail-recursive.

   Fortunately, we can rewrite it so that it is, if we change the way we
allocate data:

78. <Recursive copy of BST, take 2 78> =
/* Copies tree rooted at x to y, which latter is allocated but not
   yet initialized. */
static void
bst_copy_recursive_2 (struct bst_node *x, struct bst_node *y)
{
  y->bst_data = x->bst_data;

  if (x->bst_link[0] != NULL)
    {
      y->bst_link[0] = malloc (sizeof *y->bst_link[0]);
      bst_copy_recursive_2 (x->bst_link[0], y->bst_link[0]);
    }
  else
    y->bst_link[0] = NULL;

  if (x->bst_link[1] != NULL)
    {
      y->bst_link[1] = malloc (sizeof *y->bst_link[1]);
      bst_copy_recursive_2 (x->bst_link[1], y->bst_link[1]);
    }
  else
    y->bst_link[1] = NULL;
}

Exercises:

1. When malloc() returns a null pointer, bst_copy_recursive_1() fails
"silently", that is, without notifying its caller about the error, and
the output is a partial copy of the original tree.  Without removing the
recursion, implement two different ways to propagate such errors upward
to the function's caller:

  a. Change the function's prototype to:

         static int bst_robust_copy_recursive_1 (struct bst_node *,
                                                 struct bst_node **);

  b. Without changing the function's prototype.  (Hint: use a statically
     declared struct bst_node).

In each case make sure that any allocated memory is safely freed if an
allocation error occurs.  [*Note answer: 4-10-1#1..]

2. bst_copy_recursive_2() is even worse than bst_copy_recursive_1() at
handling allocation failure.  It actually invokes undefined behavior
when an allocation fails.  Fix this, changing it to return an int, with
nonzero return values indicating success.  Be careful not to leak
memory.  [*Note answer: 4-10-1#2..]


File: libavl.info,  Node: Copying a BST Iteratively,  Next: Handling Errors in Iterative BST Copying,  Prev: Copying a BST Recursively,  Up: Copying a BST

4.10.2 Iterative Copying
------------------------

Now we can factor out the recursion, starting with the tail recursion.
This process is very similar to what we did with the traversal code, so
the details are left for Exercise 1.  Let's look at the results part by
part:

79. <Iterative copy of BST 79> =
/* Copies org to a newly created tree, which is returned. */
struct bst_table *
bst_copy_iterative (const struct bst_table *org)
{
  struct bst_node *stack[2 * (BST_MAX_HEIGHT + 1)];
                                     /* Stack. */
  int height = 0;                    /* Stack height. */
   See also *Note 80::, *Note 81::, and*Note 82::.

   This time, our stack will have two pointers added to it at a time,
one from the original tree and one from the copy.  Thus, the stack
needs to be twice as big.  In addition, we'll see below that there'll
be an extra item on the stack representing the pointer to the tree's
root, so our stack needs room for an extra pair of items, which is the
reason for the "+ 1" in stack[]'s size.

80. <Iterative copy of BST 79> +=
  struct bst_table *new;             /* New tree. */
  const struct bst_node *x;          /* Node currently being copied. */
  struct bst_node *y;                /* New node being copied from x. */

  new = bst_create (org->bst_compare, org->bst_param, org->bst_alloc);
  new->bst_count = org->bst_count;
  if (new->bst_count == 0)
    return new;

  x = (const struct bst_node *) &org->bst_root;
  y = (struct bst_node *) &new->bst_root;

   This is the same kind of "dirty trick" already described in Exercise
4.7-1.

81. <Iterative copy of BST 79> +=
  for (;;)
    {
      while (x->bst_link[0] != NULL)
        {
          y->bst_link[0]
            = org->bst_alloc->libavl_malloc (org->bst_alloc,
                                             sizeof *y->bst_link[0]);
          stack[height++] = (struct bst_node *) x;
          stack[height++] = y;
          x = x->bst_link[0];
          y = y->bst_link[0];
        }
      y->bst_link[0] = NULL;

   This code moves x down and to the left in the tree until it runs out
of nodes, allocating space in the new tree for left children and pushing
nodes from the original tree and the copy onto the stack as it goes.
The cast on x suppresses a warning or error due to x, a pointer to a
const structure, being stored into a non-constant pointer in stack[].
We won't ever try to store into the pointer that we store in there, so
this is legitimate.

   We've switched from using malloc() to using the allocation function
provided by the user.  This is easy now because we have the tree
structure to work with.  To do this earlier, we would have had to
somehow pass the tree structure to each recursive call of the copy
function, wasting time and space.

82. <Iterative copy of BST 79> +=
      for (;;)
        {
          y->bst_data = x->bst_data;

          if (x->bst_link[1] != NULL)
            {
              y->bst_link[1] =
                org->bst_alloc->libavl_malloc (org->bst_alloc,
                                              sizeof *y->bst_link[1]);
              x = x->bst_link[1];
              y = y->bst_link[1];
              break;
            }
          else
            y->bst_link[1] = NULL;

          if (height <= 2)
            return new;

          y = stack[--height];
          x = stack[--height];
        }
    }
}

   We do not pop the bottommost pair of items off the stack because
these items contain the fake struct bst_node pointer that is actually
the address of bst_root.  When we get down to these items, we're done
copying and can return the new tree.

See also:  *Note Knuth 1997::, algorithm 2.3.1C; *Note ISO 1990::,
section 6.5.2.1.

Exercises:

1. Suggest a step between bst_copy_recursive_2() and
bst_copy_iterative().  [*Note answer: 4-10-2#1..]


File: libavl.info,  Node: Handling Errors in Iterative BST Copying,  Prev: Copying a BST Iteratively,  Up: Copying a BST

4.10.3 Error Handling
---------------------

So far, outside the exercises, we've ignored the question of handling
memory allocation errors during copying.  In our other routines, we've
been careful to implement to handle allocation failures by cleaning up
and returning an error indication to the caller.  Now we will apply this
same policy to tree copying, as libavl semantics require (*note
Creation and Destruction::): a memory allocation error causes the
partially copied tree to be destroyed and returns a null pointer to the
caller.

   This is a little harder to do than recovering after a single
operation, because there are potentially many nodes that have to be
freed, and each node might include additional user data that also has
to be freed.  The new BST might have as-yet-uninitialized pointer
fields as well, and we must be careful to avoid reading from these
fields as we destroy the tree.

   We could use a number of strategies to destroy the partially copied
tree while avoiding uninitialized pointers.  The strategy that we will
actually use is to initialize these pointers to NULL, then call the
general tree destruction routine bst_destroy().  We haven't yet written
bst_destroy(), so for now we'll treat it as a "black box" (*note black
box::) that does what we want, even if we don't understand how.

   Next question: _which_ pointers in the tree are not initialized?
The answer is simple: during the copy, we will not revisit nodes not
currently on the stack, so only pointers in the current node (y) and on
the stack can be uninitialized.  For its part, depending on what we're
doing to it, y might not have any of its fields initialized.  As for
the stack, nodes are pushed onto it because we have to come back later
and build their right subtrees, so we must set their right child
pointers to NULL.

   We will need this error recovery code in a number of places, so it is
worth making it into a small helper function:

83. <BST copy error helper function 83> =
static void
copy_error_recovery (struct bst_node **stack, int height,
                     struct bst_table *new, bst_item_func *destroy)
{
  assert (stack != NULL && height >= 0 && new != NULL);

  for (; height > 2; height -= 2)
    stack[height - 1]->bst_link[1] = NULL;
  bst_destroy (new, destroy);
}
   This code is included in *Note 84:: and *Note 187::.

   Another problem that can arise in copying a binary tree is stack
overflow.  We will handle stack overflow by destroying the partial copy,
balancing the original tree, and then restarting the copy.  The balanced
tree is guaranteed to have small enough height that it will not overflow
the stack.

   The code below for our final version of bst_copy() takes three new
parameters: two function pointers and a memory allocator.  The meaning
of these parameters was explained earlier (*note Creation and
Destruction::).  Their use within the function should be
self-explanatory.

84. <BST copy function 84> =
<*Note BST copy error helper function: 83.>

struct bst_table *
bst_copy (const struct bst_table *org, bst_copy_func *copy,
          bst_item_func *destroy, struct libavl_allocator *allocator)
{
  struct bst_node *stack[2 * (BST_MAX_HEIGHT + 1)];
  int height = 0;

  struct bst_table *new;
  const struct bst_node *x;
  struct bst_node *y;

  assert (org != NULL);
  new = bst_create (org->bst_compare, org->bst_param,
                    allocator != NULL ? allocator : org->bst_alloc);
  if (new == NULL)
    return NULL;
  new->bst_count = org->bst_count;
  if (new->bst_count == 0)
    return new;

  x = (const struct bst_node *) &org->bst_root;
  y = (struct bst_node *) &new->bst_root;
  for (;;)
    {
      while (x->bst_link[0] != NULL)
        {
          if (height >= 2 * (BST_MAX_HEIGHT + 1))
            {
              y->bst_data = NULL;
              y->bst_link[0] = y->bst_link[1] = NULL;
              copy_error_recovery (stack, height, new, destroy);

              bst_balance ((struct bst_table *) org);
              return bst_copy (org, copy, destroy, allocator);
            }

          y->bst_link[0] =
            new->bst_alloc->libavl_malloc (new->bst_alloc,
                                           sizeof *y->bst_link[0]);
          if (y->bst_link[0] == NULL)
            {
              if (y != (struct bst_node *) &new->bst_root)
                {
                  y->bst_data = NULL;
                  y->bst_link[1] = NULL;
                }

              copy_error_recovery (stack, height, new, destroy);
              return NULL;
            }

          stack[height++] = (struct bst_node *) x;
          stack[height++] = y;
          x = x->bst_link[0];
          y = y->bst_link[0];
        }
      y->bst_link[0] = NULL;

      for (;;)
        {
          if (copy == NULL)
            y->bst_data = x->bst_data;
          else
            {
              y->bst_data = copy (x->bst_data, org->bst_param);
              if (y->bst_data == NULL)
                {
                  y->bst_link[1] = NULL;
                  copy_error_recovery (stack, height, new, destroy);
                  return NULL;
                }
            }

          if (x->bst_link[1] != NULL)
            {
              y->bst_link[1] =
                new->bst_alloc->libavl_malloc (new->bst_alloc,
                                               sizeof *y->bst_link[1]);
              if (y->bst_link[1] == NULL)
                {
                  copy_error_recovery (stack, height, new, destroy);
                  return NULL;
                }

              x = x->bst_link[1];
              y = y->bst_link[1];
              break;
            }
          else
            y->bst_link[1] = NULL;

          if (height <= 2)
            return new;

          y = stack[--height];
          x = stack[--height];
        }
    }
}
   This code is included in *Note 30::.


File: libavl.info,  Node: Destroying a BST,  Next: Balancing a BST,  Prev: Copying a BST,  Up: Binary Search Trees

4.11 Destruction
================

Eventually, we'll want to get rid of the trees we've spent all this time
constructing.  When this happens, it's time to destroy them by freeing
their memory.

* Menu:

* Destroying a BST by Rotation::
* Destroying a BST Recursively::
* Destroying a BST Iteratively::


File: libavl.info,  Node: Destroying a BST by Rotation,  Next: Destroying a BST Recursively,  Prev: Destroying a BST,  Up: Destroying a BST

4.11.1 Destruction by Rotation
------------------------------

The method actually used in libavl for destruction of binary trees is
somewhat novel.  This section will cover this method.  Later sections
will cover more conventional techniques using recursive or iterative
"postorder traversal" (*note postorder traversal::).

   To destroy a binary tree, we must visit and free each node.  We have
already covered one way to traverse a tree (inorder traversal) and used
this technique for traversing and copying a binary tree.  But, both
times before, we were subject to both the explicit constraint that we
had to visit the nodes in sorted order and the implicit constraint that
we were not to change the structure of the tree, or at least not to
change it for the worse.

   Neither of these constraints holds for destruction of a binary tree.
As long as the tree finally ends up freed, it doesn't matter how much
it is mangled in the process.  In this case, "the end justifies the
means" and we are free to do it however we like.

   So let's consider why we needed a stack before.  It was to keep
track of nodes whose left subtree we were currently visiting, in order
to go back later and visit them and their right subtrees.  Hmm...what
if we rearranged nodes so that they _didn't have_ any left subtrees?
Then we could just descend to the right, without need to keep track of
anything on a stack.

   We can do this.  For the case where the current node p has a left
child q, consider the transformation below where we rotate right at p:

 [image src="destroy.png" text="                               p        q
                              / \\      / \\
                             q   c => a   p
                             ^            ^
                            a b          b c

" ]

where a, b, and c are arbitrary subtrees or even empty trees.  This
transformation shifts nodes from the left to the right side of the root
(which is now q).  If it is performed enough times, the root node will
no longer have a left child.  After the transformation, q becomes the
current node.

   For the case where the current node has no left child, we can just
destroy the current node and descend to its right.  Because the
transformation used does not change the tree's ordering, we end up
destroying nodes in inorder.  It is instructive to verify this by
simulating with paper and pencil the destruction of a few trees this
way.

   The code to implement destruction in this manner is brief and
straightforward:

85. <BST destruction function 85> =
void
bst_destroy (struct bst_table *tree, bst_item_func *destroy)
{
  struct bst_node *p, *q;

  assert (tree != NULL);

  for (p = tree->bst_root; p != NULL; p = q)
    if (p->bst_link[0] == NULL)
      {
        q = p->bst_link[1];
        if (destroy != NULL && p->bst_data != NULL)
          destroy (p->bst_data, tree->bst_param);
        tree->bst_alloc->libavl_free (tree->bst_alloc, p);
      }
    else
      {
        q = p->bst_link[0];
        p->bst_link[0] = q->bst_link[1];
        q->bst_link[1] = p;
      }

  tree->bst_alloc->libavl_free (tree->bst_alloc, tree);
}
   This code is included in *Note 30::, *Note 147::, *Note 198::, *Note
491::, *Note 524::, and *Note 556::.

See also:  *Note Stout 1986::, tree_to_vine procedure.

Exercises:

1. Before calling destroy() above, we first test that we are not passing
it a NULL pointer, because we do not want destroy() to have to deal
with this case.  How can such a pointer get into the tree in the first
place, since bst_probe() refuses to insert such a pointer into a tree?
[*Note answer: 4-11-1#1..]


File: libavl.info,  Node: Destroying a BST Recursively,  Next: Destroying a BST Iteratively,  Prev: Destroying a BST by Rotation,  Up: Destroying a BST

4.11.2 Aside: Recursive Destruction
-----------------------------------

The algorithm used in the previous section is easy and fast, but it is
not the most common method for destroying a tree.  The usual way is to
perform a traversal of the tree, in much the same way we did for tree
traversal and copying.  Once again, we'll start from a recursive
implementation, because these are so easy to write.  The only tricky
part is that subtrees have to be freed _before_ the root.  This code is
hard-wired to use free() for simplicity:

86. <Destroy a BST recursively 86> =
static void
bst_destroy_recursive (struct bst_node *node)
{
  if (node == NULL)
    return;

  bst_destroy_recursive (node->bst_link[0]);
  bst_destroy_recursive (node->bst_link[1]);
  free (node);
}


File: libavl.info,  Node: Destroying a BST Iteratively,  Prev: Destroying a BST Recursively,  Up: Destroying a BST

4.11.3 Aside: Iterative Destruction
-----------------------------------

As we've done before for other algorithms, we can factor the recursive
destruction algorithm into an equivalent iteration.  In this case,
neither recursive call is tail recursive, and we can't easily modify
the code so that it is.  We could still factor out the recursion by our
usual methods, although it would be more difficult, but this problem is
simple enough to figure out from first principles.  Let's do it that
way, instead, this time.

   The idea is that, for the tree's root, we traverse its left subtree,
then its right subtree, then free the root.  This pattern is called a
"postorder traversal" (*note postorder traversal::).

   Let's think about how much state we need to keep track of.  When
we're traversing the root's left subtree, we still need to remember the
root, in order to come back to it later.  The same is true while
traversing the root's right subtree, because we still need to come back
to free the root.  What's more, we need to keep track of what state
we're in: have we traversed the root's left subtree or not, have we
traversed the root's right subtree or not?

   This naturally suggests a stack that holds two-part items (root,
state), where root is the root of the tree or subtree and state is the
state of the traversal at that node.  We start by selecting the tree's
root as our current node p, then pushing (p, 0) onto the stack and
moving down to the left as far as we can, pushing as we go.  Then we
start popping off the stack into (p, state) and notice that state is 0,
which tells us that we've traversed p's left subtree but not its right.
So, we push (p, 1) back onto the stack, then we traverse p's right
subtree.  When, later, we pop off that same node back off the stack,
the 1 tells us that we've already traversed both subtrees, so we free
the node and keep popping.  The pattern follows as we continue back up
the tree.

   That sounds pretty complicated, so let's work through an example to
help clarify.  Consider this binary search tree:

 [image src="traversal.png" text="                                    4
                                   / \\
                                  2   5
                                  ^
                                 1 3

" ]

Abstractly speaking, we start with 4 as p and an empty stack.  First,
we work our way down the left-child pointers, pushing onto the stack as
we go.  We push (4, 0), then (2, 0), then (1, 0), and then p is NULL
and we've fallen off the bottom of the tree.  We pop the top item off
the stack into (p, state), getting (1, 0).  Noticing that we have 0 for
state, we push (1, 1) on the stack and traverse 1's right subtree, but
it is empty so there is nothing to do.  We pop again and notice that
state is 1, meaning that we've fully traversed 1's subtrees, so we free
node 1.  We pop again, getting 2 for p and 0 for state.  Because state
is 0, we push (2, 1) and traverse 2's right subtree, which means that
we push (3, 0).  We traverse 3's null right subtree (again, it is empty
so there is nothing to do), pushing and popping (3, 1), then free node
3, then move back up to 2.  Because we've traversed 2's right subtree,
state is 1 and p is 2, and we free node 2.  You should be able to
figure out how 4 and 5 get freed.

   A straightforward implementation of this approach looks like this:

87. <Destroy a BST iteratively 87> =
void
bst_destroy (struct bst_table *tree, bst_item_func *destroy)
{
  struct bst_node *stack[BST_MAX_HEIGHT];
  unsigned char state[BST_MAX_HEIGHT];
  int height = 0;

  struct bst_node *p;

  assert (tree != NULL);
  p = tree->bst_root;
  for (;;)
    {
      while (p != NULL)
        {
          if (height >= BST_MAX_HEIGHT)
            {
              fprintf (stderr, "tree too deep\n");
              exit (EXIT_FAILURE);
            }
          stack[height] = p;
          state[height] = 0;
          height++;

          p = p->bst_link[0];
        }

      for (;;)
        {
          if (height == 0)
            {
              tree->bst_alloc->libavl_free (tree->bst_alloc, tree);
              return;
            }

          height--;
          p = stack[height];
          if (state[height] == 0)
            {
              state[height++] = 1;
              p = p->bst_link[1];
              break;
            }
          else
            {
              if (destroy != NULL && p->bst_data != NULL)
                destroy (p->bst_data, tree->bst_param);
              tree->bst_alloc->libavl_free (tree->bst_alloc, p);
            }
        }
    }
}

See also:  *Note Knuth 1997::, exercise 13 in section 2.3.1.


File: libavl.info,  Node: Balancing a BST,  Next: Joining BSTs,  Prev: Destroying a BST,  Up: Binary Search Trees

4.12 Balance
============

Sometimes binary trees can grow to become much taller than their optimum
height.  For example, the following binary tree was one of the tallest
from a sample of 100 15-node trees built by inserting nodes in random
order:

 [image src="bal1.png" text="                       0
                        `-..__
                              5
                            _' `---...___
                           3             12
                         _' \\     __..--'  \\
                        1    4   7          13
                         \\      / `-.__       \\
                          2    6       11      14
                                   _.-'
                                  8
                                   `_
                                     10
                                    /
                                   9

" ]

The average number of comparisons required to find a random node in this
tree is (1 + 2 + (3 * 2) + (4 * 4) + (5 * 4) + 6 + 7 + 8) / 15 = 4.4
comparisons.  In contrast, the corresponding optimal binary tree, shown
below, requires only (1 + (2 * 2) + (3 * 4) + (4 * 8))/15 = 3.3
comparisons, on average.  Moreover, the optimal tree requires a maximum
of 4, as opposed to 8, comparisons for any search:

 [image src="bal2.png" text="                                7
                             _.' `-..__
                            3          11
                           / \\      _.'  `_
                          1   5    9       13
                          ^   ^   / \\     /  \\
                         0 2 4 6 8   10  12   14

" ]

Besides this inefficiency in time, trees that grow too tall can cause
inefficiency in space, leading to an overflow of the stack in
bst_t_next(), bst_copy(), or other functions.  For both reasons, it is
helpful to have a routine to rearrange a tree to its minimum possible
height, that is, to "balance" (*note balance::) the tree.

   The algorithm we will use for balancing proceeds in two stages.  In
the first stage, the binary tree is "flattened" into a pathological,
linear binary tree, called a "vine."  In the second stage, binary tree
structure is restored by repeatedly "compressing" the vine into a
minimal-height binary tree.

   Here's a top-level view of the balancing function:

88. <BST balance function 88> =
<*Note BST to vine function: 90.>
<*Note Vine to balanced BST function: 91.>

void
bst_balance (struct bst_table *tree)
{
  assert (tree != NULL);

  tree_to_vine (tree);
  vine_to_tree (tree);
  tree->bst_generation++;
}
   This code is included in *Note 30::.

89. <BST extra function prototypes 89> =

/* Special BST functions. */
void bst_balance (struct bst_table *tree);
   This code is included in *Note 25::, *Note 249::, *Note 374::, and
*Note 488::.

See also:  *Note Stout 1986::, rebalance procedure.

* Menu:

* Transforming a BST into a Vine::
* Transforming a Vine into a Balanced BST::


File: libavl.info,  Node: Transforming a BST into a Vine,  Next: Transforming a Vine into a Balanced BST,  Prev: Balancing a BST,  Up: Balancing a BST

4.12.1 From Tree to Vine
------------------------

The first stage of balancing converts a binary tree into a linear
structure resembling a linked list, called a "vine" (*note vine::).
The vines we will create have the greatest value in the binary tree at
the root and decrease descending to the left.  Any binary search tree
that contains a particular set of values, no matter its shape,
corresponds to the same vine of this type.  For instance, all binary
search trees of the integers 0...4 will be transformed into the
following vine:

 [image src="vine.png" text="                                        4
                                       /
                                      3
                                     /
                                    2
                                   /
                                  1
                                 /
                                0

" ]

The method for transforming a tree into a vine of this type is similar
to that used for destroying a tree by rotation (*note Destroying a BST
by Rotation::).  We step pointer p through the tree, starting at the
root of the tree, maintaining pointer q as p's parent.  (Because we're
building a vine, p is always the left child of q.)  At each step, we do
one of two things:

   * If p has no right child, then this part of the tree is already the
     shape we want it to be.  We step p and q down to the left and
     continue.

   * If p has a right child r, then we rotate left at p, performing the
     following transformation:

      [image src="tree2vine.png" text="                                 |          |
                                      q          q
                                   _.'         _'
                                  p           r
                                 / \\    =>   / \\
                                a   r       p   c
                                    ^       ^
                                   b c     a b

     " ]

     where a, b, and c are arbitrary subtrees or empty trees.  Node r
     then becomes the new p.  If c is an empty tree, then, in the next
     step, we will continue down the tree.  Otherwise, the right
     subtree of p is smaller (contains fewer nodes) than previously, so
     we're on the right track.

   This is all it takes:

90. <BST to vine function 90> =
/* Converts tree into a vine. */
static void
tree_to_vine (struct bst_table *tree)
{
  struct bst_node *q, *p;

  q = (struct bst_node *) &tree->bst_root;
  p = tree->bst_root;
  while (p != NULL)
    if (p->bst_link[1] == NULL)
      {
        q = p;
        p = p->bst_link[0];
      }
    else
      {
        struct bst_node *r = p->bst_link[1];
        p->bst_link[1] = r->bst_link[0];
        r->bst_link[0] = p;
        p = r;
        q->bst_link[0] = r;
      }
}
   This code is included in *Note 88::, *Note 513::, and *Note 681::.

See also:  *Note Stout 1986::, tree_to_vine procedure.


File: libavl.info,  Node: Transforming a Vine into a Balanced BST,  Prev: Transforming a BST into a Vine,  Up: Balancing a BST

4.12.2 From Vine to Balanced Tree
---------------------------------

Converting the vine, once we have it, into a balanced tree is the
interesting and clever part of the balancing operation.  However, at
first it may be somewhat less than obvious how this is actually done.
We will tackle the subject by presenting an example, then the
generalized form.

   Suppose we have a vine, as above, with 2**n - 1 nodes for positive
integer n.  For the sake of example, take n = 4, corresponding to a
tree with 15 nodes.  We convert this vine into a balanced tree by
performing three successive "compression" (*note compression::)
operations.

   To perform the first compression, move down the vine, starting at the
root.  Conceptually assign each node a "color", alternating between red
and black and starting with red at the root.(1) Then, take each red
node, except the bottommost, and remove it from the vine, making it the
child of its black former child node.

   After this transformation, we have something that looks a little more
like a tree.  Instead of a 15-node vine, we have a 7-node black vine
with a 7-node red vine as its right children and a single red node as
its left child.  Graphically, this first compression step on a 15-node
vine looks like this:

 [image src="vine2tree.png" text="                                                                   14
                                                                   <b>
                                                             __..-'   \\
               15                                           12         15
               <r>                                          <b>        <r>
             _'                                       __..-'   \\
            14                                       10         13
            <b>                                      <b>        <r>
          _'                                   __..-'   \\
         13                                    8         11
         <r>                                  <b>        <r>
       _'          =>                   __..-'   \\
      ...                               6          9
    _'                                 <b>        <r>
    2                            __..-'   \\
   <b>                           4          7
 _'                             <b>        <r>
 1                        __..-'   \\
<r>                       2          5
                         <b>        <r>
                       _'   \\
                       1      3
                      <r>    <r>

" ]

To perform the second compression, recolor all the red nodes to white,
then change the color of alternate black nodes to red, starting at the
root.  As before, extract each red node, except the bottommost, and
reattach it as the child of its black former child node.  Attach each
black node's right subtree as the left subtree of the corresponding red
node.  Thus, we have the following:

 [image src="vine2tree2.png" text="                                  14
                                  <r>
                             __.-'   \\
                            12        15
                            <b>                                         12
                       __.-'   \\                                        <b>
                      10        13                            ___...---'   `_
                      <r>                                     8              14
                  _.-'   \\                                   <b>             <r>
                  8       11                        ___...--'   `_          /   \\
                 <b>                                4             10       13    15
             _.-'   \\                    =>        <b>            <r>
             6       9                         _.-'   `_         /   \\
            <r>                                2         6      9     11
        _.-'   \\                              <r>       <r>
        4       7                            /   \\     /   \\
       <b>                                  1     3   5     7
   _.-'   \\
   2       5
  <r>
 /   \\
1     3

" ]

The third compression is the same as the first two.  Nodes 12 and 4 are
recolored red, then node 12 is removed and reattached as the right
child of its black former child node 8, receiving node 8's right subtree
as its left subtree:

 [image src="vine2tree3.png" text="                        12
                        <r>                     8
               ___...--'   `_                  <b>
               8             14           __.-'   `--..__
              <b>           /  \\          4              12
         __.-'   `_        13   15       <r>             <r>
         4         10              =>   /   \\        _.-'   `_
        <r>       /  \\                 2     6      10        14
       /   \\     9    11               ^     ^     /  \\      /  \\
      2     6                         1 3   5 7   9    11   13   15
      ^     ^
     1 3   5 7

" ]

The result is a fully balanced tree.

* Menu:

* Balancing General Trees::
* Balancing Implementation::
* Implementing Compression::

   ---------- Footnotes ----------

   (1) These colors are for the purpose of illustration only.  They are
not stored in the nodes and are not related to those used in a
"red-black tree" (*note red-black tree::).


File: libavl.info,  Node: Balancing General Trees,  Next: Balancing Implementation,  Prev: Transforming a Vine into a Balanced BST,  Up: Transforming a Vine into a Balanced BST

4.12.2.1 General Trees
......................

A compression is the repeated application of a right rotation, called
in this context a "compression transformation", once for each black
node, like so:

 [image src="compress.png" text="                              |          |
                              R          B
                             <r>        <b>
                         _.-'   \\      /   `_
                         B       c => a       R
                        <b>                  <r>
                       /   \\                /   \\
                      a     b              b     c

" ]

So far, all of the compressions we've performed have involved all 2**k
- 1 nodes composing the "main vine."  This works out well for an
initial vine of exactly 2**n - 1 nodes.  In this case, a total of n - 1
compressions are required, where for successive compressions k = n, n -
1, ..., 2.

   For trees that do not have exactly one fewer than a power of two
nodes, we need to begin with a compression that does not involve all of
the nodes in the vine.  Suppose that our vine has m nodes, where 2**n -
1 < m < 2**(n+1) - 1 for some value of n.  Then, by applying the
compression transformation shown above m - (2**n - 1) times, we reduce
the length of the main vine to exactly 2**n - 1 nodes.  After that, we
can treat the problem in the same way as the former case.  The result
is a balanced tree with n full levels of nodes, and a bottom level
containing m - (2**n - 1) nodes and (2**(n + 1) - 1) - m vacancies.

   An example is indicated.  Suppose that the vine contains m == 9 nodes
numbered from 1 to 9.  Then n == 3 since we have 2**3 - 1 = 7 < 9 < 15
= 2**4 - 1, and we must perform the compression transformation shown
above 9 - (2**3 - 1) = 2 times initially, reducing the main vine's
length to 7 nodes.  Afterward, we treat the problem the same way as for
a tree that started off with only 7 nodes, performing one compression
with k == 3 and one with k == 2.  The entire sequence, omitting the
initial vine, looks like this:

 [image src="balance9.png" text="                       8                    6
                      <r>                  <r>           4
                  _.-'   \\             _.-'   \\         <r>
                  6       9            4       8       /   `_
                 <b>                  <b>      ^      2      6
               _'   \\             _.-'   \\    7 9 =>  ^     / \\
               5     7      =>    2       5          1 3   5   8
              <r>                <r>                           ^
            _'                  /   \\                         7 9
           ...                 1     3
         _'
         1
        <r>

" ]

Now we have a general technique that can be applied to a vine of any
size.


File: libavl.info,  Node: Balancing Implementation,  Next: Implementing Compression,  Prev: Balancing General Trees,  Up: Transforming a Vine into a Balanced BST

4.12.2.2 Implementation
.......................

Implementing this algorithm is more or less straightforward.  Let's
start from an outline:

91. <Vine to balanced BST function 91> =
<*Note BST compression function: 96.>

/* Converts tree, which must be in the shape of a vine, into a balanced
   tree. */
static void
vine_to_tree (struct bst_table *tree)
{
  unsigned long vine;   /* Number of nodes in main vine. */
  unsigned long leaves; /* Nodes in incomplete bottom level, if any. */
  int height;           /* Height of produced balanced tree. */

  <*Note Calculate leaves: 92.>
  <*Note Reduce vine general case to special case: 93.>
  <*Note Make special case vine into balanced tree and count height: 94.>
  <*Note Check for tree height in range: 95.>
}
   This code is included in *Note 88::.

   The first step is to calculate the number of compression
transformations necessary to reduce the general case of a tree with m
nodes to the special case of exactly 2**n - 1 nodes, i.e., calculate m
- (2**n - 1), and store it in variable leaves.  We are given only the
value of m, as tree->bst_count.  Rewriting the calculation as the
equivalent m + 1 - 2**n, one way to calculate it is evident from
looking at the pattern in binary:

               m    n      m + 1        2**n       m + 1 - 2**n
               1    1     2 = 00010   2 = 00010    0 = 00000
               2    1     3 = 00011   2 = 00010    1 = 00001
               3    2     4 = 00100   4 = 00100    0 = 00000
               4    2     5 = 00101   4 = 00100    1 = 00001
               5    2     6 = 00110   4 = 00100    2 = 00010
               6    2     7 = 00111   4 = 00100    3 = 00011
               7    3     8 = 01000   8 = 01000    0 = 00000
               8    3     9 = 01001   8 = 01000    1 = 00000
               9    3    10 = 01001   8 = 01000    2 = 00000

   See the pattern?  It's simply that m + 1 - 2**n is m with the
leftmost 1-bit turned off.  So, if we can find the leftmost 1-bit in ,
we can figure out the number of leaves.

   In turn, there are numerous ways to find the leftmost 1-bit in a
number.  The one used here is based on the principle that, if x is a
positive integer, then x & (x - 1) is x with its rightmost 1-bit turned
off.

   Here's the code that calculates the number of leaves and stores it in
leaves:

92. <Calculate leaves 92> =
leaves = tree->bst_count + 1;
for (;;)
  {
    unsigned long next = leaves & (leaves - 1);
    if (next == 0)
      break;
    leaves = next;
  }
leaves = tree->bst_count + 1 - leaves;
   This code is included in *Note 91::, *Note 287::, *Note 514::, and
*Note 682::.

   Once we have the number of leaves, we perform a compression composed
of leaves compression transformations.  That's all it takes to reduce
the general case to the 2**n - 1 special case.  We'll write the
compress() function itself later:

93. <Reduce vine general case to special case 93> =
compress ((struct bst_node *) &tree->bst_root, leaves);
   This code is included in *Note 91::, *Note 514::, and *Note 682::.

   The heart of the function is the compression of the vine into the
tree.  Before each compression, vine contains the number of nodes in
the main vine of the tree.  The number of compression transformations
necessary for the compression is vine / 2; e.g., when the main vine
contains 7 nodes, 7 / 2 = 3 transformations are necessary.  The number
of nodes in the vine afterward is the same number (*note Transforming a
Vine into a Balanced BST::).

   At the same time, we keep track of the height of the balanced tree.
The final tree always has height at least 1.  Each compression step
means that it is one level taller than that.  If the tree needed
general-to-special-case transformations, that is, leaves > 0, then it's
one more than that.

94. <Make special case vine into balanced tree and count height 94> =
vine = tree->bst_count - leaves;
height = 1 + (leaves > 0);
while (vine > 1)
  {
    compress ((struct bst_node *) &tree->bst_root, vine / 2);
    vine /= 2;
    height++;
  }
   This code is included in *Note 91::, *Note 514::, and *Note 682::.

   Finally, we make sure that the height of the tree is within range for
what the functions that use stacks can handle.  Otherwise, we could end
up with an infinite loop, with bst_t_next() (for example) calling
bst_balance() repeatedly to balance the tree in order to reduce its
height to the acceptable range.

95. <Check for tree height in range 95> =
if (height > BST_MAX_HEIGHT)
  {
    fprintf (stderr, "libavl: Tree too big (%lu nodes) to handle.",
             (unsigned long) tree->bst_count);
    exit (EXIT_FAILURE);
  }
   This code is included in *Note 91::.


File: libavl.info,  Node: Implementing Compression,  Prev: Balancing Implementation,  Up: Transforming a Vine into a Balanced BST

4.12.2.3 Implementing Compression
.................................

The final bit of code we need is that for performing a compression.  The
following code performs a compression consisting of count applications
of the compression transformation starting at root:

96. <BST compression function 96> =
/* Performs a compression transformation count times,
   starting at root. */
static void
compress (struct bst_node *root, unsigned long count)
{
  assert (root != NULL);

  while (count--)
    {
      struct bst_node *red = root->bst_link[0];
      struct bst_node *black = red->bst_link[0];

      root->bst_link[0] = black;
      red->bst_link[0] = black->bst_link[1];
      black->bst_link[1] = red;
      root = black;
    }
}
   This code is included in *Note 91:: and *Note 514::.

   The operation of compress() should be obvious, given the discussion
earlier.  *Note Balancing General Trees::, above, for a review.

See also:  *Note Stout 1986::, vine_to_tree procedure.


File: libavl.info,  Node: Joining BSTs,  Next: Testing BST Functions,  Prev: Balancing a BST,  Up: Binary Search Trees

4.13 Aside: Joining BSTs
========================

Occasionally we may want to take a pair of BSTs and merge or "join"
their contents, forming a single BST that contains all the items in the
two original BSTs.  It's easy to do this with a series of calls to
bst_insert(), but we can optimize the process if we write a function
exclusively for the purpose.  We'll write such a function in this
section.

   There are two restrictions on the trees to be joined.  First, the
BSTs' contents must be disjoint.  That is, no item in one may match any
item in the other.  Second, the BSTs must have compatible comparison
functions.  Typically, they are the same.  Speaking more precisely, if
f() and g() are the comparison functions, p and q are nodes in either
BST, and r and s are the BSTs' user-provided extra comparison
parameters, then the expressions f(p, q, r), f(p, q, s), g(p, q, r),
and g(p, q, s) must all have the same value for all possible choices of
p and q.

   Suppose we're trying to join the trees shown below:

 [image src="bstjoin.png" text="                            4,a           7,b
                          _'   \\         /   \\
                         1      6       3     8
                          \\      \\      ^
                           2      9    0 5

" ]

Our first inclination is to try a "divide and conquer" approach by
reducing the problem of joining a and b to the subproblems of joining
a's left subtree with b's left subtree and joining a's right subtree
with b's right subtree.  Let us postulate for the moment that we are
able to solve these subproblems and that the solutions that we come up
with are the following:

 [image src="bstjoin2.png" text="                                  3       8
                              _.-' \\      ^
                             0      5    6 9
                              \\
                               1
                                \\
                                 2

" ]

To convert this partial solution into a full solution we must combine
these two subtrees into a single tree and at the same time reintroduce
the nodes a and b into the combined tree.  It is easy enough to do this
by making a (or b) the root of the combined tree with these two
subtrees as its children, then inserting b (or a) into the combined
tree.  Unfortunately, in neither case will this actually work out
properly for our example.  The diagram below illustrates one
possibility, the result of combining the two subtrees as the child of
node 4, then inserting node 7 into the final tree.  As you can see,
nodes 4 and 5 are out of order:(1)

 [image src="bstjoin3.png" text="                                  4
                                _' `._
                               3      8
                           _.-' \\   _' \\
                          0      5 6    9    **
                           \\        \\
                            1        7
                             \\
                              2

" ]

Now let's step back and analyze why this attempt failed.  It was
essentially because, when we recombined the subtrees, a node in the
combined tree's left subtree had a value larger than the root.  If we
trace it back to the original trees to be joined, we see that this was
because node 5 in the left subtree of b is greater than a.  (If we had
chosen 7 as the root of the combined tree we would have found instead
node 6 in the right subtree of b to be the culprit.)

   On the other hand, if every node in the left subtree of a had a
value less than b's value, and every node in the right subtree of a had
a value greater than b's value, there would be no problem.  Hey, wait a
second... we can force that condition.  If we perform a root insertion
(*note Root Insertion in a BST::) of b into subtree a, then we end up
with one pair of subtrees whose node values are all less than 7 (the
new and former left subtrees of node 7) and one pair of subtrees whose
node values are all greater than 7 (the new and former right subtrees
of node 7).  Conceptually it looks like this, although in reality we
would need to remove node 7 from the tree on the right as we inserted
it into the tree on the left:

 [image src="bstjoin4.png" text="                                 7         7
                               _' \\       / \\
                              4    9     3   8
                            _' \\         ^
                           1    6       0 5
                            \\
                             2

" ]

We can then combine the two subtrees with values less than 7 with each
other, and similarly for the ones with values greater than 7, using the
same algorithm recursively, and safely set the resulting subtrees as
the left and right subtrees of node 7, respectively.  The final product
is a correctly joined binary tree:

 [image src="bstjoin5.png" text="                                      7
                                   _.' \\
                                  3     8
                              _.-' \\     \\
                             0      5     9
                              \\     ^
                               1   4 6
                                \\
                                 2

" ]

Of course, since we've defined a join recursively in terms of itself,
there must be some maximum depth to the recursion, some simple case
that can be defined without further recursion.  This is easy: the join
of an empty tree with another tree is the second tree.

Implementation
..............

It's easy to implement this algorithm recursively.  The only nonobvious
part of the code below is the treatment of node b.  We want to insert
node b, but not b's children, into the subtree rooted at a.  However,
we still need to keep track of b's children.  So we temporarily save
b's children as b0 and b1 and set its child pointers to NULL before the
root insertion.

   This code makes use of root_insert() from <*Note Robust root
insertion of existing node in arbitrary subtree: 627.>.

97. <BST join function, recursive version 97> =
/* Joins a and b, which are subtrees of tree,
   and returns the resulting tree. */
static struct bst_node *
join (struct bst_table *tree, struct bst_node *a, struct bst_node *b)
{
  if (b == NULL)
    return a;
  else if (a == NULL)
    return b;
  else
    {
      struct bst_node *b0 = b->bst_link[0];
      struct bst_node *b1 = b->bst_link[1];
      b->bst_link[0] = b->bst_link[1] = NULL;
      root_insert (tree, &a, b);
      a->bst_link[0] = join (tree, b0, a->bst_link[0]);
      a->bst_link[1] = join (tree, b1, a->bst_link[1]);
      return a;
    }
}

/* Joins a and b, which must be disjoint and have compatible
   comparison functions.
   b is destroyed in the process. */
void
bst_join (struct bst_table *a, struct bst_table *b)
{
  a->bst_root = join (a, a->bst_root, b->bst_root);
  a->bst_count += b->bst_count;
  free (b);
}

See also:  *Note Sedgewick 1998::, program 12.16.

Exercises:

1. Rewrite bst_join() to avoid use of recursion.  [*Note answer:
4-13#1..]

   ---------- Footnotes ----------

   (1) The ** notation in the diagram emphasizes that this is a
counterexample.


File: libavl.info,  Node: Testing BST Functions,  Next: Additional Exercises for BSTs,  Prev: Joining BSTs,  Up: Binary Search Trees

4.14 Testing
============

Whew!  We're finally done with building functions for performing BST
operations.  But we haven't tested any of our code.  Testing is an
essential step in writing programs, because untested software cannot be
assumed to work.

   Let's build a test program that exercises all of the functions we
wrote.  We'll also do our best to make parts of it generic, so that we
can reuse test code in later chapters when we want to test other
BST-based structures.

   The first step is to figure out how to test the code.  One goal in
testing is to exercise as much of the code as possible.  Ideally, every
line of code would be executed sometime during testing.  Often, this is
difficult or impossible, but the principle remains valid, with the goal
modified to testing as much of the code as possible.

   In applying this principle to the BST code, we have to consider why
each line of code is executed.  If we look at the code for most
functions in <*Note bst.c: 26.>, we can see that, if we execute them
for any BST of reasonable size, most or all of their code will be
tested.

   This is encouraging.  It means that we can just construct some trees
and try out the BST functions on them, check that the results make
sense, and have a pretty good idea that they work.  Moreover, if we
build trees in a random fashion, and delete their nodes in a random
order, and do it several times, we'll even have a good idea that the
bst_probe() and bst_delete() cases have all come up and worked
properly.  (If you want to be sure, then you can insert printf() calls
for each case to record when they trip.)  This is not the same as a
proof of correctness, but proofs of correctness can only be constructed
by computer scientists with fancy degrees, not by mere clever
programmers.

   There are three notably missing pieces of code coverage if we just do
the above.  These are stack overflow handling, memory allocation failure
handling, and traverser code to deal with modified trees.  But we can
mop up these extra problems with a little extra effort:(1)

   * Stack overflow handling can be tested by forcing the stack to
     overflow.  Stack overflow can occur in many places, so for best
     effect we must test each possible spot.  We will write special
     tests for these problems.

   * Memory allocation failure handling can be tested by simulating
     memory allocation failures.  We will write a replacement memory
     allocator that "fails" after a specified number of calls.  This
     allocator will also allow for memory leak detection.

   * Traverser code to deal with modified trees.  This can be tested by
     modifying trees during traversal and making sure that the traversal
     functions still work as expected.

   The testing code can be broken into the following groups of
functions:

Testing and verification
     These functions actually try out the BST routines and do their
     best to make sure that their results are correct.

Test set generation
     Generates the order of node insertion and deletion, for use during
     testing.

Memory manager
     Handles memory issues, including memory leak detection and failure
     simulation.

User interaction
     Figures out what the user wants to test in this run.

Main program
     Glues everything else together by calling functions in the proper
     order.

Utilities
     Miscellaneous routines that don't fit comfortably into another
     category.

   Most of the test code will also work nicely for testing other binary
tree-based structures.  This code is grouped into a single file, <*Note
test.c: 98.>, which has the following structure:

98. <test.c 98> =
<*Note Program License: 2.>
#include <assert.h>
#include <limits.h>
#include <stdarg.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>
#include "test.h"

<*Note Test declarations: 122.>
<*Note Test utility functions: 135.>
<*Note Memory tracker: 127.>
<*Note Option parser: 588.>
<*Note Command line parser: 591.>
<*Note Insertion and deletion order generation: 644.>
<*Note Random number seeding: 645.>
<*Note Test main program: 141.>

   The code specifically for testing BSTs goes into <*Note bst-test.c:
99.>, outlined like this:

99. <bst-test.c 99> =
<*Note Program License: 2.>
#include <assert.h>
#include <limits.h>
#include <stdio.h>
#include "bst.h"
#include "test.h"

<*Note BST print function: 120.>
<*Note BST traverser check function: 105.>
<*Note Compare two BSTs for structure and content: 107.>
<*Note Recursively verify BST structure: 114.>
<*Note BST verify function: 110.>
<*Note BST test function: 101.>
<*Note BST overflow test function: 123.>

   The interface between <*Note test.c: 98.> and <*Note bst-test.c:
99.> is contained in <*Note test.h: 100.>:

100. <test.h 100> =
<*Note Program License: 2.>
#ifndef TEST_H
#define TEST_H 1

<*Note Memory allocator: 6.>
<*Note Test prototypes: 102.>

#endif /* test.h */

   Although much of the test program code is nontrivial, only some of
the interesting parts fall within the scope of this book.  The remainder
will be listed without comment or relegated to the exercises.  The most
tedious code is listed in an appendix (*note Supplementary Code::).

* Menu:

* Testing BSTs::
* Test Set Generation::
* Testing Overflow::
* Memory Manager::
* User Interaction::
* Utility Functions::
* Main Program::

   ---------- Footnotes ----------

   (1) Some might scoff at this amount of detail, calling it wasted
effort, but this thorough testing in fact revealed a number of subtle
bugs during development of libavl that had otherwise gone unnoticed.


File: libavl.info,  Node: Testing BSTs,  Next: Test Set Generation,  Prev: Testing BST Functions,  Up: Testing BST Functions

4.14.1 Testing BSTs
-------------------

As suggested above, the main way we will test the BST routines is by
using them and checking the results, with checks performed by slow but
simple routines.  The idea is that bugs in the BST routines are unlikely
to be mirrored in the check routines, and vice versa.  This way,
identical results from the BST and checks tend to indicate that both
implementations are correct.

   The main test routine is designed to exercise as many of the BST
functions as possible.  It starts by creating a BST and inserting nodes
into it, then deleting the nodes.  Midway, various traversals are
tested, including the ability to traverse a tree while its content is
changing.  After each operation that modifies the tree, its structure
and content are verified for correspondence with expectations.  The
function for copying a BST is also tested.  This function, test(), has
the following outline:

101. <BST test function 101> =
/* Tests tree functions.
   insert[] and delete[] must contain some permutation of values
   0...n - 1.
   Uses allocator as the allocator for tree and node data.
   Higher values of verbosity produce more debug output. */
int
test_correctness (struct libavl_allocator *allocator,
                  int insert[], int delete[], int n, int verbosity)
{
  struct bst_table *tree;
  int okay = 1;
  int i;

  <*Note Test creating a BST and inserting into it: 103.>
  <*Note Test BST traversal during modifications: 104.>
  <*Note Test deleting nodes from the BST and making copies of it: 106.>
  <*Note Test deleting from an empty tree: 108.>
  <*Note Test destroying the tree: 109.>

  return okay;
}
   This code is included in *Note 99::, *Note 188::, *Note 240::, *Note
332::, *Note 370::, *Note 451::, *Note 484::, *Note 550::, and *Note
585::.

102. <Test prototypes 102> =
int test_correctness (struct libavl_allocator *allocator,
                      int insert[], int delete[], int n, int verbosity);
   See also *Note 124:: and *Note 136::.
This code is included in *Note 100::.

   The first step is to create a BST and insert items into it in the
order specified by the caller.  We use the comparison function
compare_ints() from <*Note Comparison function for ints: 4.> to put the
tree's items into ordinary numerical order.  After each insertion we
call verify_tree(), which we'll write later and which checks that the
tree actually contains the items that it should:

103. <Test creating a BST and inserting into it 103> =
tree = bst_create (compare_ints, NULL, allocator);
if (tree == NULL)
  {
    if (verbosity >= 0)
      printf ("  Out of memory creating tree.\n");
    return 1;
  }

for (i = 0; i < n; i++)
  {
    if (verbosity >= 2)
      printf ("  Inserting %d...\n", insert[i]);

    /* Add the ith element to the tree. */
    {
      void **p = bst_probe (tree, &insert[i]);
      if (p == NULL)
        {
          if (verbosity >= 0)
            printf ("    Out of memory in insertion.\n");
          bst_destroy (tree, NULL);
          return 1;
        }
      if (*p != &insert[i])
        printf ("    Duplicate item in tree!\n");
    }

    if (verbosity >= 3)
      print_whole_tree (tree, "    Afterward");

    if (!verify_tree (tree, insert, i + 1))
      return 0;
  }
   This code is included in *Note 101:: and *Note 297::.

   If the tree is being modified during traversal, that causes a little
more stress on the tree routines, so we should test this specially.  We
initialize one traverser, x, at a selected item, then delete and
reinsert a different item in order to invalidate that traverser.  We
make a copy, y, of the traverser in order to check that bst_t_copy()
works properly and initialize a third traverser, z, with the inserted
item.  After the deletion and reinsertion we check that all three of the
traversers behave properly.

104. <Test BST traversal during modifications 104> =
for (i = 0; i < n; i++)
  {
    struct bst_traverser x, y, z;
    int *deleted;

    if (insert[i] == delete[i])
      continue;

    if (verbosity >= 2)
      printf ("   Checking traversal from item %d...\n", insert[i]);

    if (bst_t_find (&x, tree, &insert[i]) == NULL)
      {
        printf ("    Can't find item %d in tree!\n", insert[i]);
        continue;
      }

    okay &= check_traverser (&x, insert[i], n, "Predeletion");

    if (verbosity >= 3)
      printf ("    Deleting item %d.\n", delete[i]);

    deleted = bst_delete (tree, &delete[i]);
    if (deleted == NULL || *deleted != delete[i])
      {
        okay = 0;
        if (deleted == NULL)
          printf ("    Deletion failed.\n");
        else
          printf ("    Wrong node %d returned.\n", *deleted);
      }

    bst_t_copy (&y, &x);

    if (verbosity >= 3)
      printf ("    Reinserting item %d.\n", delete[i]);
    if (bst_t_insert (&z, tree, &delete[i]) == NULL)
      {
        if (verbosity >= 0)
          printf ("    Out of memory reinserting item.\n");
        bst_destroy (tree, NULL);
        return 1;
      }

    okay &= check_traverser (&x, insert[i], n, "Postdeletion");
    okay &= check_traverser (&y, insert[i], n, "Copied");
    okay &= check_traverser (&z, delete[i], n, "Insertion");

    if (!verify_tree (tree, insert, n))
      return 0;
  }
   This code is included in *Note 101:: and *Note 297::.

   The check_traverser() function used above checks that a traverser
behaves properly, by checking that the traverser is at the correct item
and that the previous and next items are correct as well.

105. <BST traverser check function 105> =
/* Checks that the current item at trav is i
   and that its previous and next items are as they should be.
   label is a name for the traverser used in reporting messages.
   There should be n items in the tree numbered 0...n - 1.
   Returns nonzero only if there is an error. */
static int
check_traverser (struct bst_traverser *trav, int i, int n, const char *label)
{
  int okay = 1;
  int *cur, *prev, *next;

  prev = bst_t_prev (trav);
  if ((i == 0 && prev != NULL)
      || (i > 0 && (prev == NULL || *prev != i - 1)))
    {
      printf ("   %s traverser ahead of %d, but should be ahead of %d.\n",
              label, prev != NULL ? *prev : -1, i == 0 ? -1 : i - 1);
      okay = 0;
    }
  bst_t_next (trav);

  cur = bst_t_cur (trav);
  if (cur == NULL || *cur != i)
    {
      printf ("   %s traverser at %d, but should be at %d.\n",
              label, cur != NULL ? *cur : -1, i);
      okay = 0;
    }

  next = bst_t_next (trav);
  if ((i == n - 1 && next != NULL)
      || (i != n - 1 && (next == NULL || *next != i + 1)))
    {
      printf ("   %s traverser behind %d, but should be behind %d.\n",
              label, next != NULL ? *next : -1, i == n - 1 ? -1 : i + 1);
      okay = 0;
    }
  bst_t_prev (trav);

  return okay;
}
   This code is included in *Note 99::, *Note 188::, *Note 240::, *Note
292::, *Note 332::, *Note 370::, *Note 413::, *Note 451::, *Note 484::,
*Note 517::, *Note 550::, and *Note 585::.

   We also need to test deleting nodes from the tree and making copies
of a tree.  Here's the code to do that:

106. <Test deleting nodes from the BST and making copies of it 106> =
for (i = 0; i < n; i++)
  {
    int *deleted;

    if (verbosity >= 2)
      printf ("  Deleting %d...\n", delete[i]);

    deleted = bst_delete (tree, &delete[i]);
    if (deleted == NULL || *deleted != delete[i])
      {
        okay = 0;
        if (deleted == NULL)
          printf ("    Deletion failed.\n");
        else
          printf ("    Wrong node %d returned.\n", *deleted);
      }

    if (verbosity >= 3)
      print_whole_tree (tree, "    Afterward");

    if (!verify_tree (tree, delete + i + 1, n - i - 1))
      return 0;

    if (verbosity >= 2)
      printf ("  Copying tree and comparing...\n");

    /* Copy the tree and make sure it's identical. */
    {
      struct bst_table *copy = bst_copy (tree, NULL, NULL, NULL);
      if (copy == NULL)
        {
          if (verbosity >= 0)
            printf ("  Out of memory in copy\n");
          bst_destroy (tree, NULL);
          return 1;
        }

      okay &= compare_trees (tree->bst_root, copy->bst_root);
      bst_destroy (copy, NULL);
    }
  }
   This code is included in *Note 101:: and *Note 297::.

   The actual comparison of trees is done recursively for simplicity:

107. <Compare two BSTs for structure and content 107> =
/* Compares binary trees rooted at a and b,
   making sure that they are identical. */
static int
compare_trees (struct bst_node *a, struct bst_node *b)
{
  int okay;

  if (a == NULL || b == NULL)
    {
      assert (a == NULL && b == NULL);
      return 1;
    }

  if (*(int *) a->bst_data != *(int *) b->bst_data
      || ((a->bst_link[0] != NULL) != (b->bst_link[0] != NULL))
      || ((a->bst_link[1] != NULL) != (b->bst_link[1] != NULL)))
    {
      printf (" Copied nodes differ: a=%d b=%d a:",
              *(int *) a->bst_data, *(int *) b->bst_data);

      if (a->bst_link[0] != NULL)
        printf ("l");
      if (a->bst_link[1] != NULL)
        printf ("r");

      printf (" b:");
      if (b->bst_link[0] != NULL)
        printf ("l");
      if (b->bst_link[1] != NULL)
        printf ("r");

      printf ("\n");
      return 0;
    }

  okay = 1;
  if (a->bst_link[0] != NULL)
    okay &= compare_trees (a->bst_link[0], b->bst_link[0]);
  if (a->bst_link[1] != NULL)
    okay &= compare_trees (a->bst_link[1], b->bst_link[1]);
  return okay;
}
   This code is included in *Note 99::.

   As a simple extra check, we make sure that attempting to delete from
an empty tree fails in the expected way:

108. <Test deleting from an empty tree 108> =
if (bst_delete (tree, &insert[0]) != NULL)
  {
    printf (" Deletion from empty tree succeeded.\n");
    okay = 0;
  }
   This code is included in *Note 101::.

   Finally, we're done with the tree and can get rid of it.

109. <Test destroying the tree 109> =
/* Test destroying the tree. */
bst_destroy (tree, NULL);
   This code is included in *Note 101:: and *Note 297::.

Exercises:

1. Which functions in <*Note bst.c: 26.> are not exercised by test()?
[*Note answer: 4-14-1#1..]

2. Some errors within test() just set the okay flag to zero, whereas
others cause an immediate unsuccessful return to the caller without
performing any cleanup.  A third class of errors causes cleanup followed
by a successful return.  Why and how are these distinguished?  [*Note
answer: 4-14-1#2..]

* Menu:

* BST Verification::
* Displaying BST Structures::


File: libavl.info,  Node: BST Verification,  Next: Displaying BST Structures,  Prev: Testing BSTs,  Up: Testing BSTs

4.14.1.1 BST Verification
.........................

After each change to the tree in the testing program, we call
verify_tree() to check that the tree's structure and content are what
we think they should be.  This function runs through a full gamut of
checks, with the following outline:

110. <BST verify function 110> =
/* Checks that tree is well-formed
   and verifies that the values in array[] are actually in tree.
   There must be n elements in array[] and tree.
   Returns nonzero only if no errors detected. */
static int
verify_tree (struct bst_table *tree, int array[], size_t n)
{
  int okay = 1;

  <*Note Check tree->bst_count is correct: 111.>

  if (okay)
    {
      <*Note Check BST structure: 112.>
    }

  if (okay)
    {
      <*Note Check that the tree contains all the elements it should: 116.>
    }

  if (okay)
    {
      <*Note Check that forward traversal works: 117.>
    }

  if (okay)
    {
      <*Note Check that backward traversal works: 118.>
    }

  if (okay)
    {
      <*Note Check that traversal from the null element works: 119.>
    }

  return okay;
}
   This code is included in *Note 99::, *Note 413::, and *Note 517::.

   The first step just checks that the number of items passed in as n is
the same as tree->bst_count.

111. <Check tree->bst_count is correct 111> =
/* Check tree's bst_count against that supplied. */
if (bst_count (tree) != n)
  {
    printf (" Tree count is %lu, but should be %lu.\n",
            (unsigned long) bst_count (tree), (unsigned long) n);
    okay = 0;
  }
   This code is included in *Note 110::, *Note 192::, *Note 246::, and
*Note 296::.

   Next, we verify that the BST has proper structure and that it has the
proper number of items.  We'll do this recursively because that's
easiest and most obviously correct way.  Function recurse_verify_tree()
for this returns the number of nodes in the BST.  After it returns, we
verify that this is the expected number.

112. <Check BST structure 112> =
/* Recursively verify tree structure. */
size_t count;

recurse_verify_tree (tree->bst_root, &okay, &count, 0, INT_MAX);
<*Note Check counted nodes: 113.>
   This code is included in *Note 110:: and *Note 296::.

113. <Check counted nodes 113> =
if (count != n)
  {
    printf (" Tree has %lu nodes, but should have %lu.\n",
            (unsigned long) count, (unsigned long) n);
    okay = 0;
  }
   This code is included in *Note 112::, *Note 193::, and *Note 248::.

   The function recurse_verify_tree() does the recursive verification.
It checks that nodes' values increase down to the right and decrease
down to the left.  We also use it to count the number of nodes actually
in the tree:

114. <Recursively verify BST structure 114> =
/* Examines the binary tree rooted at node.
   Zeroes *okay if an error occurs.
   Otherwise, does not modify *okay.
   Sets *count to the number of nodes in that tree,
   including node itself if node != NULL.
   All the nodes in the tree are verified to be at least min
   but no greater than max. */
static void
recurse_verify_tree (struct bst_node *node, int *okay, size_t *count,
                     int min, int max)
{
  int d;                /* Value of this node's data. */
  size_t subcount[2];   /* Number of nodes in subtrees. */

  if (node == NULL)
    {
      *count = 0;
      return;
    }
  d = *(int *) node->bst_data;

  <*Note Verify binary search tree ordering: 115.>

  recurse_verify_tree (node->bst_link[0], okay, &subcount[0], min, d - 1);
  recurse_verify_tree (node->bst_link[1], okay, &subcount[1], d + 1, max);
  *count = 1 + subcount[0] + subcount[1];
}
   This code is included in *Note 99::.

115. <Verify binary search tree ordering 115> =
if (min > max)
  {
    printf (" Parents of node %d constrain it to empty range %d...%d.\n",
            d, min, max);
    *okay = 0;
  }
else if (d < min || d > max)
  {
    printf (" Node %d is not in range %d...%d implied by its parents.\n",
            d, min, max);
    *okay = 0;
  }
   This code is included in *Note 114::, *Note 190::, *Note 242::,
*Note 295::, *Note 334::, *Note 372::, *Note 416::, *Note 453::, *Note
486::, *Note 519::, *Note 552::, and *Note 587::.

   The third step is to check that the BST indeed contains all of the
items that it should:

116. <Check that the tree contains all the elements it should 116> =
/* Check that all the values in array[] are in tree. */
size_t i;

for (i = 0; i < n; i++)
  if (bst_find (tree, &array[i]) == NULL)
    {
      printf (" Tree does not contain expected value %d.\n", array[i]);
      okay = 0;
    }
   This code is included in *Note 110::, *Note 192::, *Note 246::, and
*Note 296::.

   The final steps all check traversal of the BST, first by traversing
in forward order from the beginning to the end, then in reverse order,
then by checking that the null item behaves correctly.  The forward
traversal checks that the proper number of items are in the BST.  It
could appear to have too few items if the tree's pointers are screwed
up in one way, or it could appear to have too many items if they are
screwed up in another way.  We try to figure out how many items
actually appear in the tree during traversal, but give up if the count
gets to be more than twice that expected, assuming that this indicates
a "loop" that will cause traversal to never terminate.

117. <Check that forward traversal works 117> =
/* Check that bst_t_first() and bst_t_next() work properly. */
struct bst_traverser trav;
size_t i;
int prev = -1;
int *item;

for (i = 0, item = bst_t_first (&trav, tree); i < 2 * n && item != NULL;
     i++, item = bst_t_next (&trav))
  {
    if (*item <= prev)
      {
        printf (" Tree out of order: %d follows %d in traversal\n",
                *item, prev);
        okay = 0;
      }

    prev = *item;
  }

if (i != n)
  {
    printf (" Tree should have %lu items, but has %lu in traversal\n",
            (unsigned long) n, (unsigned long) i);
    okay = 0;
  }
   This code is included in *Note 110::, *Note 192::, *Note 246::, and
*Note 296::.

   We do a similar traversal in the reverse order:

118. <Check that backward traversal works 118> =
/* Check that bst_t_last() and bst_t_prev() work properly. */
struct bst_traverser trav;
size_t i;
int next = INT_MAX;
int *item;

for (i = 0, item = bst_t_last (&trav, tree); i < 2 * n && item != NULL;
     i++, item = bst_t_prev (&trav))
  {
    if (*item >= next)
      {
        printf (" Tree out of order: %d precedes %d in traversal\n",
                *item, next);
        okay = 0;
      }

    next = *item;
  }

if (i != n)
  {
    printf (" Tree should have %lu items, but has %lu in reverse\n",
            (unsigned long) n, (unsigned long) i);
    okay = 0;
  }
   This code is included in *Note 110::, *Note 192::, *Note 246::, and
*Note 296::.

   The final check to perform on the traverser is to make sure that the
traverser null item works properly.  We start out a traverser at the
null item with bst_t_init(), then make sure that the next item after
that, as reported by bst_t_next(), is the same as the item returned by
bst_t_init(), and similarly for the previous item:

119. <Check that traversal from the null element works 119> =
/* Check that bst_t_init() works properly. */
struct bst_traverser init, first, last;
int *cur, *prev, *next;

bst_t_init (&init, tree);
bst_t_first (&first, tree);
bst_t_last (&last, tree);

cur = bst_t_cur (&init);
if (cur != NULL)
  {
    printf (" Inited traverser should be null, but is actually %d.\n",
            *cur);
    okay = 0;
  }

next = bst_t_next (&init);
if (next != bst_t_cur (&first))
  {
    printf (" Next after null should be %d, but is actually %d.\n",
            *(int *) bst_t_cur (&first), *next);
    okay = 0;
  }
bst_t_prev (&init);

prev = bst_t_prev (&init);
if (prev != bst_t_cur (&last))
  {
    printf (" Previous before null should be %d, but is actually %d.\n",
            *(int *) bst_t_cur (&last), *prev);
    okay = 0;
  }
bst_t_next (&init);
   This code is included in *Note 110::, *Note 192::, *Note 246::, and
*Note 296::.

Exercises:

1. Many of the segments of code in this section cast size_t arguments to
printf() to unsigned long.  Why?  [*Note answer: 4-14-1-1#1..]

2. Does test() work properly for testing trees with only one item in
them?  Zero items?  [*Note answer: 4-14-1-1#2..]


File: libavl.info,  Node: Displaying BST Structures,  Prev: BST Verification,  Up: Testing BSTs

4.14.1.2 Displaying BST Structures
..................................

The print_tree_structure() function below can be useful for debugging,
but it is not used very much by the testing code.  It prints out the
structure of a tree, with the root first, then its children in
parentheses separated by a comma, and their children in inner
parentheses, and so on.  This format is easy to print but difficult to
visualize, so it's a good idea to have a notebook on hand to sketch out
the shape of the tree.  Alternatively, this output is in the right
format to feed directly into the `texitree' program used to draw the
tree diagrams in this book, which can produce output in plain text or
PostScript form.

120. <BST print function 120> =
/* Prints the structure of node,
   which is level levels from the top of the tree. */
static void
print_tree_structure (const struct bst_node *node, int level)
{
  /* You can set the maximum level as high as you like.
     Most of the time, you'll want to debug code using small trees,
     so that a large level indicates a "loop", which is a bug. */
  if (level > 16)
    {
      printf ("[...]");
      return;
    }

  if (node == NULL)
    return;

  printf ("%d", *(int *) node->bst_data);
  if (node->bst_link[0] != NULL || node->bst_link[1] != NULL)
    {
      putchar ('(');

      print_tree_structure (node->bst_link[0], level + 1);
      if (node->bst_link[1] != NULL)
        {
          putchar (',');
          print_tree_structure (node->bst_link[1], level + 1);
        }

      putchar (')');
    }
}
   See also *Note 121::.
This code is included in *Note 99::, *Note 188::, *Note 240::, *Note
517::, *Note 550::, and *Note 585::.

   A function print_whole_tree() is also provided as a convenient
wrapper for printing an entire BST's structure.

121. <BST print function 120> +=
/* Prints the entire structure of tree with the given title. */
void
print_whole_tree (const struct bst_table *tree, const char *title)
{
  printf ("%s: ", title);
  print_tree_structure (tree->bst_root, 0);
  putchar ('\n');
}


File: libavl.info,  Node: Test Set Generation,  Next: Testing Overflow,  Prev: Testing BSTs,  Up: Testing BST Functions

4.14.2 Test Set Generation
--------------------------

We need code to generate a random permutation of numbers to order
insertion and deletion of items.  We will support some other orders
besides random permutation as well for completeness and to allow for
overflow testing.  Here is the complete list:

122. <Test declarations 122> =
/* Insertion order. */
enum insert_order
  {
    INS_RANDOM,			/* Random order. */
    INS_ASCENDING,		/* Ascending order. */
    INS_DESCENDING,		/* Descending order. */
    INS_BALANCED,		/* Balanced tree order. */
    INS_ZIGZAG,			/* Zig-zag order. */
    INS_ASCENDING_SHIFTED,      /* Ascending from middle, then beginning. */
    INS_CUSTOM,			/* Custom order. */

    INS_CNT                     /* Number of insertion orders. */
  };

/* Deletion order. */
enum delete_order
  {
    DEL_RANDOM,			/* Random order. */
    DEL_REVERSE,		/* Reverse of insertion order. */
    DEL_SAME,			/* Same as insertion order. */
    DEL_CUSTOM,			/* Custom order. */

    DEL_CNT                     /* Number of deletion orders. */
  };
   See also *Note 126::, *Note 134::, *Note 139::, *Note 140::,
and*Note 142::.
This code is included in *Note 98::.

The code to actually generate these orderings is left to the exercises.

Exercises:

1. Write a function to generate a random permutation of the n ints
between 0 and n - 1 into a provided array.  [*Note answer:
4-14-2#1..]

*2. Write a function to generate an ordering of ints that, when inserted
into a binary tree, produces a balanced tree of the integers from min to
max inclusive.  (Hint: what kind of recursive traversal makes this
easy?)  [*Note answer: 4-14-2#2..]

3. Write one function to generate an insertion order of n integers into
a provided array based on an enum insert_order and the functions written
in the previous two exercises.  Write a second function to generate a
deletion order using similar parameters plus the order of insertion.
[*Note answer: 4-14-2#3..]

*4. By default, the C random number generator produces the same sequence
every time the program is run.  In order to generate different
sequences, it has to be "seeded" using srand() with a unique value.
Write a function to select a random number seed based on the current
time.  [*Note answer: 4-14-2#4..]


File: libavl.info,  Node: Testing Overflow,  Next: Memory Manager,  Prev: Test Set Generation,  Up: Testing BST Functions

4.14.3 Testing Overflow
-----------------------

Testing for overflow requires an entirely different set of test
functions.  The idea is to create a too-tall tree using one of the
pathological insertion orders (ascending, descending, zig-zag, shifted
ascending), then try out each of the functions that can overflow on it
and make sure that they behave as they should.

   There is a separate test function for each function that can
overflow a stack but which is not tested by test().  These functions
are called by driver function test_overflow(), which also takes care of
creating, populating, and destroying the tree.

123. <BST overflow test function 123> =
<*Note Overflow testers: 125.>

/* Tests the tree routines for proper handling of overflows.
   Inserting the n elements of order[] should produce a tree
   with height greater than BST_MAX_HEIGHT.
   Uses allocator as the allocator for tree and node data.
   Use verbosity to set the level of chatter on stdout. */
int
test_overflow (struct libavl_allocator *allocator,
               int order[], int n, int verbosity)
{
  /* An overflow tester function. */
  typedef int test_func (struct bst_table *, int n);

  /* An overflow tester. */
  struct test
    {
      test_func *func;                  /* Tester function. */
      const char *name;                 /* Test name. */
    };

  /* All the overflow testers. */
  static const struct test test[] =
    {
      {test_bst_t_first, "first item"},
      {test_bst_t_last, "last item"},
      {test_bst_t_find, "find item"},
      {test_bst_t_insert, "insert item"},
      {test_bst_t_next, "next item"},
      {test_bst_t_prev, "previous item"},
      {test_bst_copy, "copy tree"},
    };

  const struct test *i;                 /* Iterator. */

  /* Run all the overflow testers. */
  for (i = test; i < test + sizeof test / sizeof *test; i++)
    {
      struct bst_table *tree;
      int j;

      if (verbosity >= 2)
        printf ("  Running %s test...\n", i->name);

      tree = bst_create (compare_ints, NULL, allocator);
      if (tree == NULL)
        {
          printf ("    Out of memory creating tree.\n");
          return 1;
        }

      for (j = 0; j < n; j++)
        {
          void **p = bst_probe (tree, &order[j]);
          if (p == NULL || *p != &order[j])
            {
              if (p == NULL && verbosity >= 0)
                printf ("    Out of memory in insertion.\n");
              else if (p != NULL)
                printf ("    Duplicate item in tree!\n");
              bst_destroy (tree, NULL);
              return p == NULL;
            }
        }

      if (i->func (tree, n) == 0)
        return 0;

      if (verify_tree (tree, order, n) == 0)
        return 0;
      bst_destroy (tree, NULL);
    }

  return 1;
}
   This code is included in *Note 99::, *Note 188::, *Note 240::, *Note
292::, *Note 332::, *Note 370::, *Note 413::, *Note 451::, *Note 484::,
*Note 517::, *Note 550::, and *Note 585::.

124. <Test prototypes 102> +=
int test_overflow (struct libavl_allocator *, int order[], int n,
                   int verbosity);

   There is an overflow tester for almost every function that can
overflow.  Here is one example:

125. <Overflow testers 125> =
static int
test_bst_t_first (struct bst_table *tree, int n)
{
  struct bst_traverser trav;
  int *first;

  first = bst_t_first (&trav, tree);
  if (first == NULL || *first != 0)
    {
      printf ("    First item test failed: expected 0, got %d\n",
              first != NULL ? *first : -1);
      return 0;
    }

  return 1;
}
   See also *Note 646::.
This code is included in *Note 123::.

Exercises:

1. Write the rest of the overflow tester functions.  (The
test_overflow() function lists all of them.)  [*Note answer:
4-14-3#1..]


File: libavl.info,  Node: Memory Manager,  Next: User Interaction,  Prev: Testing Overflow,  Up: Testing BST Functions

4.14.4 Memory Manager
---------------------

We want to test our code to make sure that it always releases allocated
memory and that it behaves robustly when memory allocations fail.  We
can do the former by building our own memory manager that keeps tracks
of blocks as they are allocated and freed.  The memory manager can also
disallow allocations according to a policy set by the user, taking care
of the latter.

   The available policies are:

126. <Test declarations 122> +=
/* Memory tracking policy. */
enum mt_policy
  {
    MT_TRACK,			/* Track allocation for leak detection. */
    MT_NO_TRACK,		/* No leak detection. */
    MT_FAIL_COUNT,      	/* Fail allocations after a while. */
    MT_FAIL_PERCENT,		/* Fail allocations randomly. */
    MT_SUBALLOC                 /* Suballocate from larger blocks. */
  };

MT_TRACK and MT_NO_TRACK should be self-explanatory.  MT_FAIL_COUNT
takes an argument specifying after how many allocations further
allocations should always fail.  MT_FAIL_PERCENT takes an argument
specifying an integer percentage of allocations to randomly fail.

   MT_SUBALLOC causes small blocks to be carved out of larger ones
allocated with malloc().  This is a good idea for two reasons: malloc()
can be slow and malloc() can waste a lot of space dealing with the
small blocks that libavl uses for its node.  Suballocation cannot be
implemented in an entirely portable way because of alignment issues,
but the test program here requires the user to specify the alignment
needed, and its use is optional anyhow.

   The memory manager keeps track of allocated blocks using struct
block:

127. <Memory tracker 127> =
/* Memory tracking allocator. */

/* A memory block. */
struct block
  {
    struct block *next;                 /* Next in linked list. */

    int idx;                            /* Allocation order index number. */
    size_t size;                        /* Size in bytes. */
    size_t used;                        /* MT_SUBALLOC: amount used so far. */
    void *content;                      /* Allocated region. */
  };
   See also *Note 128::, *Note 129::, *Note 130::, *Note 131::, *Note
132::, and*Note 133::.
This code is included in *Note 98::.

The next member of struct block is used to keep a linked list of all
the currently allocated blocks.  Searching this list is inefficient, but
there are at least two reasons to do it this way, instead of using a
more efficient data structure, such as a binary tree.  First, this code
is for testing binary tree routines--using a binary tree data structure
to do it is a strange idea!  Second, the ISO C standard says that, with
few exceptions, using the relational operators (<, <=, >, >=) to
compare pointers that do not point inside the same array produces
undefined behavior, but allows use of the equality operators (==, !=)
for a larger class of pointers.

   We also need a data structure to keep track of settings and a list of
blocks.  This memory manager uses the technique discussed in Exercise
2.5-3 to provide this structure to the allocator.

128. <Memory tracker 127> +=
/* Indexes into arg[] within struct mt_allocator. */
enum mt_arg_index
  {
    MT_COUNT = 0,      /* MT_FAIL_COUNT: Remaining successful allocations. */
    MT_PERCENT = 0,    /* MT_FAIL_PERCENT: Failure percentage. */
    MT_BLOCK_SIZE = 0, /* MT_SUBALLOC: Size of block to suballocate. */
    MT_ALIGN = 1       /* MT_SUBALLOC: Alignment of suballocated blocks. */
  };

/* Memory tracking allocator. */
struct mt_allocator
  {
    struct libavl_allocator allocator;  /* Allocator.  Must be first member. */

    /* Settings. */
    enum mt_policy policy;              /* Allocation policy. */
    int arg[2];                         /* Policy arguments. */
    int verbosity;                      /* Message verbosity level. */

    /* Current state. */
    struct block *head, *tail;          /* Head and tail of block list. */
    int alloc_idx;                      /* Number of allocations so far. */
    int block_cnt;                      /* Number of still-allocated blocks. */
  };

   Function mt_create() creates a new instance of the memory tracker.
It takes an allocation policy and policy argument, as well as a number
specifying how verbose it should be in reporting information.  It uses
utility function xmalloc(), a simple wrapper for malloc() that aborts
the program on failure.  Here it is:

129. <Memory tracker 127> +=
static void *mt_allocate (struct libavl_allocator *, size_t);
static void mt_free (struct libavl_allocator *, void *);

/* Initializes the memory manager for use
   with allocation policy policy and policy arguments arg[],
   at verbosity level verbosity, where 0 is a "normal" value. */
struct mt_allocator *
mt_create (enum mt_policy policy, int arg[2], int verbosity)
{
  struct mt_allocator *mt = xmalloc (sizeof *mt);

  mt->allocator.libavl_malloc = mt_allocate;
  mt->allocator.libavl_free = mt_free;

  mt->policy = policy;
  mt->arg[0] = arg[0];
  mt->arg[1] = arg[1];
  mt->verbosity = verbosity;

  mt->head = mt->tail = NULL;
  mt->alloc_idx = 0;
  mt->block_cnt = 0;

  return mt;
}

   After allocations and deallocations are done, the memory manager
must be freed with mt_destroy(), which also reports any memory leaks.
Blocks are removed from the block list as they are freed, so any
remaining blocks must be leaked memory:

130. <Memory tracker 127> +=
/* Frees and destroys memory tracker mt,
   reporting any memory leaks. */
void
mt_destroy (struct mt_allocator *mt)
{
  assert (mt != NULL);

  if (mt->block_cnt == 0)
    {
      if (mt->policy != MT_NO_TRACK && mt->verbosity >= 1)
        printf ("  No memory leaks.\n");
    }
  else
    {
      struct block *iter, *next;

      if (mt->policy != MT_SUBALLOC)
        printf ("  Memory leaks detected:\n");
      for (iter = mt->head; iter != NULL; iter = next)
        {
          if (mt->policy != MT_SUBALLOC)
            printf ("    block #%d: %lu bytes\n",
                    iter->idx, (unsigned long) iter->size);

          next = iter->next;
          free (iter->content);
          free (iter);
        }
    }

  free (mt);
}

   For the sake of good encapsulation, mt_allocator() returns the
struct libavl_allocator associated with a given memory tracker:

131. <Memory tracker 127> +=
/* Returns the struct libavl_allocator associated with mt. */
void *
mt_allocator (struct mt_allocator *mt)
{
  return &mt->allocator;
}

   The allocator function mt_allocate() is in charge of implementing the
selected allocation policy.  It delegates most of the work to a pair of
helper functions new_block() and reject_request() and makes use of
utility function xmalloc(), a simple wrapper for malloc() that aborts
the program on failure.  The implementation is straightforward:

132. <Memory tracker 127> +=
/* Creates a new struct block containing size bytes of content
   and returns a pointer to content. */
static void *
new_block (struct mt_allocator *mt, size_t size)
{
  struct block *new;

  /* Allocate and initialize new struct block. */
  new = xmalloc (sizeof *new);
  new->next = NULL;
  new->idx = mt->alloc_idx++;
  new->size = size;
  new->used = 0;
  new->content = xmalloc (size);

  /* Add block to linked list. */
  if (mt->head == NULL)
    mt->head = new;
  else
    mt->tail->next = new;
  mt->tail = new;

  /* Alert user. */
  if (mt->verbosity >= 3)
    printf ("    block #%d: allocated %lu bytes\n",
            new->idx, (unsigned long) size);

  /* Finish up and return. */
  mt->block_cnt++;
  return new->content;
}

/* Prints a message about a rejected allocation if appropriate. */
static void
reject_request (struct mt_allocator *mt, size_t size)
{
  if (mt->verbosity >= 2)
    printf ("    block #%d: rejected request for %lu bytes\n",
            mt->alloc_idx++, (unsigned long) size);
}

/* Allocates and returns a block of size bytes. */
static void *
mt_allocate (struct libavl_allocator *allocator, size_t size)
{
  struct mt_allocator *mt = (struct mt_allocator *) allocator;

  /* Special case. */
  if (size == 0)
    return NULL;

  switch (mt->policy)
    {
    case MT_TRACK:
      return new_block (mt, size);

    case MT_NO_TRACK:
      return xmalloc (size);

    case MT_FAIL_COUNT:
      if (mt->arg[MT_COUNT] == 0)
        {
          reject_request (mt, size);
          return NULL;
        }
      mt->arg[MT_COUNT]--;
      return new_block (mt, size);

    case MT_FAIL_PERCENT:
      if (rand () / (RAND_MAX / 100 + 1) < mt->arg[MT_PERCENT])
        {
          reject_request (mt, size);
          return NULL;
        }
      else
        return new_block (mt, size);

    case MT_SUBALLOC:
      if (mt->tail == NULL
          || mt->tail->used + size > (size_t) mt->arg[MT_BLOCK_SIZE])
        new_block (mt, mt->arg[MT_BLOCK_SIZE]);
      if (mt->tail->used + size <= (size_t) mt->arg[MT_BLOCK_SIZE])
        {
          void *p = (char *) mt->tail->content + mt->tail->used;
          size = ((size + mt->arg[MT_ALIGN] - 1)
                  / mt->arg[MT_ALIGN] * mt->arg[MT_ALIGN]);
          mt->tail->used += size;
          if (mt->verbosity >= 3)
            printf ("    block #%d: suballocated %lu bytes\n",
                    mt->tail->idx, (unsigned long) size);
          return p;
        }
      else
        fail ("blocksize %lu too small for %lubyte allocation",
              (unsigned long) mt->tail->size, (unsigned long) size);

    default:
      assert (0);
    }
}

   The corresponding function mt_free() searches the block list for the
specified block, removes it, and frees the associated memory.  It
reports an error if the block is not in the list:

133. <Memory tracker 127> +=
/* Releases block previously returned by mt_allocate(). */
static void
mt_free (struct libavl_allocator *allocator, void *block)
{
  struct mt_allocator *mt = (struct mt_allocator *) allocator;
  struct block *iter, *prev;

  /* Special cases. */
  if (block == NULL || mt->policy == MT_NO_TRACK)
    {
      free (block);
      return;
    }
  if (mt->policy == MT_SUBALLOC)
    return;

  /* Search for block within the list of allocated blocks. */
  for (prev = NULL, iter = mt->head; iter; prev = iter, iter = iter->next)
    {
      if (iter->content == block)
        {
          /* Block found.  Remove it from the list. */
          struct block *next = iter->next;

          if (prev == NULL)
            mt->head = next;
          else
            prev->next = next;
          if (next == NULL)
            mt->tail = prev;

          /* Alert user. */
          if (mt->verbosity >= 4)
            printf ("    block #%d: freed %lu bytes\n",
                    iter->idx, (unsigned long) iter->size);

          /* Free block. */
          free (iter->content);
          free (iter);

          /* Finish up and return. */
          mt->block_cnt--;
          return;
        }
    }

  /* Block not in list. */
  printf ("    attempt to free unknown block %p (already freed?)\n", block);
}

See also:  *Note ISO 1990::, sections 6.3.8 and 6.3.9.

Exercises:

1. As its first action, mt_allocate() checks for and special-cases a
size of 0.  Why?  [*Note answer: 4-14-4#1..]


File: libavl.info,  Node: User Interaction,  Next: Utility Functions,  Prev: Memory Manager,  Up: Testing BST Functions

4.14.5 User Interaction
-----------------------

This section briefly discusses libavl's data structures and functions
for parsing command-line arguments.  For more information on the
command-line arguments accepted by the testing program, refer to the
libavl reference manual.

   The main way that the test program receives instructions from the
user is through the set of arguments passed to main().  The program
assumes that these arguments can be controlled easily by the user,
presumably through some kind of command-based "shell" program.  It
allows for two kinds of options: traditional UNIX "short options" that
take the form `-o' and GNU-style "long options" of the form `--option'.
Either kind of option may take an argument.

   Options are specified using an array of struct option, terminated by
an all-zero structure:

134. <Test declarations 122> +=
/* A single command-line option. */
struct option
  {
    const char *long_name;	/* Long name ("-name"). */
    int short_name;		/* Short name ("n"); value returned. */
    int has_arg;		/* Has a required argument? */
  };

   There are two public functions in the option parser:

struct option_state *option_init (struct option *options, char **args)
     Creates and returns a struct option_state, initializing it based on
     the array of arguments passed in.  This structure is used to keep
     track of the option parsing state.  Sets options as the set of
     options to parse.

int option_get (struct option_state *state, char **argp)
     Parses the next option from state and returns the value of the
     short_name member from its struct option.  Sets *argp to the
     option's argument or NULL if none.  Returns -1 and destroys state
     if no options remain.

   These functions' implementation are not too interesting for our
purposes, so they are relegated to an appendix.  *Note Option Parser::,
for the full story.

   The option parser provides a lot of support for parsing the command
line, but of course the individual options have to be handled once they
are retrieved by option_get().  The parse_command_line() function takes
care of the whole process:

void parse_command_line (char **args, struct test_options *options)
     Parses the command-line arguments in args[], which must be
     terminated with an element set to all zeros, using option_init()
     and option_get().  Sets up options appropriately to correspond.

   *Note Command-Line Parser::, for source code.  The struct
test_options initialized by parse_command_line() is described in detail
below.


File: libavl.info,  Node: Utility Functions,  Next: Main Program,  Prev: User Interaction,  Up: Testing BST Functions

4.14.6 Utility Functions
------------------------

The first utility function is compare_ints().  This function is not
used by <*Note test.c: 98.> but it is included there because it is used
by the test modules for all the individual tree structures.

135. <Test utility functions 135> =
/* Utility functions. */

<*Note Comparison function for ints: 4.>
   See also *Note 137:: and *Note 138::.
This code is included in *Note 98::.

   It is prototyped in <*Note test.h: 100.>:

136. <Test prototypes 102> +=
int compare_ints (const void *pa, const void *pb, void *param);

   The fail() function prints a provided error message to stderr,
formatting it as with printf(), and terminates the program
unsuccessfully:

137. <Test utility functions 135> +=
/* Prints message on stderr, which is formatted as for printf(),
   and terminates the program unsuccessfully. */
static void
fail (const char *message, ...)
{
  va_list args;

  fprintf (stderr, "%s: ", pgm_name);

  va_start (args, message);
  vfprintf (stderr, message, args);
  va_end (args);

  putchar ('\n');

  exit (EXIT_FAILURE);
}

   Finally, the xmalloc() function is a malloc() wrapper that aborts
the program if allocation fails:

138. <Test utility functions 135> +=
/* Allocates and returns a pointer to size bytes of memory.
   Aborts if allocation fails. */
static void *
xmalloc (size_t size)
{
  void *block = malloc (size);
  if (block == NULL && size != 0)
    fail ("out of memory");
  return block;
}


File: libavl.info,  Node: Main Program,  Prev: Utility Functions,  Up: Testing BST Functions

4.14.7 Main Program
-------------------

Everything comes together in the main program.  The test itself
(default or overflow) is selected with enum test:

139. <Test declarations 122> +=
/* Test to perform. */
enum test
  {
    TST_CORRECTNESS,		/* Default tests. */
    TST_OVERFLOW,		/* Stack overflow test. */
    TST_NULL                    /* No test, just overhead. */
  };

   The program's entire behavior is controlled by struct test_options,
defined as follows:

140. <Test declarations 122> +=
/* Program options. */
struct test_options
  {
    enum test test;                     /* Test to perform. */
    enum insert_order insert_order;     /* Insertion order. */
    enum delete_order delete_order;     /* Deletion order. */

    enum mt_policy alloc_policy;        /* Allocation policy. */
    int alloc_arg[2];                   /* Policy arguments. */
    int alloc_incr; /* Amount to increment alloc_arg each iteration. */

    int node_cnt;                       /* Number of nodes in tree. */
    int iter_cnt;                       /* Number of runs. */

    int seed_given;                     /* Seed provided on command line? */
    unsigned seed;                      /* Random number seed. */

    int verbosity;                      /* Verbosity level, 0=default. */
    int nonstop;                        /* Don't stop after one error? */
  };

   The main() function for the test program is perhaps a bit long, but
simple.  It begins by parsing the command line and allocating memory,
then repeats a loop once for each repetition of the test.  Within the
loop, an insertion and a deletion order are selected, the memory tracker
is set up, and test function (either test() or test_overflow()) is
called.

141. <Test main program 141> =
int
main (int argc, char *argv[])
{
  struct test_options opts;	/* Command-line options. */
  int *insert, *delete;		/* Insertion and deletion orders. */
  int success;                  /* Everything okay so far? */

  /* Initialize pgm_name, using argv[0] if sensible. */
  pgm_name = argv[0] != NULL && argv[0][0] != '\0' ? argv[0] : "bsttest";

  /* Parse command line into options. */
  parse_command_line (argv, &opts);

  if (opts.verbosity >= 0)
    fputs ("bsttest for GNU libavl 2.0.3; use -help to get help.\n", stdout);

  if (!opts.seed_given)
    opts.seed = time_seed () % 32768u;

  insert = xmalloc (sizeof *insert * opts.node_cnt);
  delete = xmalloc (sizeof *delete * opts.node_cnt);

  /* Run the tests. */
  success = 1;
  while (opts.iter_cnt--)
    {
      struct mt_allocator *alloc;

      if (opts.verbosity >= 0)
        {
          printf ("Testing seed=%u", opts.seed);
          if (opts.alloc_incr)
            printf (", alloc arg=%d", opts.alloc_arg[0]);
          printf ("...\n");
          fflush (stdout);
        }

      /* Generate insertion and deletion order.
         Seed them separately to ensure deletion order is
         independent of insertion order. */
      srand (opts.seed);
      gen_insertions (opts.node_cnt, opts.insert_order, insert);

      srand (++opts.seed);
      gen_deletions (opts.node_cnt, opts.delete_order, insert, delete);

      if (opts.verbosity >= 1)
        {
          int i;

          printf ("  Insertion order:");
          for (i = 0; i < opts.node_cnt; i++)
            printf (" %d", insert[i]);
          printf (".\n");

          if (opts.test == TST_CORRECTNESS)
            {
              printf ("Deletion order:");
              for (i = 0; i < opts.node_cnt; i++)
                printf (" %d", delete[i]);
              printf (".\n");
            }
        }

      alloc = mt_create (opts.alloc_policy, opts.alloc_arg, opts.verbosity);

      {
        int okay;
        struct libavl_allocator *a = mt_allocator (alloc);

        switch (opts.test)
          {
          case TST_CORRECTNESS:
            okay = test_correctness (a, insert, delete, opts.node_cnt,
                                     opts.verbosity);
            break;

          case TST_OVERFLOW:
            okay = test_overflow (a, insert, opts.node_cnt, opts.verbosity);
            break;

          case TST_NULL:
            okay = 1;
            break;

          default:
            assert (0);
          }

        if (okay)
          {
            if (opts.verbosity >= 1)
              printf ("  No errors.\n");
          }
        else
          {
            success = 0;
            printf ("  Error!\n");
          }
      }

      mt_destroy (alloc);
      opts.alloc_arg[0] += opts.alloc_incr;

      if (!success && !opts.nonstop)
        break;
    }

  free (delete);
  free (insert);

  return success ? EXIT_SUCCESS : EXIT_FAILURE;
}
   This code is included in *Note 98::.

   The main program initializes our single global variable, pgm_name,
which receives the name of the program at start of execution:

142. <Test declarations 122> +=
/* Program name. */
char *pgm_name;


File: libavl.info,  Node: Additional Exercises for BSTs,  Prev: Testing BST Functions,  Up: Binary Search Trees

4.15 Additional Exercises
=========================

Exercises:

1. Sentinels were a main theme of the chapter before this one.  Figure
out how to apply sentinel techniques to binary search trees.  Write
routines for search and insertion in such a binary search tree with
sentinel.  Test your functions.  (You need not make your code fully
generic; e.g., it is acceptable to "hard-code" the data type stored in
the tree.)  [*Note answer: 4-15#1..]


File: libavl.info,  Node: AVL Trees,  Next: Red-Black Trees,  Prev: Binary Search Trees,  Up: Top

5 AVL Trees
***********

In the last chapter, we designed and implemented a table ADT using
binary search trees.  We were interested in binary trees from the
beginning because of their promise of speed compared to linear lists.

   But we only get these speed improvements if our binary trees are
arranged more or less optimally, with the tree's height as small as
possible.  If we insert and delete items in the tree in random order,
then chances are that we'll come pretty close to this optimal tree.(1)

   In "pathological" cases, search within binary search trees can be as
slow as sequential search, or even slower when the extra bookkeeping
needed for a binary tree is taken into account.  For example, after
inserting items into a BST in sorted order, we get something like the
vines on the left and the right below.  The BST in the middle below
illustrates a more unusual case, a "zig-zag" BST that results from
inserting items from alternating ends of an ordered list.

 [image src="patholog2.png" text="                            5    1         1
                           /      `-._      \\
                          4           5      2
                         /         _.'        \\
                        3         2            3
                       /           `_           \\
                      2              4           4
                     /              /             \\
                    1              3               5

" ]

Unfortunately, these pathological cases can easily come up in practice,
because sorted data in the input to a program is common.  We could
periodically balance the tree using some heuristic to detect that it is
"too tall".  In the last chapter, in fact, we used a weak version of
this idea, rebalancing when a stack overflow force it.  We could
abandon the idea of a binary search tree, using some other data
structure.  Finally, we could adopt some modifications to binary search
trees that prevent the pathological case from occurring.

   For the remainder of this book, we're only interested in the latter
choice.  We'll look at two sets of rules that, when applied to the
basic structure of a binary search tree, ensure that the tree's height
is kept within a constant factor of the minimum value.  Although this
is not as good as keeping the BST's height at its minimum, it comes
pretty close, and the required operations are much faster.  A tree
arranged to rules such as these is called a "balanced tree" (*note
balanced tree::).  The operations used for minimizing tree height are
said to "rebalance" (*note rebalance::) the tree, even though this is
different from the sort of rebalancing we did in the previous chapter,
and are said to maintain the tree's "balance."

   A balanced tree arranged according to the first set of rebalancing
rules that we'll examine is called an "AVL tree" (*note AVL tree::),
after its inventors, G. M. Adel'son-Vel'skii< and E. M.  Landis.  AVL
trees are the subject of this chapter, and the next chapter will
discuss red-black trees, another type of balanced tree.

   In the following sections, we'll construct a table implementation
based on AVL trees.  Here's an outline of the AVL code:

143. <avl.h 143> =
<*Note Library License: 1.>
#ifndef AVL_H
#define AVL_H 1

#include <stddef.h>

<*Note Table types; tbl => avl: 15.>
<*Note AVL maximum height: 145.>
<*Note BST table structure; bst => avl: 28.>
<*Note AVL node structure: 146.>
<*Note BST traverser structure; bst => avl: 62.>
<*Note Table function prototypes; tbl => avl: 16.>

#endif /* avl.h */

144. <avl.c 144> =
<*Note Library License: 1.>
#include <assert.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include "avl.h"

<*Note AVL functions: 147.>

* Menu:

* AVL Balancing Rule::
* AVL Data Types::
* AVL Operations::
* Inserting into an AVL Tree::
* Deleting from an AVL Tree::
* Traversal of an AVL Tree::
* Copying an AVL Tree::
* Testing AVL Trees::

See also:  *Note Knuth 1998b::, sections 6.2.2 and 6.2.3; *Note Cormen
1990::, section 13.4.

   ---------- Footnotes ----------

   (1) This seems true intuitively, but there are some difficult
mathematics in this area.  For details, refer to *Note Knuth 1998b::
theorem 6.2.2H, *Note Knuth 1977::, and *Note Knuth 1978::.


File: libavl.info,  Node: AVL Balancing Rule,  Next: AVL Data Types,  Prev: AVL Trees,  Up: AVL Trees

5.1 Balancing Rule
==================

A binary search tree is an AVL tree if the difference in height between
the subtrees of each of its nodes is between -1 and +1.  Said another
way, a BST is an AVL tree if it is an empty tree or if its subtrees are
AVL trees and the difference in height between its left and right
subtree is between -1 and +1.

   Here are some AVL trees:

 [image src="avlex.png" text="                                   3         4
                         2        / \\       / \\
                         ^       2   4     2   5
                        1 3     /          ^
                               1          1 3

" ]

These binary search trees are not AVL trees:

 [image src="notavlex.png" text="                                  3       4
                                 /       /
                                2       2
                               /        ^
                              1        1 3

" ]

In an AVL tree, the height of a node's right subtree minus the height of
its left subtree is called the node's "balance factor" (*note balance
factor::).  Balance factors are always -1, 0, or +1.  They are often
represented as one of the single characters -, 0, or +.  Because of
their importance in AVL trees, balance factors will often be shown in
this chapter in AVL tree diagrams along with or instead of data items.
In tree diagrams, balance factors are enclosed in angle brackets:
`<->', `<0>', `<+>'.  Here are the AVL trees from above, but with
balance factors shown in place of data values:

 [image src="avlbalex.png" text="                                <->                  <->
               <0>            _'   \\           __..-'   \\
             _'   \\          <->    <0>       <0>        <0>
            <0>    <0>     _'               _'   \\
                          <0>              <0>    <0>

" ]

See also:  *Note Knuth 1998b::, section 6.2.3.

* Menu:

* Analysis of AVL Balancing Rule::


File: libavl.info,  Node: Analysis of AVL Balancing Rule,  Prev: AVL Balancing Rule,  Up: AVL Balancing Rule

5.1.1 Analysis
--------------

How good is the AVL balancing rule?  That is, before we consider how
much complication it adds to BST operations, what does this balancing
rule guarantee about performance?  This is a simple question only if
you're familiar with the mathematics behind computer science.  For our
purposes, it suffices to state the results:

     An AVL tree with n nodes has height between log2 (n + 1) and 1.44
     * log2 (n + 2) - 0.328.  An AVL tree with height h has between
     1.17 * pow (1.618, h) - 2 and pow (2, h) - 1 nodes.

     For comparison, an optimally balanced BST with n nodes has height
     ceil (log2 (n + 1)).  An optimally balanced BST with height h has
     between pow (2, h - 1) and pow (2, h) - 1 nodes.(1)

   The average speed of a search in a binary tree depends on the tree's
height, so the results above are quite encouraging: an AVL tree will
never be more than about 50% taller than the corresponding optimally
balanced tree.  Thus, we have a guarantee of good performance even in
the worst case, and optimal performance in the best case.

   To support at least 2**64 - 1 nodes in an AVL tree, as we do for
unbalanced binary search trees, we must define the maximum AVL tree
height to be 1.44 * log2 ((2**64 - 1) + 2) - 0.328, which is 92:

145. <AVL maximum height 145> =
/* Maximum AVL tree height. */
#ifndef AVL_MAX_HEIGHT
#define AVL_MAX_HEIGHT 92
#endif
   This code is included in *Note 143::.

See also:  *Note Knuth 1998b::, theorem 6.2.3A.

   ---------- Footnotes ----------

   (1) Here log2 is the standard C base-2 logarithm function, pow is
the exponentiation function, and ceil is the "ceiling" or "round up"
function.  For more information, consult a C reference guide, such as
*Note Kernighan 1988::.


File: libavl.info,  Node: AVL Data Types,  Next: AVL Operations,  Prev: AVL Balancing Rule,  Up: AVL Trees

5.2 Data Types
==============

We need to define data types for AVL trees like we did for BSTs.  AVL
tree nodes contain all the fields that a BST node does, plus a field
recording its balance factor:

146. <AVL node structure 146> =
/* An AVL tree node. */
struct avl_node
  {
    struct avl_node *avl_link[2];  /* Subtrees. */
    void *avl_data;                /* Pointer to data. */
    signed char avl_balance;       /* Balance factor. */
  };
   This code is included in *Note 143::.

   We're using avl_ as the prefix for all AVL-related identifiers.

   The other data structures for AVL trees are the same as for BSTs.


File: libavl.info,  Node: AVL Operations,  Next: Inserting into an AVL Tree,  Prev: AVL Data Types,  Up: AVL Trees

5.3 Operations
==============

Now we'll implement for AVL trees all the operations that we did for
BSTs.  Here's the outline.  Creation and search of AVL trees is exactly
like that for plain BSTs, and the generic table functions for insertion
convenience, assertion, and memory allocation are still relevant, so we
just reuse the code.  Of the remaining functions, we will write new
implementations of the insertion and deletion functions and revise the
traversal and copy functions.

147. <AVL functions 147> =
<*Note BST creation function; bst => avl: 31.>
<*Note BST search function; bst => avl: 32.>
<*Note AVL item insertion function: 148.>
<*Note Table insertion convenience functions; tbl => avl: 594.>
<*Note AVL item deletion function: 166.>
<*Note AVL traversal functions: 180.>
<*Note AVL copy function: 187.>
<*Note BST destruction function; bst => avl: 85.>
<*Note Default memory allocation functions; tbl => avl: 7.>
<*Note Table assertion functions; tbl => avl: 596.>
   This code is included in *Note 144::.


File: libavl.info,  Node: Inserting into an AVL Tree,  Next: Deleting from an AVL Tree,  Prev: AVL Operations,  Up: AVL Trees

5.4 Insertion
=============

The insertion function for unbalanced BSTs does not maintain the AVL
balancing rule, so we have to write a new insertion function.  But
before we get into the nitty-gritty details, let's talk in generalities.
This is time well spent because we will be able to apply many of the
same insights to AVL deletion and insertion and deletion in red-black
trees.

   Conceptually, there are two stages to any insertion or deletion
operation in a balanced tree.  The first stage may lead to violation of
the tree's balancing rule.  If so, we fix it in the second stage.  The
insertion or deletion itself is done in the first stage, in much the
same way as in an unbalanced BST, and we may also do a bit of
additional bookkeeping work, such as updating balance factors in an AVL
tree, or swapping node "colors" in red-black trees.

   If the first stage of the operation does not lead to a violation of
the tree's balancing rule, nothing further needs to be done.  But if it
does, the second stage rearranges nodes and modifies their attributes
to restore the tree's balance.  This process is said to "rebalance"
(*note rebalance::) the tree.  The kinds of rebalancing that might be
necessary depend on the way the operation is performed and the tree's
balancing rule.  A well-chosen balancing rule helps to minimize the
necessity for rebalancing.

   When rebalancing does become necessary in an AVL or red-black tree,
its effects are limited to the nodes along or near the direct path from
the inserted or deleted node up to the root of the tree.  Usually, only
one or two of these nodes are affected, but, at most, one simple
manipulation is performed at each of the nodes along this path.  This
property ensures that balanced tree operations are efficient (see
Exercise 1 for details).

   That's enough theory for now.  Let's return to discussing the
details of AVL insertion.  There are four steps in libavl's
implementation of AVL insertion:

  1. *Search* for the location to insert the new item.

  2. *Insert* the item as a new leaf.

  3. *Update* balance factors in the tree that were changed by the
     insertion.

  4. *Rebalance* the tree, if necessary.

   Steps 1 and 2 are the same as for insertion into a BST.  Step 3
performs the additional bookkeeping alluded to above in the general
description of balanced tree operations.  Finally, step 4 rebalances the
tree, if necessary, to restore the AVL balancing rule.

   The following sections will cover all the details of AVL insertion.
For now, here's an outline of avl_probe():

148. <AVL item insertion function 148> =
void **
avl_probe (struct avl_table *tree, void *item)
{
  <*Note avl_probe() local variables: 149.>

  assert (tree != NULL && item != NULL);

  <*Note Step 1: Search AVL tree for insertion point: 150.>
  <*Note Step 2: Insert AVL node: 151.>
  <*Note Step 3: Update balance factors after AVL insertion: 152.>
  <*Note Step 4: Rebalance after AVL insertion: 153.>
}
   This code is included in *Note 147::.

149. <avl_probe() local variables 149> =
struct avl_node *y, *z; /* Top node to update balance factor, and parent. */
struct avl_node *p, *q; /* Iterator, and parent. */
struct avl_node *n;     /* Newly inserted node. */
struct avl_node *w;     /* New root of rebalanced subtree. */
int dir;                /* Direction to descend. */

unsigned char da[AVL_MAX_HEIGHT]; /* Cached comparison results. */
int k = 0;              /* Number of cached results. */
   This code is included in *Note 148::, *Note 303::, and *Note 421::.

* Menu:

* Step 1 in AVL Insertion::
* Step 2 in AVL Insertion::
* Step 3 in AVL Insertion::
* Rebalancing AVL Trees::
* AVL Insertion Symmetric Case::
* AVL Insertion Example::
* Recursive Insertion::

See also:  *Note Knuth 1998b::, algorithm 6.2.3A.

Exercises:

*1. When rebalancing manipulations are performed on the chain of nodes
from the inserted or deleted node to the root, no manipulation takes
more than a fixed amount of time.  In other words, individual
manipulations do not involve any kind of iteration or loop.  What can
you conclude about the speed of an individual insertion or deletion in
a large balanced tree, compared to the best-case speed of an operation
for unbalanced BSTs?  [*Note answer: 5-4#1..]


File: libavl.info,  Node: Step 1 in AVL Insertion,  Next: Step 2 in AVL Insertion,  Prev: Inserting into an AVL Tree,  Up: Inserting into an AVL Tree

5.4.1 Step 1: Search
--------------------

The search step is an extended version of the corresponding code for
BST insertion in <*Note BST item insertion function: 33.>.  The earlier
code had only two variables to maintain: the current node the direction
to descend from p.  The AVL code does this, but it maintains some other
variables, too.  During each iteration of the for loop, p is the node
we are examining, q is p's parent, y is the most recently examined node
with nonzero balance factor, z is y's parent, and elements 0...k - 1 of
array da[] record each direction descended, starting from z, in order
to arrive at p.  The purposes for many of these variables are surely
uncertain right now, but they will become clear later.

150. <Step 1: Search AVL tree for insertion point 150> =
z = (struct avl_node *) &tree->avl_root;
y = tree->avl_root;
dir = 0;
for (q = z, p = y; p != NULL; q = p, p = p->avl_link[dir])
  {
    int cmp = tree->avl_compare (item, p->avl_data, tree->avl_param);
    if (cmp == 0)
      return &p->avl_data;

    if (p->avl_balance != 0)
      z = q, y = p, k = 0;
    da[k++] = dir = cmp > 0;
  }
   This code is included in *Note 148::.


File: libavl.info,  Node: Step 2 in AVL Insertion,  Next: Step 3 in AVL Insertion,  Prev: Step 1 in AVL Insertion,  Up: Inserting into an AVL Tree

5.4.2 Step 2: Insert
--------------------

Following the search loop, q is the last non-null node examined, so it
is the parent of the node to be inserted.  The code below creates and
initializes a new node as a child of q on side dir, and stores a
pointer to it into n.  Compare this code for insertion to that within
<*Note BST item insertion function: 33.>.

151. <Step 2: Insert AVL node 151> =
n = q->avl_link[dir] =
  tree->avl_alloc->libavl_malloc (tree->avl_alloc, sizeof *n);
if (n == NULL)
  return NULL;

tree->avl_count++;
n->avl_data = item;
n->avl_link[0] = n->avl_link[1] = NULL;
n->avl_balance = 0;
if (y == NULL)
  return &n->avl_data;
   This code is included in *Note 148::.

Exercises:

1. How can y be NULL?  Why is this special-cased?  [*Note answer:
5-4-2#1..]


File: libavl.info,  Node: Step 3 in AVL Insertion,  Next: Rebalancing AVL Trees,  Prev: Step 2 in AVL Insertion,  Up: Inserting into an AVL Tree

5.4.3 Step 3: Update Balance Factors
------------------------------------

When we add a new node n to an AVL tree, the balance factor of n's
parent must change, because the new node increases the height of one of
the parent's subtrees.  The balance factor of n's parent's parent may
need to change, too, depending on the parent's balance factor, and in
fact the change can propagate all the way up the tree to its root.

   At each stage of updating balance factors, we are in a similar
situation.  First, we are examining a particular node p that is one of
n's direct ancestors.  The first time around, p is n's parent, the next
time, if necessary, p is n's grandparent, and so on.  Second, the
height of one of p's subtrees has increased, and which one can be
determined using da[].

   In general, if the height of p's left subtree increases, p's balance
factor decreases.  On the other hand, if the right subtree's height
increases, p's balance factor increases.  If we account for the three
possible starting balance factors and the two possible sides, there are
six possibilities.  The three of these corresponding to an increase in
one subtree's height are symmetric with the others that go along with
an increase in the other subtree's height.  We treat these three cases
below.

Case 1: p has balance factor 0
..............................

If p had balance factor 0, its new balance factor is - or +, depending
on the side of the root to which the node was added.  After that, the
change in height propagates up the tree to p's parent (unless p is the
tree's root) because the height of the subtree rooted at p's parent has
also increased.

   The example below shows a new node n inserted as the left child of a
node with balance factor 0.  On the far left is the original tree before
insertion; in the middle left is the tree after insertion but before any
balance factors are adjusted; in the middle right is the tree after the
first adjustment, with p as n's parent; on the far right is the tree
after the second adjustment, with p as n's grandparent.  Only in the
trees on the far left and far right are all of the balance factors
correct.

 [image src="avlins1.png" text="                          <0>              <0>               p
                        _'   \\           _'   \\             <->
         <0>           <0>    <0>        p     <0>        _'   \\
       _'   \\    =>  _'           =>    <->        =>    <->    <0>
      <0>    <0>     n                _'               _'
                    <0>               n                n
                                     <0>              <0>

" ]

Case 2: p's shorter subtree has increased in height
...................................................

If the new node was added to p's shorter subtree, then the subtree has
become more balanced and its balance factor becomes 0.  If p started
out with balance factor +, this means the new node is in p's left
subtree.  If p had a - balance factor, this means the new node is in
the right subtree.  Since tree p has the same height as it did before,
the change does not propagate up the tree any farther, and we are done.
Here's an example that shows pre-insertion and post-balance factor
updating views:

 [image src="avlins2.png" text="                                            <0>
                      <0>             __..-'   `._
                __..-'   \\           <+>           p
               <+>        <+>     =>    \\         <0>
                  \\          \\           <0>    _'   \\
                   <0>        <0>               n     <0>
                                               <0>

" ]

Case 3: p's taller subtree has increased in height
..................................................

If the new node was added on the taller side of a subtree with nonzero
balance factor, the balance factor becomes +2 or -2.  This is a
problem, because balance factors in AVL trees must be between -1 and
+1.  We have to rebalance the tree in this case.  We will cover
rebalancing later.  For now, take it on faith that rebalancing does not
increase the height of subtree p as a whole, so there is no need to
propagate changes any farther up the tree.

   Here's an example of an insertion that leads to rebalancing.  On the
left is the tree before insertion; in the middle is the tree after
insertion and updating balance factors; on the right is the tree after
rebalancing to.  The -2 balance factor is shown as two minus signs
(--).  The rebalanced tree is the same height as the original tree
before insertion.

 [image src="avlins3.png" text="                                   <-->
                      <->        _'           <0>
                    _'          <->         _'   \\
                   <0>    =>  _'        =>  n     <0>
                              n            <0>
                             <0>

" ]

As another demonstration that the height of a rebalanced subtree does
not change after insertion, here's a similar example that has one more
layer of nodes.  The trees below follow the same pattern as the ones
above, but the rebalanced subtree has a parent.  Even though the tree's
root has the wrong balance factor in the middle diagram, it turns out to
be correct after rebalancing.

 [image src="avlins4.png" text="                                    <->
               <->               _.'   \\                 <->
             _'   \\             <-->    <0>        __..-'   \\
            <->    <0>        _'                  <0>        <0>
          _'           =>    <->            =>  _'   \\
         <0>               _'                   n     <0>
                           n                   <0>
                          <0>

" ]

Implementation
..............

Looking at the rules above, we can see that only in case 1, where p's
balance factor is 0, do changes to balance factors continue to propagate
upward in the tree.  So we can start from n's parent and move upward in
the tree, handling case 1 each time, until we hit a nonzero balance
factor, handle case 2 or case 3 at that node, and we're done (except for
possible rebalancing afterward).

   Wait a second--there is no efficient way to move upward in a binary
search tree!(1)  Fortunately, there is another approach we can use.
Remember the extra code we put into <*Note Step 1: Search AVL tree for
insertion point: 150.>?  This code kept track of the last node we'd
passed through that had a nonzero balance factor as y.  We can use y to
move downward, instead of upward, through the nodes whose balance
factors are to be updated.

   Node y itself is the topmost node to be updated; when we arrive at
node n, we know we're done.  We also kept track of the directions we
moved downward in da[].  Suppose that we've got a node p whose balance
factor is to be updated and a direction d that we moved from it.  We
know that if we moved down to the left (d == 0) then the balance factor
must be decreased, and that if we moved down to the right (d == 1) then
the balance factor must be increased.

   Now we have enough knowledge to write the code to update balance
factors.  The results are almost embarrassingly short:

152. <Step 3: Update balance factors after AVL insertion 152> =
for (p = y, k = 0; p != n; p = p->avl_link[da[k]], k++)
  if (da[k] == 0)
    p->avl_balance--;
  else
    p->avl_balance++;
   This code is included in *Note 148::, *Note 303::, and *Note 421::.

   Now p points to the new node as a consequence of the loop's exit
condition.  Variable p will not be modified again in this function, so
it is used in the function's final return statement to take the address
of the new node's avl_data member (see <*Note AVL item insertion
function: 148.> above).

Exercises:

1. Can case 3 be applied to the parent of the newly inserted node?
[*Note answer: 5-4-3#1..]

2. For each of the AVL trees below, add a new node with a value smaller
than any already in the tree and update the balance factors of the
existing nodes.  For each balance factor that changes, indicate the
numbered case above that applies.  Which of the trees require
rebalancing after the insertion?

 [image src="avlexer.png" text="                    <0>             <+>
              __..-'   `._        _'   `._              <->
             <+>          <->    <0>      <0>         _'
                \\       _'              _'   \\       <0>
                 <0>   <0>             <0>    <0>

" ]
[*Note answer: 5-4-3#2..]

3. Earlier versions of libavl used chars, not unsigned chars, to cache
the results of comparisons, as the elements of da[] are used here.  At
some warning levels, this caused the GNU C compiler to emit the warning
"array subscript has type `char'" when it encountered expressions like
q->avl_link[da[k]].  Explain why this can be a useful warning message.
[*Note answer: 5-4-3#3..]

4. If our AVL trees won't ever have a height greater than 32, then we
can portably use the bits in a single unsigned long to compactly store
what the entire da[] array does.  Write a new version of step 3 to use
this form, along with any necessary modifications to other steps and
avl_probe()'s local variables.  [*Note answer: 5-4-3#4..]

   ---------- Footnotes ----------

   (1) We could make a list of the nodes as we move down the tree and
reuse it on the way back up.  We'll do that for deletion, but there's a
simpler way for insertion, so keep reading.


File: libavl.info,  Node: Rebalancing AVL Trees,  Next: AVL Insertion Symmetric Case,  Prev: Step 3 in AVL Insertion,  Up: Inserting into an AVL Tree

5.4.4 Step 4: Rebalance
-----------------------

We've covered steps 1 through 3 so far.  Step 4, rebalancing, is
somewhat complicated, but it's the key to the entire insertion
procedure.  It is also similar to, but simpler than, other rebalancing
procedures we'll see later.  As a result, we're going to discuss it in
detail.  Follow along carefully and it should all make sense.

   Before proceeding, let's briefly review the circumstances under which
we need to rebalance.  Looking back a few sections, we see that there
is only one case where this is required: case 3, when the new node is
added in the taller subtree of a node with nonzero balance factor.

   Case 3 is the case where y has a -2 or +2 balance factor after
insertion.  For now, we'll just consider the -2 case, because we can
write code for the +2 case later in a mechanical way by applying the
principle of symmetry.  In accordance with this idea, step 4 branches
into three cases immediately, one for each rebalancing case and a third
that just returns from the function if no rebalancing is necessary:

153. <Step 4: Rebalance after AVL insertion 153> =
if (y->avl_balance == -2)
  {
    <*Note Rebalance AVL tree after insertion in left subtree: 154.>
  }
else if (y->avl_balance == +2)
  {
    <*Note Rebalance AVL tree after insertion in right subtree: 159.>
  }
else
  return &n->avl_data;
   See also *Note 155:: and *Note 156::.
This code is included in *Note 148::.

   We will call y's left child x.  The new node is somewhere in the
subtrees of x.  There are now only two cases of interest, distinguished
on whether x has a + or - balance factor.  These cases are almost
entirely separate:

154. <Rebalance AVL tree after insertion in left subtree 154> =
struct avl_node *x = y->avl_link[0];
if (x->avl_balance == -1)
  {
    <*Note Rotate right at y in AVL tree: 157.>
  }
else
  {
    <*Note Rotate left at x then right at y in AVL tree: 158.>
  }
   This code is included in *Note 153:: and *Note 164::.

   In either case, w receives the root of the rebalanced subtree, which
is used to update the parent's pointer to the subtree root (recall that
z is the parent of y):

155. <Step 4: Rebalance after AVL insertion 153> +=
z->avl_link[y != z->avl_link[0]] = w;

   Finally, we increment the generation number, because the tree's
structure has changed.  Then we're done and we return to the caller:

156. <Step 4: Rebalance after AVL insertion 153> +=
tree->avl_generation++;
return &n->avl_data;

Case 1: x has - balance factor
..............................

For a - balance factor, we just rotate right at y.  Then the entire
process, including insertion and rebalancing, looks like this:

 [image src="avlcase1.png" text="                      |                |          |
                      y                y          x
                     <->             <-->        <0>
                 _.-'   \\        _.-'    \\      /   `_
                 x       c =>    x        c => a*      y
                <0>             <->                   <0>
               /   \\           /   \\                 /   \\
              a     b         a*    b               b     c

" ]

This figure also introduces a new graphical convention.  The change in
subtree a between the first and second diagrams is indicated by an
asterisk (*).(1) In this case, it indicates that the new node was
inserted in subtree a.

The code here is similar to rotate_right() in the solution to
Exercise 4.3-2:

157. <Rotate right at y in AVL tree 157> =
w = x;
y->avl_link[0] = x->avl_link[1];
x->avl_link[1] = y;
x->avl_balance = y->avl_balance = 0;
   This code is included in *Note 154:: and *Note 531::.

Case 2: x has + balance factor
..............................

This case is just a little more intricate.  First, let x's right child
be w.  Either w is the new node, or the new node is in one of w's
subtrees.  To restore balance, we rotate left at x, then rotate right
at y (this is a kind of "double rotation").  The process, starting just
after the insertion and showing the results of each rotation, looks
like this:

 [image src="avlcase2.png" text="                         |               |
                         y               y           |
                       <-->            <-->          w
                  __.-'    \\         _'    \\        <0>
                  x         d       w       d =>   /   \\
                 <+>          =>   / \\            x     y
                /   \\             x   c           ^     ^
               a     w            ^              a b   c d
                     ^           a b
                    b c

" ]

At the beginning, the figure does not show the balance factor of w.
This is because there are three possibilities:

*Case 2.1:* w has balance factor 0.
     This means that w is the new node.  a, b, c, and d have height 0.
     After the rotations, x and y have balance factor 0.

*Case 2.2:* w has balance factor -.
     a, b, and d have height h > 0, and c has height h - 1.

*Case 2.3:* w has balance factor +.
     a, c, and d have height h > 0, and b has height h - 1.

158. <Rotate left at x then right at y in AVL tree 158> =
assert (x->avl_balance == +1);
w = x->avl_link[1];
x->avl_link[1] = w->avl_link[0];
w->avl_link[0] = x;
y->avl_link[0] = w->avl_link[1];
w->avl_link[1] = y;
if (w->avl_balance == -1)
  x->avl_balance = 0, y->avl_balance = +1;
else if (w->avl_balance == 0)
  x->avl_balance = y->avl_balance = 0;
else /* w->avl_balance == +1 */
  x->avl_balance = -1, y->avl_balance = 0;
w->avl_balance = 0;
   This code is included in *Note 154::, *Note 179::, *Note 309::,
*Note 429::, and *Note 532::.

Exercises:

1. Why can't the new node be x rather than a node in x's subtrees?
[*Note answer: 5-4-4#1..]

2. Why can't x have a 0 balance factor?  [*Note answer: 5-4-4#2..]

3. For each subcase of case 2, draw a figure like that given for generic
case 2 that shows the specific balance factors at each step.  [*Note
answer: 5-4-4#3..]

4. Explain the expression z->avl_link[y != z->avl_link[0]] = w in the
second part of <*Note Step 4: Rebalance after AVL insertion: 153.>
above.  Why would it be a bad idea to substitute the apparent equivalent
z->avl_link[y == z->avl_link[1]] = w?  [*Note answer: 5-4-4#4..]

5. Suppose that we wish to make a copy of an AVL tree, preserving the
original tree's shape, by inserting nodes from the original tree into a
new tree, using avl_probe().  Will inserting the original tree's nodes
in level order (see the answer to Exercise 4.7-4) have the desired
effect?  [*Note answer: 5-4-4#5..]

   ---------- Footnotes ----------

   (1) A "prime" (') is traditional, but primes are easy to overlook.


File: libavl.info,  Node: AVL Insertion Symmetric Case,  Next: AVL Insertion Example,  Prev: Rebalancing AVL Trees,  Up: Inserting into an AVL Tree

5.4.5 Symmetric Case
--------------------

Finally, we need to write code for the case that we chose not to discuss
earlier, where the insertion occurs in the right subtree of y.  All we
have to do is invert the signs of balance factors and switch avl_link[]
indexes between 0 and 1.  The results are this:

159. <Rebalance AVL tree after insertion in right subtree 159> =
struct avl_node *x = y->avl_link[1];
if (x->avl_balance == +1)
  {
    <*Note Rotate left at y in AVL tree: 160.>
  }
else
  {
    <*Note Rotate right at x then left at y in AVL tree: 161.>
  }
   This code is included in *Note 153:: and *Note 164::.

160. <Rotate left at y in AVL tree 160> =
w = x;
y->avl_link[1] = x->avl_link[0];
x->avl_link[0] = y;
x->avl_balance = y->avl_balance = 0;
   This code is included in *Note 159:: and *Note 534::.

161. <Rotate right at x then left at y in AVL tree 161> =
assert (x->avl_balance == -1);
w = x->avl_link[0];
x->avl_link[0] = w->avl_link[1];
w->avl_link[1] = x;
y->avl_link[1] = w->avl_link[0];
w->avl_link[0] = y;
if (w->avl_balance == +1)
  x->avl_balance = 0, y->avl_balance = -1;
else if (w->avl_balance == 0)
  x->avl_balance = y->avl_balance = 0;
else /* w->avl_balance == -1 */
  x->avl_balance = +1, y->avl_balance = 0;
w->avl_balance = 0;
   This code is included in *Note 159::, *Note 176::, *Note 312::,
*Note 430::, and *Note 535::.

